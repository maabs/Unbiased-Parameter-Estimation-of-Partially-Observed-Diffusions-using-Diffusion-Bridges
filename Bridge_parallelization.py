# THIS FILE IS MADE IN ORDER TO PARALLELIZE THE CODE.
# IT WILL FOLLOW A CLOSE STRUCTURE TO THE PARALLELIZATION FILES OF PREVIOUS PROJECTS.
#%%
import math
from scipy.stats import ncx2
import sympy
import bridge as bdg
import multiprocessing
import numpy as np
import PF_functions_def as pff
from scipy.special import gamma as gamma_den, iv 
import matplotlib.pyplot as plt
from scipy import linalg as la
import copy
from scipy.stats import gamma

from scipy.sparse import identity
from scipy.sparse import rand
from scipy.sparse import diags
from scipy.sparse import triu
import sys

#from sklearn.linear_model import LinearRegression
from scipy.stats import ortho_group
import time
from scipy.stats import norm
import scipy.stats as ss
#
# %%

def Prl_Grad_Cond_PF_bridge_back_samp(args):

    [lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
    fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate]=args

    # NEW ARGUMENTS:
    # A_fd,Sig_fd,fi_fd, fi_til_fd,r_pars_fd,H_pars_fd are parameter used to obtain the finite difference 
    # derivative. They have the same format as their counterparts without the _fd.

    # Grad_log_G if a function that computes the gradient of the observation likelihood,
    # it has the same parameters as the observation likelihood.
    # Grad_log_aux is the gradient of the transition density of the auxiliary process.
    # It has the same parameters as the transition density of the auxiliary process.


    # NEW OUTPUTS:

    # Grads: is a 3 dimensional array with the gradient of the score function with respect to the
    # parameters A and fi (this is a bit simplified since the parameters A and fi might include more parameters
    # thank just the ones we get the derivative w.r.t.) and the gradient of the observation likelihood.


    [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond]\
    =bdg.Cond_PF_bridge_back_samp(lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
    r,r_pars,H,H_pars,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, resamp_coef, l, d,N,seed)
    
    Grad_log_Gs=0
    Grad_log_aux_transition=np.zeros(2)
    Grad_A_intGs=0
    Grad_fi_intGs=0
    Grads=np.zeros(3)
    for j in range(int(T/d)):
            
            t_in=t0+j*d
            t_fin=t0+(j+1)*d
            Grad_log_Gs+=bdg.Grad_log_G(cond_path[j],obs[j],g_den_par)

            if j==0:
                x_in=x0[0]
            else:
                x_in=cond_path[j-1]
            Grad_log_aux_transition+=\
            bdg.Grad_log_aux_trans(t_in,x_in,t_fin,cond_path[j],atdp)
            # IMPORTANT: Notice that here the argument is atdp, which is related to the auxiliar 
            # arguments, careful consideration is necessary in the definition of the funciton 
            # Grad_log_aux_trans.

            intG_mod_A=bdg.Bridge_1d(t_in,x_in,t_fin,cond_path[j],b,A_fd,Sig,fi,b_til,\
            A_til,Sig_til,fi_til,r,r_pars,H,\
            H_pars,l,d,1,seeds_cond[j,0],\
            j=seeds_cond[j,1],fd=True,N_pf=N)

            intG_mod_fi=bdg.Bridge_1d(t_in,x_in,t_fin,cond_path[j],b,A,Sig,fi_fd,b_til,\
            A_til,Sig_til,fi_til_fd,r,r_pars_fd,H,\
            H_pars_fd,l,d,1,seeds_cond[j,0],\
            j=seeds_cond[j,1],fd=True,N_pf=N)
            
            Grad_A_intGs-=((cond_int_G[j]-intG_mod_A)/fd_rate)
            Grad_fi_intGs-=((cond_int_G[j]-intG_mod_fi)/fd_rate)
        
    Grads[0]=Grad_A_intGs[0]#+Grad_log_aux_transition[0]
    # The comment in the previous line is because we assume the auxiliar drift parameter does not 
    # depend on the original drift parameter.
    Grads[1]=Grad_fi_intGs[0]+Grad_log_aux_transition[1]
    
    Grads[2]=Grad_log_Gs
    return [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]



def Prl_Grad_Cond_PF_bridge_back_samp_an(args):

    [lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
    fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate]=args

    # NEW ARGUMENTS:
    # A_fd,Sig_fd,fi_fd, fi_til_fd,r_pars_fd,H_pars_fd are parameter used to obtain the finite difference 
    # derivative. They have the same format as their counterparts without the _fd.

    # Grad_log_G if a function that computes the gradient of the observation likelihood,
    # it has the same parameters as the observation likelihood.
    # Grad_log_aux is the gradient of the transition density of the auxiliary process.
    # It has the same parameters as the transition density of the auxiliary process.


    # NEW OUTPUTS:

    # Grads: is a 3 dimensional array with the gradient of the score function with respect to the
    # parameters A and fi (this is a bit simplified since the parameters A and fi might include more parameters
    # thank just the ones we get the derivative w.r.t.) and the gradient of the observation likelihood.


    [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond]\
    =bdg.Cond_PF_bridge_back_samp(lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
    r,r_pars,H,H_pars,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, resamp_coef, l, d,N,seed)
    
    Grad_log_Gs=0
    Grad_log_aux_transition=np.zeros(2)
    Grad_A_intGs=0
    Grad_fi_intGs=0
    Grads=np.zeros(3)
    for j in range(int(T/d)):
            
            t_in=t0+j*d
            t_fin=t0+(j+1)*d
            Grad_log_Gs+=bdg.Grad_log_G(cond_path[j],obs[j],g_den_par)

            if j==0:
                x_in=x0[0]
            else:
                x_in=cond_path[j-1]
            Grad_log_aux_transition+=\
            bdg.Grad_log_aux_trans(t_in,x_in,t_fin,cond_path[j],atdp)
            # IMPORTANT: Notice that here the argument is atdp, which is related to the auxiliar 
            # arguments, careful consideration is necessary in the definition of the funciton 
            # Grad_log_aux_trans.

            intG, int_psi_the,int_psi_sig =bdg.Bridge_1d(t_in,x_in,t_fin,cond_path[j],b,A_fd,Sig,fi,b_til,\
            A_til,Sig_til,fi_til,r,r_pars,H,\
            H_pars,l,d,1,seeds_cond[j,0],\
            j=seeds_cond[j,1],fd=True,N_pf=N,an=True)

            Grad_the_int_psi+=int_psi_the
            Grad_sig_int_psi+=int_psi_sig
            
    Grads[0]=Grad_the_int_psi#+Grad_log_aux_transition[0]
    # The comment in the previous line is because we assume the auxiliar drift parameter does not 
    # depend on the original drift parameter.

    Grads[1]=Grad_sig_int_psi+Grad_log_aux_transition[1]
    
    Grads[2]=Grad_log_Gs
    return [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]




def Pr_Grad_Cond_PF_bridge_back_samp(args):


    [lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
    fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate]=args

    # NEW ARGUMENTS:
    # A_fd,Sig_fd,fi_fd, fi_til_fd,r_pars_fd,H_pars_fd are parameter used to obtain the finite difference 
    # derivative. They have the same format as their counterparts without the _fd.

    # Grad_log_G if a function that computes the gradient of the observation likelihood,
    # it has the same parameters as the observation likelihood.
    # Grad_log_aux is the gradient of the transition density of the auxiliary process.
    # It has the same parameters as the transition density of the auxiliary process.

    # NEW OUTPUTS:

    # Grads: is a 3 dimensional array with the gradient of the score function with respect to the
    # parameters A and fi (this is a bit simplified since the parameters A and fi might include more parameters
    # thank just the ones we get the derivative w.r.t.) and the gradient of the observation likelihood.

    [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond]\
    =bdg.Cond_PF_bridge_back_samp(lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
    r,r_pars,H,H_pars,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, resamp_coef,l,d,N,seed,crossed=False)
    
    Grad_log_Gs=0
    Grad_log_aux_transition=np.zeros(2)
    Grad_A_intGs=0
    Grad_fi_intGs=0
    Grads=np.zeros(3)
    for j in range(int(T/d)):
            
            t_in=t0+j*d
            t_fin=t0+(j+1)*d
            Grad_log_Gs+=Grad_log_G(cond_path[j],obs[j],g_den_par)

            if j==0:
                x_in=x0[0]
            else:
                x_in=cond_path[j-1]
            Grad_log_aux_transition+=\
            Grad_log_aux_trans(t_in,x_in,t_fin,cond_path[j],atdp)
            # IMPORTANT: Notice that here the argument is atdp, which is related to the auxiliar 
            # arguments, careful consideration is necessary in the definition of the funciton 
            # Grad_log_aux_trans.

            intG_mod_A=bdg.Bridge_1d(t_in,x_in,t_fin,cond_path[j],b,A_fd,Sig,fi,b_til,\
            A_til,Sig_til,fi_til,r,r_pars,H,\
            H_pars,l,d,1,seeds_cond[j,0],\
            j=seeds_cond[j,1],fd=True,N_pf=N)

            intG_mod_fi=bdg.Bridge_1d(t_in,x_in,t_fin,cond_path[j],b,A,Sig,fi_fd,b_til,\
            A_til,Sig_til,fi_til_fd,r,r_pars_fd,H,\
            H_pars_fd,l,d,1,seeds_cond[j,0],\
            j=seeds_cond[j,1],fd=True,N_pf=N)
            
            Grad_A_intGs-=((cond_int_G[j]-intG_mod_A)/fd_rate)
            Grad_fi_intGs-=((cond_int_G[j]-intG_mod_fi)/fd_rate)
        
    Grads[0]=Grad_A_intGs[0]#+Grad_log_aux_transition[0]
    # The comment in the previous line is because we assume the auxiliar drift parameter does not 
    # depend on the original drift parameter.
    Grads[1]=Grad_fi_intGs[0]+Grad_log_aux_transition[1]
    
    Grads[2]=Grad_log_Gs
    return [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]



def Prl_SGD_bridge_vanilla(args):

    # new parameters:
    # A_0, fi_0, g_par_0: initial parameters for the SGD algorithm.
    # mcmc_links, SGD_steps: number of mcmc links and number of SGD steps.
    # gamma, alpha
    [t0,x0,T,b,A_0,A_fd_0,Sig,fi_0,fi_fd_0,b_til,A_til_0,Sig_til,fi_til_0,\
    fi_til_fd_0,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,\
    obs,log_g_den,g_den_par_0, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
    mcmc_links,SGD_steps,gamma, alpha, update_pars]=args
    
    B=mcmc_links*SGD_steps
    mcmc_mean=np.zeros((int(T/d)))
    #gamma=0.1
    #alpha=0.25
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    #A_0=0.2
    #fi_0=1.6
    #g_par_0=1
    pars[0]=np.array([A_0,fi_0,g_den_par_0])
    Grads_test=np.zeros((B,3))
    A=A_0
    A_til=A_til_0
    A_fd=A_fd_0
    fi=fi_0
    fi_fd=fi_fd_0
    fi_til=fi_til_0
    fi_til_fd=fi_til_fd_0
    g_den_par=g_den_par_0
    #g_par
    # The next part might depend on the specific dynamics of the example
    # since we have to define the finite difference for the parameters and 
    # A, fi, g_par might be a list parameters, each. 

    
    # Similarly for the auxiliar parameters, these might be related to 
    # the parameters of the dynamics.

    """
    [log_weights,int_Gs,x_pr]=PF_bridge(t0,x0,T,b_ou_1d,theta,Sig_ou_1d,sigma,b_ou_aux,theta_aux,Sig_ou_aux,sigma_aux,\
    r_quasi_normal_1d,[ou_sd,[theta_aux,sigma_aux]],H_quasi_normal,[ou_sd,[theta_aux,sigma_aux],theta_aux],\
    sampling_ou, [theta_aux,sigma_aux],obs,log_g_normal_den,sd,\
    ou_trans_den,[theta_aux,sigma_aux],ou_trans_den,\
    resamp_coef,l,d, N,seed)
    """
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
    r,r_pars,H,H_pars,\
    sample_funct,sample_pars,obs,log_g_den,g_den_par,\
    aux_trans_den,atdp,prop_trans_den,\
    resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    Grads_mcmc=np.zeros((SGD_steps,3))
    cond_int_G=int_Gs[:,index]
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    #cov
    #print("The starting seed is: ",seed)
    #print("The conditional seed is: ",seeds_cond)
    #print("The condtional path is:",cond_path) 
    n=1
    cond_log_weights_test,cond_int_G_test,cond_path_test,seeds_cond_test=\
    cond_log_weights.copy() ,cond_int_G.copy(),cond_path.copy(),seeds_cond.copy()

    [A_fd,fi_fd,A_til,fi_til,fi_til_fd,\
    r_pars,r_pars_fd,H_pars,H_pars_fd,sample_pars,g_den_par,\
    atdp]=update_pars([A_0,fi_0,g_den_par_0],[A_0,fi_0,g_den_par_0],fd_rate,levels=1)

    for b_ind in range(B):
        # the varaible int_Gs is meant to have the record of int_G of the 
        # backward sampled path.
        #print("mcmc iteration is:", b)
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        np.random.seed(b_ind)

        """
        (lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
        fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,crossed=False)
        """
        [log_weights_test,x_pr_test,cond_log_weights_test,cond_int_G_test,cond_path_test,seeds_cond_test,Grads_t]=\
        bdg.Grad_Cond_PF_bridge_new(cond_log_weights_test,cond_int_G_test,cond_path_test,seeds_cond_test,t0,x0,T,b,\
        A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,fi_til_fd,r,r_pars,\
        r_pars_fd,H,H_pars,H_pars_fd,\
        sample_funct, sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,crossed=False)
        Grads_test[b_ind]=Grads_t
        ch_paths[b_ind]=cond_path_test
        if (b_ind+1)%mcmc_links==0:
            Grad_mcmc=np.mean(Grads_test[b_ind+1-mcmc_links:b_ind+1],axis=0)
            Grads_mcmc[n-1]=Grad_mcmc
            A+=np.exp(gamma*Grad_mcmc[0]/n**(0.5+alpha))
            #fi+=gamma*Grad_mcmc[1]/n**(0.5+alpha)
            fi*=np.exp(gamma*Grad_mcmc[1]*fi/n**(0.5+alpha))
            #g_den_par+=gamma*Grad_mcmc[2]/n**(0.5+alpha)
            g_den_par*=np.exp(gamma*Grad_mcmc[2]*g_den_par/n**(0.5+alpha))
            
            pars[n]=np.array([A,fi,g_den_par])
            
            [A_fd,fi_fd,A_til,fi_til,fi_til_fd,\
            r_pars,r_pars_fd,H_pars,H_pars_fd,sample_pars,g_den_par,\
            atdp]=update_pars(pars[n],pars[n],fd_rate,levels=1)

            n+=1
            #print("The new parameters are: ",A,fi,g_den_par)
    #mcmc_mean[i]=np.mean(ch_paths,axis=0)

    return ch_paths,pars,Grads_mcmc



def Prl_SGD_bridge(args):
    
    [t0,x0,T,b,A_0,A_fd_0,Sig,fi_0,fi_fd_0,b_til,A_til_0,Sig_til,fi_til_0,\
    fi_til_fd_0,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,\
    obs,log_g_den,g_den_par_0, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
    mcmc_links,SGD_steps,gamma, alpha, update_pars]\
    =args

    # new parameters:
    # A_0, fi_0, g_par_0: initial parameters for the SGD algorithm.
    # mcmc_links, SGD_steps: number of mcmc links and number of SGD steps.
    # gamma, alpha
    B=mcmc_links*SGD_steps
    mcmc_mean=np.zeros((int(T/d)))
    #gamma=0.1
    #alpha=0.25
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    #A_0=0.2
    #fi_0=1.6
    #g_par_0=1
    pars[0]=np.array([A_0,fi_0,g_den_par_0])
    Grads_test=np.zeros((B,3))
    A=A_0
    A_til=A_til_0
    A_fd=A_fd_0
    fi=fi_0
    fi_fd=fi_fd_0
    fi_til=fi_til_0
    fi_til_fd=fi_til_fd_0
    g_den_par=g_den_par_0
    #g_par
    # The next part might depend on the specific dynamics of the example
    # since we have to define the finite difference for the parameters and 
    # A, fi, g_par might be a list parameters, each. 

    
    # Similarly for the auxiliar parameters, these might be related to 
    # the parameters of the dynamics.

    """
    [log_weights,int_Gs,x_pr]=PF_bridge(t0,x0,T,b_ou_1d,theta,Sig_ou_1d,sigma,b_ou_aux,theta_aux,Sig_ou_aux,sigma_aux,\
    r_quasi_normal_1d,[ou_sd,[theta_aux,sigma_aux]],H_quasi_normal,[ou_sd,[theta_aux,sigma_aux],theta_aux],\
    sampling_ou, [theta_aux,sigma_aux],obs,log_g_normal_den,sd,\
    ou_trans_den,[theta_aux,sigma_aux],ou_trans_den,\
    resamp_coef,l,d, N,seed)
    """
   
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
    r,r_pars,H,H_pars,\
    sample_funct,sample_pars,obs,log_g_den,g_den_par,\
    aux_trans_den,atdp,prop_trans_den,\
    resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    cond_int_G=int_Gs[:,index]
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    Grads_mcmc=np.zeros((SGD_steps,3))
    #cov
    #print("The starting seed is: ",seed)
    #print("The conditional seed is: ",seeds_cond)
    #print("The condtional path is:",cond_path) 
    n=1
    cond_log_weights_test,cond_int_G_test,cond_path_test,seeds_cond_test=\
    cond_log_weights.copy() ,cond_int_G.copy(),cond_path.copy(),seeds_cond.copy()

    [A_fd,fi_fd,A_til,fi_til,fi_til_fd,\
    r_pars,r_pars_fd,H_pars,H_pars_fd,sample_pars,g_den_par,\
    atdp]=update_pars([A_0,fi_0,g_den_par_0],[A_0,fi_0,g_den_par_0],fd_rate,levels=1)


    for b_ind in range(B):
        # the varaible int_Gs is meant to have the record of int_G of the 
        # backward sampled path.
        #print("mcmc iteration is:", b)
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        np.random.seed(b_ind)
        [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads_t]=\
        bdg.Grad_Cond_PF_bridge_back_samp(cond_log_weights,cond_int_G,cond_path,seeds_cond,t0,x0,T,b,\
        A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,fi_til_fd,r,r_pars,\
        r_pars_fd,H,H_pars,H_pars_fd,\
        sample_funct, sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,crossed=False)
        Grads_test[b_ind]=Grads_t
        ch_paths[b_ind]=cond_path
        if (b_ind+1)%mcmc_links==0:
            Grad_mcmc=np.mean(Grads_test[b_ind+1-mcmc_links:b_ind+1],axis=0)
            Grads_mcmc[n-1]=Grad_mcmc
            A+=gamma*Grad_mcmc[0]/n**(0.5+alpha)
            #fi+=gamma*Grad_mcmc[1]/n**(0.5+alpha)
            fi*=np.exp(gamma*Grad_mcmc[1]*fi/n**(0.5+alpha))
            #g_den_par+=gamma*Grad_mcmc[2]/n**(0.5+alpha)
            g_den_par*=np.exp(gamma*Grad_mcmc[2]*g_den_par/n**(0.5+alpha))
            
            pars[n]=np.array([A,fi,g_den_par])
            
            [A_fd,fi_fd,A_til,fi_til,fi_til_fd,\
            r_pars,r_pars_fd,H_pars,H_pars_fd,sample_pars,g_den_par,\
            atdp]=update_pars(pars[n],pars[n],fd_rate,levels=1)

            n+=1
            #print("The new parameters are: ",A,fi,g_den_par)
    #mcmc_mean[i]=np.mean(ch_paths,axis=0)

    return ch_paths,pars,Grads_test



def Prl_C_SGD_bridge(args):

    [t0,x0,T,b,A_in,A_fd_in,Sig,fi_in,fi_fd_in,b_til,A_til_in,Sig_til,fi_til_in,\
    fi_til_fd_in,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,max_sample_funct,sample_pars,\
    obs,log_g_den,g_den_par_in, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,update_pars,resamp_coef, l, d,N,seed,fd_rate,\
    mcmc_links,SGD_steps,gamma, alpha]=args
    # new parameters:
    # A_in, fi_in, g_par_0: initial parameters for the SGD algorithm.
    # mcmc_links, SGD_steps: number of mcmc links and number of SGD steps.
    # gamma, alpha
    B=mcmc_links*SGD_steps
    mcmc_mean=np.zeros((2,int(T/d)))
    #gamma=0.1
    #alpha=0.25
    resamp_coef=1
    pars_0=np.zeros((SGD_steps+1,3))
    pars_1=np.zeros((SGD_steps+1,3))
    #A_in=0.2
    #fi_in=1.6
    #g_par_0=1
    pars_0[0,:]=np.array([A_in,fi_in,g_den_par_in])
    pars_1[0,:]=np.array([A_in,fi_in,g_den_par_in])
    Grads_test_0=np.zeros((B,3))
    Grads_test_1=np.zeros((B,3))
    A=A_in
    A_til=A_til_in
    A_fd=A_fd_in
    fi=fi_in
    fi_fd=fi_fd_in
    fi_til=fi_til_in
    fi_til_fd=fi_til_fd_in
    g_den_par=g_den_par_in
    
    A_0=A_in
    fi_0=fi_in
    g_den_par_0=g_den_par_in
    A_1=A_in
    fi_1=fi_in
    g_den_par_1=g_den_par_in


    [A_fd_0,A_fd_1,fi_fd_0,fi_fd_1,A_til_0,A_til_1,fi_til_0,fi_til_1,fi_til_fd_0,\
    fi_til_fd_1,r_pars_0,r_pars_1,\
    r_pars_fd_0,r_pars_fd_1,H_pars_0,H_pars_1,H_pars_fd_0,H_pars_fd_1,\
    sample_pars,g_den_par_0,g_den_par_1,atdp_0,atdp_1,\
    ind_prop_trans_par_0,ind_prop_trans_par_1]=\
    update_pars([A_0,fi_0,g_den_par_0],[A_1,fi_1,g_den_par_1],fd_rate,levels=2)
    
    #"""
    """
    A_til_0=A_til_in
    A_fd_0=A_fd_in
    fi_fd_0=fi_fd_in
    fi_til_0=fi_til_in
    fi_til_fd_0=fi_til_fd_in
    g_den_par_0=g_den_par_in
    r_pars_0=r_pars
    H_pars_0=H_pars
    r_pars_fd_0=r_pars_fd
    H_pars_fd_0=H_pars_fd
    atdp_0=atdp
    ind_prop_trans_par_0=ind_prop_trans_par
    
    
    A_til_1=A_til_in
    A_fd_1=A_fd_in
    fi_fd_1=fi_fd_in
    fi_til_1=fi_til_in
    fi_til_fd_1=fi_til_fd_in
    g_den_par_1=g_den_par_in
    r_pars_1=r_pars
    H_pars_1=H_pars
    r_pars_fd_1=r_pars_fd
    H_pars_fd_1=H_pars_fd   
    atdp_1=atdp
    ind_prop_trans_par_1=ind_prop_trans_par
    """
    
    #g_par
    # The next part might depend on the specific dynamics of the example
    # since we have to define the finite difference for the parameters and 
    # A, fi, g_par might be a list parameters, each. 

    
    # Similarly for the auxiliar parameters, these might be related to 
    # the parameters of the dynamics.

    """
    C_PF_bridge(t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    max_sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den,ind_prop_trans_par, resamp_coef, l, d,N,seed,crossed=False):
    """

    """
    [log_weights_0,log_weights_1,int_Gs_0,int_Gs_1,x_pr_0,x_pr_1]
    """
    np.random.seed(seed)
    [log_weights_0,log_weights_1,int_Gs_0,int_Gs_1,x_pr_0,x_pr_1,seeds_0_wp,seeds_1_wp]=\
    bdg.C_PF_bridge(t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
    r,r_pars,H,H_pars,\
    max_sample_funct,sample_pars,obs,log_g_den,g_den_par,\
    aux_trans_den,atdp,prop_trans_den,ind_prop_trans_par,\
    resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    
    weights_0=pff.norm_logweights(log_weights_0[-1])
    weights_1=pff.norm_logweights(log_weights_1[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    #index_1=np.random.choice(np.array(range(N)),p=weights_1)
    cond_path_0=np.copy(x_pr_0[:,index])
    cond_path_1=np.copy(x_pr_1[:,index])
    cond_log_weights_0=np.copy(log_weights_0[:,index])
    cond_log_weights_1=np.copy(log_weights_1[:,index])
    seeds_cond_0=np.copy(seeds_0_wp[:,:,index])
    seeds_cond_1=np.copy(seeds_1_wp[:,:,index])

    ch_paths_0=np.zeros((B,int(T/d))) 
    ch_weights_0=np.zeros((B,int(T/d)))
    ch_paths_1=np.zeros((B,int(T/d)))
    ch_weights_1=np.zeros((B,int(T/d)))
    """
    cond_int_G_0=int_Gs_0[:,index_0]
    cond_int_G_1=int_Gs_1[:,index_1]
    """
    
    #cov
    #print("The starting seed is: ",seed)
    #print("The conditional seed is: ",seeds_cond)
    #print("The condtional path is:",cond_path) 
    """
    n=1
    cond_log_weights_test_0,cond_int_G_test_0,cond_path_test_0,seeds_cond_test_0=\
    cond_log_weights_0.copy() ,cond_int_G_0.copy(),cond_path_0.copy(),seeds_cond_0.copy()
    
    cond_log_weights_test_1,cond_int_G_test_1,cond_path_test_1,seeds_cond_test_1=\
    cond_log_weights_1.copy() ,cond_int_G_1.copy(),cond_path_1.copy(),seeds_cond_1.copy()
    """
    n=1

    for b_ind in range(B):
        # the varaible int_Gs is meant to have the record of int_G of the 
        # backward sampled path.
        #print("mcmc iteration is:", b)
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        np.random.seed(b_ind)
 
        """
        C_Grad_Cond_PF_bridge_back_samp(x_cond_0,x_cond_1,\
        seeds_cond_0,seeds_cond_1,t0,x0,T,b,A_0,A_1,A_fd_0,A_fd_1,Sig,fi_0,fi_1,fi_fd_0,fi_fd_1,b_til,\
        A_til_0,A_til_1,Sig_til,fi_til_0,fi_til_1,fi_til_fd_0,fi_til_fd_1,r,r_pars_0,r_pars_1,r_pars_fd_0,\
        r_pars_fd_1,H,H_pars_0,H_pars_1,H_pars_fd_0,H_pars_fd_1,sample_funct,sample_pars,obs,log_g_den,\
        g_den_par_0,g_den_par_1, aux_trans_den,atdp_0,atdp_1,\
        Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par_0,ind_prop_trans_par_1,Grad_log_G,l,d,N,seed,fd_rate,crossed=False):
        """
        [log_weights_test_0,log_weights_test_1,x_pr_test_0,x_pr_test_1,cond_log_weights_test_0,cond_log_weights_test_1,\
        cond_int_G_test_0,cond_int_G_test_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,Grads_t_0,Grads_t_1]=\
        bdg.C_Grad_Cond_PF_bridge_back_samp(cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,t0,x0,T,b,\
        A_0,A_1,A_fd_0,A_fd_1,Sig,fi_0,fi_1,fi_fd_0,fi_fd_1,b_til,A_til_0,A_til_1,Sig_til,fi_til_0,fi_til_1,fi_til_fd_0,\
        fi_til_fd_1,r,r_pars_0,r_pars_1,\
        r_pars_fd_0,r_pars_fd_1,H,H_pars_0,H_pars_1,H_pars_fd_0,H_pars_fd_1,\
        max_sample_funct, sample_pars,obs,log_g_den,g_den_par_0,g_den_par_1, aux_trans_den,atdp_0,atdp_1,\
        Grad_log_aux_trans,prop_trans_den, ind_prop_trans_par_0,ind_prop_trans_par_1,Grad_log_G, l, d,N,seed,fd_rate,crossed=False)
        Grads_test_0[b_ind]=Grads_t_0
        Grads_test_1[b_ind]=Grads_t_1
        ch_paths_0[b_ind]=cond_path_0
        ch_paths_1[b_ind]=cond_path_1
        if (b_ind+1)%mcmc_links==0:
            #print("SGD step is:",n)
            Grad_mcmc_0=np.mean(Grads_test_0[b_ind+1-mcmc_links:b_ind+1],axis=0)
            Grad_mcmc_1=np.mean(Grads_test_1[b_ind+1-mcmc_links:b_ind+1],axis=0)
            A_0+=gamma*Grad_mcmc_0[0]/n**(0.5+alpha)
            A_1+=gamma*Grad_mcmc_1[0]/n**(0.5+alpha)
            # The following lines are also SGD applied to the log of the parameters
            # which translate in the following SGD equations
            fi_0*=np.exp(gamma*Grad_mcmc_0[1]*fi_0/n**(0.5+alpha))
            fi_1*=np.exp(gamma*Grad_mcmc_1[1]*fi_1/n**(0.5+alpha))
            g_den_par_0*=np.exp(gamma*Grad_mcmc_0[2]*g_den_par_0/n**(0.5+alpha))
            g_den_par_1*=np.exp(gamma*Grad_mcmc_1[2]*g_den_par_1/n**(0.5+alpha))
            #fi_0+=gamma*Grad_mcmc_0[1]/n**(0.5+alpha)
            #fi_1+=gamma*Grad_mcmc_1[1]/n**(0.5+alpha)
            #g_den_par_0+=gamma*Grad_mcmc_0[2]/n**(0.5+alpha)*0
            #g_den_par_1+=gamma*Grad_mcmc_1[2]/n**(0.5+alpha)*0
            pars_0[n]=np.array([A_0,fi_0,g_den_par_0])
            pars_1[n]=np.array([A_1,fi_1,g_den_par_1])

            """
            [A_fd_0,A_fd_1,fi_fd_0,fi_fd_1,A_til_0,A_til_1,fi_til_0,fi_til_1,fi_til_fd_0,\
            fi_til_fd_1,r_pars_0,r_pars_1,\
            r_pars_fd_0,r_pars_fd_1,H_pars_0,H_pars_1,H_pars_fd_0,H_pars_fd_1,\
            sample_pars,g_den_par_0,g_den_par_1,atdp_0,atdp_1,\
            ind_prop_trans_par_0,ind_prop_trans_par_1]
            """

            [A_fd_0,A_fd_1,fi_fd_0,fi_fd_1,A_til_0,A_til_1,fi_til_0,fi_til_1,fi_til_fd_0,\
            fi_til_fd_1,r_pars_0,r_pars_1,\
            r_pars_fd_0,r_pars_fd_1,H_pars_0,H_pars_1,H_pars_fd_0,H_pars_fd_1,\
            sample_pars,g_den_par_0,g_den_par_1,atdp_0,atdp_1,\
            ind_prop_trans_par_0,ind_prop_trans_par_1]=update_pars(pars_0[n],pars_1[n],fd_rate,levels=2)
            
            n+=1
            #print("The new parameters are: ",A,fi,g_den_par)
    #mcmc_mean[i]=np.mean(ch_paths,axis=0)

    return ch_paths_0,ch_paths_1,pars_0 ,pars_1, Grads_test_0,Grads_test_1




def Prl_Grad_chain_new(args):

    # This function is build so we can test the current Grad function without 
    # the need for SGD, meaning that this is basically the SGD function with 
    # gamma=0

    [t0,x0,\
    T,theta,theta_fd,sigma,sigma_fd,\
    theta_aux,sigma_aux,sigma_aux_fd,\
    obs,sd,resamp_coef,l,d, N,seed,fd,B]=args

    
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
    bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
    [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
    bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #ori_pf_l[k,i]=np.sum(((pff.norm_logweights(log_weights,ax=1))*x_pr),axis=1)
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    lw_cond=log_weights[:,index]
    int_Gs_cond=int_Gs[:,index]
    ch_paths=np.zeros((B,int(T/d)))
    comp_pf_diffs=np.zeros((B,int(T/d)))
    comp_pf_l=np.zeros((B,2,int(T/d)))
    ch_weights=np.zeros((B,2,int(T/d)))
    grads=np.zeros((B,3))
    ch_whole_paths=np.zeros((B,2,int(T/d)))
    ch_whole_weights=np.zeros((B,2,int(T/d)))
    seed+=(int(T/d))*int(int(2**l*d-1))
    cond_whole_path=cond_path
    cond_whole_log_weights=cond_log_weights
    for b in range(B):

        [log_weights,x_pr,cond_log_weights,\
        cond_int_G,cond_path,seeds_cond,Grads]=\
        bdg.Grad_Cond_PF_bridge_new(lw_cond,int_Gs_cond,\
        cond_path,seeds_cond,t0,x0,\
        T,bdg.b_ou_1d,theta,theta_fd,bdg.Sig_ou_1d,sigma,sigma_fd,\
        bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,sigma_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd]],bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd],theta_aux],\
        bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd)
        """
        (lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
        fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,crossed=False):
        """
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        #comp_pf_diffs[b]=np.sum((x_pr_0*pff.norm_logweights(log_weights_0,ax=1)\
        #-x_pr_1*pff.norm_logweights(log_weights_1,ax=1)),axis=1)
        #comp_pf_l[b,1]=np.sum((pff.norm_logweights(log_weights_1,ax=1)*x_pr_1),axis=1)
        #comp_pf_l[b,0]=np.sum((pff.norm_logweights(log_weights_0,ax=1)*x_pr_0),axis=1)           
        grads[b]=Grads
    mcmc_mean=np.mean(ch_paths,axis=0)
    #pf_diffs[k,i]=np.mean(comp_pf_diffs**2,axis=0)
    #pf_l[k,i]=np.mean(comp_pf_l,axis=0)
    grads_mean=np.mean(grads,axis=0)
    return mcmc_mean,grads_mean


def Prl_Grad_chain(args):

    # This function is build so we can test the current Grad function without 
    # the need for SGD, meaning that this is basically the SGD function with 
    # gamma=0

    [t0,x0,\
    T,theta,theta_fd,sigma,sigma_fd,\
    theta_aux,sigma_aux,sigma_aux_fd,\
    obs,sd,resamp_coef,l,d, N,seed,fd,B]=args

    
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
    bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
    [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
    bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #ori_pf_l[k,i]=np.sum(((pff.norm_logweights(log_weights,ax=1))*x_pr),axis=1)
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    lw_cond=log_weights[:,index]
    int_Gs_cond=int_Gs[:,index]
    ch_paths=np.zeros((B,int(T/d)))
    comp_pf_diffs=np.zeros((B,int(T/d)))
    comp_pf_l=np.zeros((B,2,int(T/d)))
    ch_weights=np.zeros((B,2,int(T/d)))
    grads=np.zeros((B,3))
    ch_whole_paths=np.zeros((B,2,int(T/d)))
    ch_whole_weights=np.zeros((B,2,int(T/d)))
    seed+=(int(T/d))*int(int(2**l*d-1))
    cond_whole_path=cond_path
    cond_whole_log_weights=cond_log_weights
    for b in range(B):
        [log_weights,x_pr,cond_log_weights,\
        cond_int_G,cond_path,seeds_cond,Grads]=\
        bdg.Grad_Cond_PF_bridge_back_samp(lw_cond,int_Gs_cond,\
        cond_path,seeds_cond,t0,x0,\
        T,bdg.b_ou_1d,theta,theta_fd,bdg.Sig_ou_1d,sigma,sigma_fd,\
        bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,sigma_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd]],bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd],theta_aux],\
        bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd)
        """
        (lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
        fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,crossed=False):
        """
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        #comp_pf_diffs[b]=np.sum((x_pr_0*pff.norm_logweights(log_weights_0,ax=1)\
        #-x_pr_1*pff.norm_logweights(log_weights_1,ax=1)),axis=1)
        #comp_pf_l[b,1]=np.sum((pff.norm_logweights(log_weights_1,ax=1)*x_pr_1),axis=1)
        #comp_pf_l[b,0]=np.sum((pff.norm_logweights(log_weights_0,ax=1)*x_pr_0),axis=1)           
        grads[b]=Grads
    mcmc_mean=np.mean(ch_paths,axis=0)
    #pf_diffs[k,i]=np.mean(comp_pf_diffs**2,axis=0)
    #pf_l[k,i]=np.mean(comp_pf_l,axis=0)
    grads_mean=np.mean(grads,axis=0)
    return mcmc_mean,grads_mean




def Prl_Grad_chain_an(args):

    # This function is build so we can test the current Grad function without 
    # the need for SGD, meaning that this is basically the SGD function with 
    # gamma=0

    [t0,x0,\
    T,theta,theta_fd,sigma,sigma_fd,\
    theta_aux,sigma_aux,sigma_aux_fd,\
    obs,sd,resamp_coef,l,d, N,seed,fd,B]=args

    
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
    bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
    [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
    bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #ori_pf_l[k,i]=np.sum(((pff.norm_logweights(log_weights,ax=1))*x_pr),axis=1)
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    lw_cond=log_weights[:,index]
    int_Gs_cond=int_Gs[:,index]
    ch_paths=np.zeros((B,int(T/d)))
    comp_pf_diffs=np.zeros((B,int(T/d)))
    comp_pf_l=np.zeros((B,2,int(T/d)))
    ch_weights=np.zeros((B,2,int(T/d)))
    grads=np.zeros((B,3))
    ch_whole_paths=np.zeros((B,2,int(T/d)))
    ch_whole_weights=np.zeros((B,2,int(T/d)))
    seed+=(int(T/d))*int(int(2**l*d-1))
    cond_whole_path=cond_path
    cond_whole_log_weights=cond_log_weights
    for b in range(B):

        [log_weights,x_pr,cond_log_weights,\
        cond_int_G,cond_path,seeds_cond,Grads]=\
        bdg.Grad_Cond_PF_bridge_back_samp_an(lw_cond,int_Gs_cond,\
        cond_path,seeds_cond,t0,x0,\
        T,bdg.b_ou_1d,theta,theta_fd,bdg.Sig_ou_1d,sigma,sigma_fd,\
        bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,sigma_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd]],bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd],theta_aux],\
        bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd)
        """
        (lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,A_fd,Sig,fi,fi_fd,b_til,A_til,Sig_til,fi_til,\
        fi_til_fd,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,crossed=False):
        """
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        #comp_pf_diffs[b]=np.sum((x_pr_0*pff.norm_logweights(log_weights_0,ax=1)\
        #-x_pr_1*pff.norm_logweights(log_weights_1,ax=1)),axis=1)
        #comp_pf_l[b,1]=np.sum((pff.norm_logweights(log_weights_1,ax=1)*x_pr_1),axis=1)
        #comp_pf_l[b,0]=np.sum((pff.norm_logweights(log_weights_0,ax=1)*x_pr_0),axis=1)           
        grads[b]=Grads
    mcmc_mean=np.mean(ch_paths,axis=0)
    #pf_diffs[k,i]=np.mean(comp_pf_diffs**2,axis=0)
    #pf_l[k,i]=np.mean(comp_pf_l,axis=0)
    grads_mean=np.mean(grads,axis=0)
    return mcmc_mean,grads_mean


def Prl_C_Grad_chain(args):

    # This function is build so we can test the current Grad function without 
    # the need for SGD, meaning that this is basically the SGD function with 
    # gamma=0


    [t0,x0,\
    T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
    theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
    obs,sd,resamp_coef,l,d, N,seed,fd,B]=args
    
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
    bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
    [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
    bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed)
    weights=pff.norm_logweights(log_weights[-1,:])
    
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_path_0=cond_path
    cond_path_1=cond_path
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    seeds_cond_0=seeds_cond
    seeds_cond_1=seeds_cond
    ch_paths=np.zeros((B,2,int(T/d)))
    comp_pf_diffs=np.zeros((B,int(T/d)))
    comp_pf_l=np.zeros((B,2,int(T/d)))
    ch_weights=np.zeros((B,2,int(T/d)))
    grads=np.zeros((B,2,3))
    ch_whole_paths=np.zeros((B,2,int(T/d)))
    ch_whole_weights=np.zeros((B,2,int(T/d)))
    seed+=(int(T/d))*int(int(2**l*d-1))
    cond_whole_path=cond_path
    cond_whole_log_weights=cond_log_weights
    for b in range(B):
        
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        cond_int_G_0,cond_int_G_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,Grads_0,Grads_1]=\
        bdg.C_Grad_Cond_PF_bridge_back_samp(\
        cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,t0,x0,\
        T,bdg.b_ou_1d,theta,theta,theta_fd_0,theta_fd_1,bdg.Sig_ou_1d,sigma,sigma,sigma_fd_0,sigma_fd_1,\
        bdg.b_ou_aux,theta_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],[bdg.ou_sd,[theta_aux,sigma_aux]],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd_0]],[bdg.ou_sd,[theta_aux,sigma_aux_fd_1]],bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],[bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd_0],theta_aux],[bdg.ou_sd,[theta_aux,sigma_aux_fd_1],theta_aux],\
        bdg.rej_max_coup_ou, [theta_aux,sigma_aux,theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,sd,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],[theta_aux,sigma_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],[theta_aux,sigma_aux],bdg.Grad_log_G_new,l,d, N,seed,fd,crossed=False)

        seed+=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=[cond_path_0,cond_path_1]
        #comp_pf_diffs[b]=np.sum((x_pr_0*pff.norm_logweights(log_weights_0,ax=1)\
        #-x_pr_1*pff.norm_logweights(log_weights_1,ax=1)),axis=1)
        #comp_pf_l[b,1]=np.sum((pff.norm_logweights(log_weights_1,ax=1)*x_pr_1),axis=1)
        #comp_pf_l[b,0]=np.sum((pff.norm_logweights(log_weights_0,ax=1)*x_pr_0),axis=1)           
        grads[b]=np.array([Grads_0,Grads_1])
    #mcmc[k,i]=ch_paths
    mcmc_mean=np.mean(ch_paths,axis=0)
    grads_mean=np.mean(grads,axis=0)

    return mcmc_mean,grads_mean



def Prl_C_Grad_chain_an(args):

    # This function is build so we can test the current Grad function without 
    # the need for SGD, meaning that this is basically the SGD function with 
    # gamma=0


    [t0,x0,\
    T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
    theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
    obs,sd,resamp_coef,l,d, N,seed,fd,B]=args
    
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
    bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
    [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
    bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed)
    weights=pff.norm_logweights(log_weights[-1,:])
    
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_path_0=cond_path
    cond_path_1=cond_path
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    seeds_cond_0=seeds_cond
    seeds_cond_1=seeds_cond
    ch_paths=np.zeros((B,2,int(T/d)))
    comp_pf_diffs=np.zeros((B,int(T/d)))
    comp_pf_l=np.zeros((B,2,int(T/d)))
    ch_weights=np.zeros((B,2,int(T/d)))
    grads=np.zeros((B,2,3))
    ch_whole_paths=np.zeros((B,2,int(T/d)))
    ch_whole_weights=np.zeros((B,2,int(T/d)))
    seed+=(int(T/d))*int(int(2**l*d-1))
    cond_whole_path=cond_path
    cond_whole_log_weights=cond_log_weights
    for b in range(B):
        
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        cond_int_G_0,cond_int_G_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,Grads_0,Grads_1]=\
        bdg.C_Grad_Cond_PF_bridge_back_samp_an(\
        cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,t0,x0,\
        T,bdg.b_ou_1d,theta,theta,theta_fd_0,theta_fd_1,bdg.Sig_ou_1d,sigma,sigma,sigma_fd_0,sigma_fd_1,\
        bdg.b_ou_aux,theta_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],[bdg.ou_sd,[theta_aux,sigma_aux]],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd_0]],[bdg.ou_sd,[theta_aux,sigma_aux_fd_1]],bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],[bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd_0],theta_aux],[bdg.ou_sd,[theta_aux,sigma_aux_fd_1],theta_aux],\
        bdg.rej_max_coup_ou, [theta_aux,sigma_aux,theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,sd,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],[theta_aux,sigma_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],[theta_aux,sigma_aux],bdg.Grad_log_G_new,l,d, N,seed,fd,crossed=False)

        seed+=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=[cond_path_0,cond_path_1]
        #comp_pf_diffs[b]=np.sum((x_pr_0*pff.norm_logweights(log_weights_0,ax=1)\
        #-x_pr_1*pff.norm_logweights(log_weights_1,ax=1)),axis=1)
        #comp_pf_l[b,1]=np.sum((pff.norm_logweights(log_weights_1,ax=1)*x_pr_1),axis=1)
        #comp_pf_l[b,0]=np.sum((pff.norm_logweights(log_weights_0,ax=1)*x_pr_0),axis=1)           
        grads[b]=np.array([Grads_0,Grads_1])
    #mcmc[k,i]=ch_paths
    mcmc_mean=np.mean(ch_paths,axis=0)
    grads_mean=np.mean(grads,axis=0)
    return mcmc_mean,grads_mean



def Prl_PG_chain(args):
    # Particle Gibbs chain
    # This function is build so we can test the current smoothing distribution function without 
    # the need for SGD or gradient.

    [t0,x0,\
    T,theta,theta_fd,sigma,sigma_fd,\
    theta_aux,sigma_aux,sigma_aux_fd,\
    obs,sd,resamp_coef,l,d, N,seed,fd,B]=args

    np.random.seed(seed)
    [log_weights,int_Gs,x_pr]=bdg.PF_bridge(t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
    bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
    [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
    bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #ori_pf_l[k,i]=np.sum(((pff.norm_logweights(log_weights,ax=1))*x_pr),axis=1)
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d),2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)))*int(int(2**l*d-1))
    seeds_cond[:,1]=index*np.ones(int(T/d))
    lw_cond=log_weights[:,index]
    int_Gs_cond=int_Gs[:,index]
    ch_paths=np.zeros((B,int(T/d)))
    comp_pf_diffs=np.zeros((B,int(T/d)))
    comp_pf_l=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,2,int(T/d)))
    grads=np.zeros((B,3))
    ch_whole_paths=np.zeros((B,2,int(T/d)))
    ch_whole_weights=np.zeros((B,2,int(T/d)))
    seed+=(int(T/d))*int(int(2**l*d-1))
    cond_whole_path=cond_path
    cond_whole_log_weights=cond_log_weights
    for b in range(B):
        #[log_weights,x_pr,new_lw_cond,new_int_G_cond,new_x_cond,new_seeds_cond]
        [log_weights,x_pr,cond_log_weights,\
        cond_int_G,cond_path,seeds_cond]=\
        bdg.Cond_PF_bridge_back_samp(lw_cond,int_Gs_cond,\
        cond_path,seeds_cond,t0,x0,\
        T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,\
        bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
        bdg.ou_trans_den,[theta_aux,sigma_aux],\
        bdg.ou_trans_den,resamp_coef,l,d, N,seed)
        """
        (lw_cond,int_Gs_cond,x_cond,seeds_cond,t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
        r,r_pars,H,H_pars,sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
        prop_trans_den, resamp_coef, l, d,N,seed,crossed=False)
        """
        seed+=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        #comp_pf_diffs[b]=np.sum((x_pr_0*pff.norm_logweights(log_weights_0,ax=1)\
        #-x_pr_1*pff.norm_logweights(log_weights_1,ax=1)),axis=1)
        comp_pf_l[b]=np.sum((pff.norm_logweights(log_weights,ax=1)*x_pr),axis=1)
        #comp_pf_l[b,0]=np.sum((pff.norm_logweights(log_weights_0,ax=1)*x_pr_0),axis=1)           
        #grads[b]=Grads
    #mcmc_mean=np.mean(ch_paths,axis=0)
    #pf_diffs[k,i]=np.mean(comp_pf_diffs**2,axis=0)
    #pf_l[k,i]=np.mean(comp_pf_l,axis=0)
    #grads_mean=np.mean(grads,axis=0)
    return ch_paths,comp_pf_l # mcmc_mean ,grads_mean


def Prl_Unbiased(args):
     
    [t0,x0,T,b,A_in,A_fd_in,Sig,fi_in,fi_fd_in,b_til,A_til_in,Sig_til,fi_til_in,\
    fi_til_fd_in,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sing_sample_funct, sing_sample_pars,max_sample_funct,sample_pars,\
    obs,log_g_den,g_den_par_in, aux_trans_den,atdp,\
    Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,update_pars,resamp_coef, d,N,seed,fd_rate,\
    mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]=args


    np.random.seed(seed+1)
    # cumulative for the time discretization
    eLes=np.arange(l0,lmax+1)
    beta=beta_l
    q=4
    P0=(1+CL*np.sum((q+eLes[1:]-l0)*np.log(q+eLes[1:]-l0)**2/2**(beta*eLes[1:]))/\
    (CL0*(q+1)*np.log(q+1)**2))**(-1)
    
    l_cumu=bdg.P_l_cumu_gen(P0,lmax-l0+1,beta,l0)
    l=eLes[bdg.sampling(l_cumu)]
    # cumulative for the number of SGD steps
    beta=beta_p
    ePes=np.arange(0,pmax+1)
    eSes=s0*2**ePes
    P0=(1+CP*np.sum((ePes[1:]+q)*np.log(ePes[1:]+q)**2/eSes[1:]**(beta))\
    /(CP0*(q+1)*np.log(1+q)**2))**(-1)
    
    p_cumu=bdg.P_p_cumu_gen(P0,pmax,beta,s0)
    p=bdg.sampling(p_cumu)
    #print("P0p, p_cumu, p",P0,p_cumu,p)
    #print(CP0/P0,CP/(p_cumu[1]-p_cumu[0])) 
    pars=np.zeros((2,2,3))
    #print("l and p are: ",l,p)
    #print("The parameters are: ",l,p)
    # The first dimensions of pars are for the two levels of time discretization. 
    
    if l==l0:
        SGD_steps=eSes[p]
        """
        SGD_bridge(t0,x0,T,b,A_0,A_fd_0,Sig,fi_0,fi_fd_0,b_til,A_til_0,Sig_til,fi_til_0,\
        fi_til_fd_0,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sample_funct,sample_pars,\
        obs,log_g_den,g_den_par_0, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha, \
        crossed=False)
        """


        parameters=bdg.SGD_bridge(t0,x0,T,b,A_in,A_fd_in,Sig,fi_in,fi_fd_in,b_til,A_til_in,Sig_til,fi_til_in,\
        fi_til_fd_in,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,sing_sample_funct,sing_sample_pars,\
        obs,log_g_den,g_den_par_in, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha,update_pars)[1]
        pars[1,1]=parameters[eSes[p]]
        if p!=0:
            pars[1,0]=parameters[eSes[p-1]]
    else:
        SGD_steps=eSes[p]
        parameters=bdg.C_SGD_bridge(t0,x0,T,b,A_in,A_fd_in,Sig,fi_in,fi_fd_in,b_til,A_til_in,Sig_til,fi_til_in,\
        fi_til_fd_in,r,r_pars,r_pars_fd,H,H_pars,H_pars_fd,max_sample_funct,sample_pars,\
        obs,log_g_den,g_den_par_in, aux_trans_den,atdp,\
        Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par,\
        Grad_log_G,update_pars,resamp_coef, l, d,N,seed,fd_rate/2**(l-l0-1),\
        mcmc_links,SGD_steps,gamma, alpha)
        pars[1,1]=parameters[3][eSes[p]]
        pars[0,1]=parameters[2][eSes[p]]
        if p!=0:
            pars[1,0]=parameters[3][eSes[p-1]]
            pars[0,0]=parameters[2][eSes[p-1]]
    
    return pars,(l,p)


def Prl_Unbiased_test(args):
     
    [seed,K, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]=args


    np.random.seed(seed+1)
    # cumulative for the time discretization
    eLes=np.arange(l0,lmax+1)
    beta=beta_l
    P0=(1+CL*2**(beta*(l0+1))*np.sum((eLes[1:]+1)*np.log(eLes[1:]+1)**2/2**(beta*eLes[1:]))/\
    (CL0*(l0+2)*np.log(l0+2)**2))**(-1)
    
    l_cumu=bdg.P_l_cumu_gen(P0,lmax-l0+1,beta,l0)
    l=eLes[bdg.sampling(l_cumu)]
    # cumulative for the number of SGD steps
    beta=beta_p
    ePes=np.arange(0,pmax+1)
    eSes=s0*2**ePes
    P0=(1+CP*eSes[1]**(beta)*np.sum((ePes[1:]+1)*np.log(ePes[1:]+1)**2/eSes[1:]**(beta))\
    /(CP0*(2)*np.log(2)**2))**(-1)
    
    p_cumu=bdg.P_p_cumu_gen(P0,pmax,beta,s0)
    #print("The l cumulative is:",l_cumu)    
    #print("The p cumulative is:",p_cumu)
    p=bdg.sampling(p_cumu)
    #print("P0p, p_cumu, p",P0,p_cumu,p)
    #print(CP0/P0,CP/(p_cumu[1]-p_cumu[0])) 
    pars=np.zeros((2,K))
    #print("l and p are: ",l,p)     
    #print("The cumulative distribution are: ",l_cumu,p_cumu)
    #print("l and p are: ",l,p)
    #print("The parameters are: ",l,p)
    # The first dimensions of pars are for the two levels of time discretization. 
    
    if l==l0: 
        pars[1]=np.sqrt(2)**(-l)+np.sqrt(2)**(-l)*np.random.uniform(-np.sqrt(3),np.sqrt(3),K)
    
    else:
        pars[1]=np.sqrt(2)**(-l)+np.sqrt(2)**(-l)*np.random.uniform(-np.sqrt(3),np.sqrt(3),K)
        pars[0]=np.sqrt(2)**(-l+1)


    return pars,(l,p)

def fibonacci(n):
    if n < 0:
        raise ValueError("Input should be a non-negative integer.")
    a, b = 0, 1
    for _ in range(n):
        a, b = b, a + b
    return a



def Prl_PF_bridge(args):

    [t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, resamp_coef, l, d,N,seed]=args
    #(T,xin,b_ou,A,Sig_ou,fi,obs,obs_time,l,d,N,dim,resamp_coef,g_den,g_par,Lambda,Lamb_par):
    # ARGUMENTS: the argument of the Kenel x0 rank 1 dims (dim) 
    # the drift and diffusion are b and Sig, respectively, and they take
    # x(either a (N) dimensional or (N,N) dimensional array) and A as arguments for the drift and x and fi for the diffusion.
    # the level of discretization l, the distance of resampling, the number of
    # particles N.
    # Grad_b is a function that takes (x,A) as argument and computes the gradnient of b wrt the 
    # parameters A, and evaluates it a (x,A).
    # b_til,A_til,Sig_til,fi_til, are the analogous functions for the auxiliar process.
    # a difference is that their arguments are (t,x) for the drift and (t,x,fi_til) for the diffusion.
    # r is the function that computes the gradient of the log of the kernel of the auxiliar process
    # and it takes (t,x,T,x_pr,r_pars) as arguments.
    # H is the function that computes the Hessian of the log of the kernel of the auxiliar process
    # and it takes (t,x,T,x_pr,H_pars) as arguments.
    # crossed is the boolean that indicates if we need the computations for the crossed terms that 
    # are needed for the smoother.

    log_weights=np.zeros((int(T/d),N))
    x_pr=np.zeros((int(T/d),N))
    xs=np.zeros((int(T*2**l*d),N))
    int_Gs=np.zeros((int(T/d),N))                      
    x_new=x0
    for i in range(int(T/d)):
        tf=t0+(i+1)*d
        ti=t0+(i)*d
        np.random.seed(seed+i*int(2**l*d-1))
        x_pr[i]=sample_funct(x_new,N,d,sample_pars)
        # what parameters do we need in order to make the auxiliar density general?
        # x_new,  d, t, x_pr,tf
        # aux_trans_den(t0,x0,T,x_pr,atdp)
        # atdp stands for auxiliar transition density parameters. 
        int_G=bdg.Bridge_1d(ti,x_new,tf,x_pr[i],b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
        r,r_pars,H,H_pars,l,d,N, seed+i*int(int(2**l*d-1)),crossed=False)
        int_Gs[i]=int_G
        #print(xi.shape)
        #print(yi,obti-i*d,)
        #print(x_new,xi)
        #print(xi)
        #Things that could be wrong
        # observations, x_new, weights
        #observations seem to be fine

        #xs[2**l*d*i:2**l*d*(i+1)]=x[:-1]

        #print("other parameteres are:",ti,x_new,tf,x_pr[i] )
        #print("atdp is ", atdp)
        #print("object is: ", aux_trans_den(ti,x_new,tf,x_pr[i],atdp))

        log_weights[i]=log_weights[i]+int_G+log_g_den(obs[i],x_pr[i],g_den_par)\
        +np.log(aux_trans_den(ti,x_new,tf,x_pr[i],atdp))-np.log(prop_trans_den(ti,x_new,tf,x_pr[i],sample_pars))
        weights=pff.norm_logweights(log_weights[i])
        #print(yi,weights)
        #seed_val=i
        #print(weights.shape)
        x_last=x_pr[i]
        ESS=0
        #print(ESS,resamp_coef*N)
        if ESS<resamp_coef*N:
            #print("resampling at time ",i)
        #if True==False:
            #[part0,part1,x0_new,x1_new]=max_coup_sr(w0,w1,N,xi0[-1],xi1[-1],dim)
            #print(x_new.shape)
            
            [part_resamp, x_new]=bdg.multi_samp_exp(weights,N,x_last,1)
            log_weights[:i+1]=log_weights[:i+1,part_resamp]
            int_Gs[:i+1]=int_Gs[:i+1,part_resamp]
            x_pr[:i+1]=x_pr[:i+1,part_resamp]
                
            #x_new=multi_samp_exp(weights,N,x_last,1)[1]
            #print(x_new.shape)
        else:
            #print("time is",i)
            x_new=x_last
            if i< int(T/d)-1:
                log_weights[i+1]=log_weights[i]
        #print(i)
        
       #x_new=sr(weights,N,x_pf[i],dim)[1]
    #weights=pff.norm_logweights(log_weights,ax=1)
    x_pf=np.mean(x_pr,axis=1)
    #Filter
    #spots=np.arange(d_steps,2**l*T+1,d_steps,dtype=int)
    #x_pf=x[spots]
    #weights=norm_logweights(log_weights,ax=1)

    #print(x_pf.shape,weights.shape)
    #suma=np.sum(x_pf[:,:,1]*weights,axis=1)
    return x_pf


def Prl_Gen_PF_bridge(args):

    [in_dist,in_dist_pars, b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, resamp_coef, l, d,N,seed] =args
    
    # ARGUMENTS: 
    # Intead of T(as in the function PF_bidge) we use the lenght of the observations obs_time. 
    # the argument of the Kenel x0 rank 1 dims (dim) 
    # the drift and diffusion are b and Sig, respectively, and they take
    # x(either a (N) dimensional or (N,N) dimensional array) and A as arguments for the drift and x and fi for the diffusion.
    # the level of discretization l, the distance of resampling, the number of
    # particles N.
    # Grad_b is a function that takes (x,A) as argument and computes the gradnient of b wrt the 
    # parameters A, and evaluates it a (x,A).
    # b_til,A_til,Sig_til,fi_til, are the analogous functions for the auxiliar process.
    # a difference is that their arguments are (t,x) for the drift and (t,x,fi_til) for the diffusion.
    # r is the function that computes the gradient of the log of the kernel of the auxiliar process
    # and it takes (t,x,T,x_pr,r_pars) as arguments.
    # H is the function that computes the Hessian of the log of the kernel of the auxiliar process
    # and it takes (t,x,T,x_pr,H_pars) as arguments.
    # crossed is the boolean that indicates if we need the computations for the crossed terms that 
    # are needed for the smoother.
    T=len(obs_times)*d # Here we change this in order to simplify the changes that we make to 
    # the funciton PF_bridge in order to get the current function.
    log_weights=np.zeros((int(T/d),N))
    x_pr=np.zeros((int(T/d),N))
    int_Gs=np.zeros((int(T/d),N))
    #sampling   
    np.random.seed(seed)                   
    x_new=in_dist(in_dist_pars,N)
    x_pr[0]= copy.deepcopy(x_new)
    log_weights[0]=log_g_den(obs[0],x_pr[0],g_den_par,crossed=False)
    # resampling
    weights=pff.norm_logweights(log_weights[0])
    [part_resamp, x_new]=bdg.multi_samp_exp(weights,N,x_new,1)
    log_weights[0]=log_weights[0,part_resamp]
    #int_Gs[0]=int_Gs[0,part_resamp]
    #x_pr[0]=x_pr[0,part_resamp]
    means=np.zeros((int(T/d)))
    sms=np.zeros((int(T/d)))    
    # here we make the first step which is different from the rest.
    Deltas=bdg.get_deltas(obs_times)
    means[0]=np.sum(x_pr[0]*weights)
    sms[0]=np.sum(x_pr[0]**2*weights)
    for i in range(int(T/d)-1):

        tf=obs_times[i+1]
        ti=obs_times[i]
        np.random.seed(seed+i*int(2**l*d-1))
        x_pr[i+1]=sample_funct(x_new,N,tf-ti,sample_pars)
        pars_0=[A,fi]
        [A_til,fi_til,r_pars,H_pars,\
        atdp]=update_func(pars_0,pars_0,ti,x_new,x_new,tf,x_pr[i+1],x_pr[i+1],levels=1)
        # what parameters do we need in order to make the auxiliar density general?
        # x_new,  d, t, x_pr,tf
        # aux_trans_den(t0,x0,T,x_pr,atdp)
        # atdp stands for auxiliar transition density parameters. 
        # Here we need to update A_til and fi_til to consider both x0 and x_pr
        int_G=bdg.Bridge_1d(ti,x_new,tf,x_pr[i+1],b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
        r,r_pars,H,H_pars,l,tf-ti,N,seed+i*int(int(2**l*d-1)),crossed=False,Dt=Deltas[i]/2**l)
        
        #print(xi.shape)
        #print(yi,obti-i*d,)
        #print(x_new,xi)
        #print(xi)
        #Things that could be wrong
        # observations, x_new, weights
        #observations seem to be fine
        #xs[2**l*d*i:2**l*d*(i+1)]=x[:-1]

        #print("other parameteres are:",ti,x_new,tf,x_pr[i] )
        #print("atdp is ", atdp)
        #print("object is: ", aux_trans_den(ti,x_new,tf,x_pr[i],atdp))
        log_weights[i+1]=int_G+log_g_den(obs[i+1],x_pr[i+1],g_den_par)\
        +np.log(aux_trans_den(ti,x_new,tf,x_pr[i+1],atdp))-np.log(prop_trans_den(ti,x_new,tf,x_pr[i+1],sample_pars))
        weights=pff.norm_logweights(log_weights[i+1])
        #print(yi,weights)
        #seed_val=i
        #print(weights.shape)
        x_last=x_pr[i+1]
        ESS=0
        means[i+1]=np.sum(x_pr[i+1]*weights)
        sms[i+1]=np.sum(x_pr[i+1]**2*weights)
        #if i==0:
        #    print("l is ",l,"int_G  is",int_G,"mean is ",means[i+1],"sm is ",sms[i+1])         
        #int_Gs[i]=int_G

        #print(ESS,resamp_coef*N)
        if ESS<resamp_coef*N:
            #print("resampling at time ",i)
        #if True==False:
            #[part0,part1,x0_new,x1_new]=max_coup_sr(w0,w1,N,xi0[-1],xi1[-1],dim)
            #print(x_new.shape)
            
            [part_resamp, x_new]=bdg.multi_samp_exp(weights,N,x_last,1)
            #log_weights[:i+2]=log_weights[:i+2,part_resamp]
            #int_Gs[:i+2]=int_Gs[:i+2,part_resamp]
            #x_pr[:i+2]=x_pr[:i+2,part_resamp]
            #x_new=multi_samp_exp(weights,N,x_last,1)[1]
            #print(x_new.shape)
        else:
            #print("time is",i)
            x_new=x_last
            if i< int(T/d)-1:
                log_weights[i+1]=log_weights[i]
        #print(i)
        
       #x_new=sr(weights,N,x_pf[i],dim)[1]
    #weights=np.reshape(norm_logweights(log_weights,ax=1),(int(T/d),N,1))
    #pf=np.sum(weights*x_pf,axis=1)
    #Filter
    #spots=np.arange(d_steps,2**l*T+1,d_steps,dtype=int)
    #x_pf=x[spots]
    #weights=norm_logweights(log_weights,ax=1)

    #print(x_pf.shape,weights.shape)
    #suma=np.sum(x_pf[:,:,1]*weights,axis=1)
    return [means,sms]




def Prl_Gen_C_PF_bridge(args):

    [in_dist,in_dist_pars, b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    update_func,max_sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, ind_prop_trans_par,resamp_coef, l, d,N,seed]=args
    #(t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    #max_sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    #prop_trans_den,ind_prop_trans_par, resamp_coef, l, d,N,seed,crossed=False):
    
    # ARGUMENTS: 

    T=len(obs_times)*d
    log_weights_0=np.zeros((int(T/d),N))
    log_weights_1=np.zeros((int(T/d),N))
    x_pr_0=np.zeros((int(T/d),N))
    x_pr_1=np.zeros((int(T/d),N))
    #xs=np.zeros((int(T*2**l*d),N))
    int_Gs_0=np.zeros((int(T/d),N))                      
    int_Gs_1=np.zeros((int(T/d),N))
    
    np.random.seed(seed)                   
    x_new_0=in_dist(in_dist_pars,N)
    x_pr_0[0]= copy.deepcopy(x_new_0)
    log_weights_0[0]=log_g_den(obs[0],x_pr_0[0],g_den_par,crossed=False)
    x_new_1=copy.deepcopy(x_new_0)
    x_pr_1[0]= copy.deepcopy(x_new_1)
    log_weights_1[0]=log_g_den(obs[0],x_pr_1[0],g_den_par,crossed=False)

    # resampling
    weights_0=pff.norm_logweights(log_weights_0[0])
    [part_resamp, x_new_0]=bdg.multi_samp_exp(weights_0,N,x_new_0,1)
    log_weights_0[0]=log_weights_0[0,part_resamp]
    #int_Gs[0]=int_Gs[0,part_resamp]
    x_pr_0[0]=x_pr_0[0,part_resamp]
    log_weights_1[0]=copy.deepcopy(log_weights_1[0,part_resamp])

    x_pr_1[0]= copy.deepcopy(x_new_0)
    x_new_1= copy.deepcopy(x_new_0)

    meanss=np.zeros((2,int(T/d)))
    smss=np.zeros((2,int(T/d)))

    seeds_0_wp=np.zeros((int(T/d)-1,2,N),dtype=int)
    seeds_1_wp=np.zeros((int(T/d)-1,2,N),dtype=int)


    meanss[:,0]=np.sum(x_pr_0[0]*weights_0)
    smss[:,0]=np.sum(x_pr_0[0]**2*weights_0)
    
    Deltas=bdg.get_deltas(obs_times)
    
    for i in range(int(T/d)-1):
        tf=obs_times[i+1]
        ti=obs_times[i]
        x_pr_0[i+1],x_pr_1[i+1]=max_sample_funct(x_new_0,x_new_1,N,tf-ti,sample_pars)

        np.random.seed(seed+i*int(2**l*d-1))
        pars_0=[A,fi]
        pars_1=[A,fi]
        [A_til_0,A_til_1,fi_til_0,fi_til_1,r_pars_0,r_pars_1,H_pars_0,H_pars_1,\
         atdp_0,atdp_1]=update_func(pars_0,pars_1,ti,x_new_0,x_new_1,tf,x_pr_0[i+1],x_pr_1[i+1],levels=2)
        #[A_til,fi_til,r_pars,H_pars,\
        #atdp]=update_func(pars_0,pars_0,ti,x_new,x_new,tf,x_pr[i+1],x_pr[i+1],levels=1)

        # what parameters do we need in order to make the auxiliar density general?
        # x_new,  d, t, x_pr,tf
        # aux_trans_den(t0,x0,T,x_pr,atdp)
        # atdp stands for auxiliar transition density parameters. 
        seeds_0_wp[i,1,:]=np.copy(np.arange(N))
        seeds_1_wp[i,1,:]=np.copy(np.arange(N))
        seeds_0_wp[i,0,:]=seed+i*int(2**l*d-1)
        seeds_1_wp[i,0,:]=seed+i*int(2**l*d-1)
        """
        (t0,x0_0,x0_1,T,x_p_0,x_p_1,b,A_0,A_1,Sig,fi_0,fi_1,b_til,A_til_0,A_til_1,\
        Sig_til,fi_til_0,fi_til_1,r,r_pars_0,r_pars_1,H,H_pars_0,H_pars_1,l,d,N,seed\
        ,crossed=False,backward=False,j_0=False,j_1=False,fd=False,N_pf=False,cond_seed_0=False,cond_seed_1=False):
        """
        int_G_0,int_G_1=bdg.C_Bridge_1d(ti,x_new_0,x_new_1,tf,x_pr_0[i+1],x_pr_1[i+1],b,A,A,Sig,fi,fi,b_til,A_til_0,A_til_1,\
        Sig_til,fi_til_0,fi_til_1,r,r_pars_0,r_pars_1,H,H_pars_0,H_pars_1,l,d,N,seed+i*int(2**l*d-1),Dt=Deltas[i]/2**l)
        #int_G=Bridge_1d(ti,x_new,tf,x_pr[i],b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
        #r,r_pars,H,H_pars,l,d,N, seed+i*int(int(2**l*d-1)),crossed=False)
        #int_Gs[i]=int_G
        #print(xi.shape)
        #print(yi,obti-i*d,)
        #print(x_new,xi)
        #print(xi)
        #Things that could be wrong
        # observations, x_new, weights
        #observations seem to be fine

        #xs[2**l*d*i:2**l*d*(i+1)]=x[:-1]

        #print("other parameteres are:",ti,x_new,tf,x_pr[i] )
        #print("atdp is ", atdp)
        #print("object is: ", aux_trans_den(ti,x_new,tf,x_pr[i],atdp))

        log_weights_0[i+1]=int_G_0+log_g_den(obs[i+1],x_pr_0[i+1],g_den_par)\
        +np.log(aux_trans_den(ti,x_new_0,tf,x_pr_0[i+1],atdp_0))-np.log(prop_trans_den(ti,x_new_0,tf,x_pr_0[i+1],ind_prop_trans_par))
        weights_0=pff.norm_logweights(log_weights_0[i+1])
        
        log_weights_1[i+1]=int_G_1+log_g_den(obs[i+1],x_pr_1[i+1],g_den_par)\
        +np.log(aux_trans_den(ti,x_new_1,tf,x_pr_1[i+1],atdp_1))-np.log(prop_trans_den(ti,x_new_1,tf,x_pr_1[i+1],ind_prop_trans_par))
        weights_1=pff.norm_logweights(log_weights_1[i+1])

        """ 
        log_weights[i]=log_weights[i]+int_G+log_g_den(obs[i],x_pr[i],g_den_par,crossed=crossed)\
        +np.log(aux_trans_den(ti,x_new,tf,x_pr[i],atdp,crossed=crossed))-np.log(prop_trans_den(ti,x_new,tf,x_pr[i],sample_pars,crossed=crossed))
        weights=pff.norm_logweights(log_weights[i])
        """
        #print(yi,weights)
        #seed_val=i 
        #print(weights.shape)
        x_last_0=x_pr_0[i+1]
        x_last_1=x_pr_1[i+1]
        
        #ESS=1/np.sum(weights**2)
        #ESS=1/np.sum(weights**2)
        
        meanss[0,i+1]=np.sum(x_pr_0[i+1]*weights_0)
        smss[0,i+1]=np.sum(x_pr_0[i+1]**2*weights_0)
        meanss[1,i+1]=np.sum(x_pr_1[i+1]*weights_1)
        smss[1,i+1]=np.sum(x_pr_1[i+1]**2*weights_1)

        """
        [part0,part1,x0_new,x1_new]=max_coup_multi(w0,w1,N,xi0[-1],xi1[-1],dim)
        """
        [part_resamp_0,part_resamp_1, x_new_0,x_new_1]=bdg.max_coup_multi\
        (weights_0,weights_1,N,x_last_0,x_last_1,1)
        #print(part_resamp_0,part_resamp_1)
        #log_weights_0[:i+2]=log_weights_0[:i+2,part_resamp_0]
        #int_Gs_0[:i+2]=int_Gs_0[:i+2,part_resamp_0]
        #x_pr_0[:i+2]=x_pr_0[:i+2,part_resamp_0]
        #log_weights_1[:i+2]=log_weights_1[:i+2,part_resamp_1]
        #int_Gs_1[:i+2]=int_Gs_1[:i+2,part_resamp_1]
        #x_pr_1[:i+2]=x_pr_1[:i+2,part_resamp_1]
#
        #seeds_0_wp[:i+1]=np.copy(seeds_0_wp[:i+1,:,part_resamp_0])
        #seeds_1_wp[:i+1]=np.copy(seeds_1_wp[:i+1,:,part_resamp_1])
        
        #x_new=multi_samp_exp(weights,N,x_last,1)[1]
        #print(x_new.shape)
        
        
       #x_new=sr(weights,N,x_pf[i],dim)[1]
    #weights=np.reshape(norm_logweights(log_weights,ax=1),(int(T/d),N,1))
    #pf=np.sum(weights*x_pf,axis=1)
    #Filter
    #spots=np.arange(d_steps,2**l*T+1,d_steps,dtype=int)
    #x_pf=x[spots]
    #weights=norm_logweights(log_weights,ax=1)

    #print(x_pf.shape,weights.shape)
    #suma=np.sum(x_pf[:,:,1]*weights,axis=1)
    return [meanss,smss]


def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]

    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


def Prl_Gen_C_smoother_logarithmic(args):


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means



def log_g_den_nonin(obs,x_pr,g_den_par,crossed=False):

    return 1


def Prl_Grad_smooth(args):

    [N,seed,B,l,obs,obs_times]=args
    #the1=2
    #the2=0.1
    #the3=0.5
    #the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean


def Prl_Gen_Grad_smooth_2(args):


    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the4,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)




        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean



def Prl_Gen_C_Grad_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,\
    the1,the2,the3,the4,the3_fd,fd]=args
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    

    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]

    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
 

    d=1
    T=len(obs_times)*d
    resamp_coef=1
    A_til=dist_params
    fi_til=the3
    r_pars=1
    H_pars=1

    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    
    ch_paths=np.zeros((2,B,int(T/d)))
    Grads=np.zeros((2,B,4))
    ch_weights=np.zeros((2,B,int(T/d)))
    
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        print(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,\
        Grads_0,Grads_1]=\
        bdg.Gen_C_Grad_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,\
        coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,dist_params_fd_0,\
        dist_params_fd_1,bdg.Sig_gbm_1d,the3,the3,the3_fd,the3_fd_1,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3],\
        bdg.Grad_log_g_nonin, resamp_coef, l, d,N,seed,fd)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        Grads[:,b]=np.array([Grads_0,Grads_1])
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    
    return Grads,ch_paths 




def Prl_Gen_SGD_bridge(args):

    # new parameters:
    # A_0, fi_0, g_par_0: initial parameters for the SGD algorithm.
    # mcmc_links, SGD_steps: number of mcmc links and number of SGD steps.
    # gamma, alpha
    [in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
    r,H,update_func,sample_funct,sample_pars,\
    obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
    Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
    mcmc_links,SGD_steps,gamma,gammas, alpha,ast, update_pars]=args
 
    # new parameters:
    # A_0, fi_0, g_par_0: initial parameters for the SGD algorithm.
    # mcmc_links, SGD_steps: number of mcmc links and number of SGD steps.
    # gamma, alpha
    B=mcmc_links*SGD_steps
    T=int(len(obs_times)*d)
    K=2*(2**ast-1)
    mcmc_mean=np.zeros((int(T/d)))
    resamp_coef=1
    pars=np.zeros((SGD_steps+1+ast,4))
    
    pars[0,:3]=in_dist_pars
    pars[0,3]=g_den_par
    Grads_test=np.zeros((B+K,4))
    # The following parameters are set to 1 since they are not actually used in the function
    # PF_bridge(the only instance where they are called), they are just vestigial parameters from
    # previous implementations.
    A_til=1
    fi_til=1
    r_pars=1
    H_pars=1
    atdp=1
    #g_par
    # The next part might depend on the specific dynamics of the example
    # since we have to define the finite difference for the parameters and 
    # A, fi, g_par might be a list parameters, each. 
    
    # Similarly for the auxiliar parameters, these might be related to 
    # the parameters of the dynamics.
    np.random.seed(seed)
    [log_weights,int_Gs,x_pr,indices]=bdg.Gen_PF_bridge(in_dist,in_dist_pars, b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, resamp_coef, l, d,N,seed)
    index=np.random.choice(np.array(range(N)))
    
    cond_path=x_pr[:,index].copy()
    cond_log_weights=log_weights[:,index].copy()
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=indices[:,index].copy()
    
    Grads_mcmc=np.zeros((SGD_steps+ast,4))
    ch_paths=np.zeros((B+K,int(T/d)))
    
    n=1
    the1=A[0]
    the2=A[1]
    the3=A[2]
    the4=g_den_par

    mcmc_step=2**ast
    count=0

    [in_dist_pars,A_fd,fi_fd,sample_pars]=update_pars([A,g_den_par],[A,g_den_par],fd_rate,levels=1)

    for b_ind in range(B+K):
        # the varaible int_Gs is meant to have the record of int_G of the 
        # backward sampled path.
        #print("mcmc iteration is:", b)
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(b_ind)

        [log_weights,x_pr,cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads_t]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(in_dist,in_dist_pars,Grad_log_in_dist,\
        cond_path,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        fd_rate)

        Grads_test[b_ind]=Grads_t
        ch_paths[b_ind]=cond_path

        if n<=ast:
            if b_ind+1==count+int(mcmc_step*2/2**n):
                count+=int(mcmc_step*2/2**n)
        

                Grad_mcmc=np.mean(Grads_test[b_ind+1-int(mcmc_step*2/2**n):b_ind+1],axis=0)
                Grads_mcmc[n-1]=Grad_mcmc

                the1*=np.exp(gamma*gammas[0]*Grad_mcmc[0]*the1/n**(0.5+alpha))
                #fi+=gamma*Grad_mcmc[1]/n**(0.5+alpha)
                the2*=np.exp(gamma*gammas[1]*Grad_mcmc[1]*the2/n**(0.5+alpha))
                #g_den_par+=gamma*Grad_mcmc[2]/n**(0.5+alpha)
                the3*=np.exp(gamma*gammas[2]*Grad_mcmc[2]*the3/n**(0.5+alpha))
                the4*=np.exp(gamma*gammas[3]*Grad_mcmc[3]*the4/n**(0.5+alpha))


                pars[n]=np.array([the1,the2,the3,the4]) 
                A=pars[n,:3].copy()
                fi=pars[n,2]
                g_den_par=pars[n,3]

                the1s=pars[n,0]
                the2s=pars[n,1]
                the3s=pars[n,2]
                the4s=pars[n,3]
                w=the3s**2/2+the1s
                xi=the2s
                sigma=the3s
                alpha_pars=2*w/sigma**2-1
                theta=sigma**2/(2*xi)
                #print(" The pars are: ",the1s,the2s,the3s,the4s)    
                #print("The mean is: ",alpha_pars*theta)  
                #print("The gradeint is:",Grad_mcmc)
                #print("The value of n is: ",n)  


                pars_0=A,g_den_par
                [in_dist_pars,A_fd,fi_fd,sample_pars]=update_pars(pars_0,pars_0,fd_rate,levels=1)

                n+=1

        else:
            if (b_ind+1-count)%mcmc_links==0:
                Grad_mcmc=np.mean(Grads_test[b_ind+1-mcmc_links:b_ind+1],axis=0)
                Grads_mcmc[n-1]=Grad_mcmc

                the1*=np.exp(gamma*gammas[0]*Grad_mcmc[0]*the1/n**(0.5+alpha))
                #fi+=gamma*Grad_mcmc[1]/n**(0.5+alpha)
                the2*=np.exp(gamma*gammas[1]*Grad_mcmc[1]*the2/n**(0.5+alpha))
                #g_den_par+=gamma*Grad_mcmc[2]/n**(0.5+alpha)
                the3*=np.exp(gamma*gammas[2]*Grad_mcmc[2]*the3/n**(0.5+alpha))
                the4*=np.exp(gamma*gammas[3]*Grad_mcmc[3]*the4/n**(0.5+alpha))


                pars[n]=np.array([the1,the2,the3,the4]) 
                A=pars[n,:3].copy()
                fi=pars[n,2]
                g_den_par=pars[n,3]

                the1s=pars[n,0]
                the2s=pars[n,1]
                the3s=pars[n,2]
                the4s=pars[n,3]
                w=the3s**2/2+the1s
                xi=the2s
                sigma=the3s
                alpha_pars=2*w/sigma**2-1
                theta=sigma**2/(2*xi)
                #print(" The pars are: ",the1s,the2s,the3s,the4s)    
                #print("The mean is: ",alpha_pars*theta)  
                #print("The gradeint is:",Grad_mcmc)
                #print("The value of n is: ",n)  


                pars_0=A,g_den_par
                [in_dist_pars,A_fd,fi_fd,sample_pars]=update_pars(pars_0,pars_0,fd_rate,levels=1)

                n+=1
            
        
    return ch_paths,pars , Grads_mcmc 



def Prl_Gen_C_SGD_bridge(args):

    # new parameters:
    # A_0, fi_0, g_par_0: initial parameters for the SGD algorithm.
    # mcmc_links, SGD_steps: number of mcmc links and number of SGD steps.
    # gamma, alpha

    [in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
    r,H,update_func,max_sample_funct,sample_pars,\
    obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
    Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
    mcmc_links,SGD_steps,gamma,gammas, alpha, ast, update_pars]=args

    B=mcmc_links*SGD_steps
    K=2*(2**ast-1)
    T=int(len(obs_times)*d)
    mcmc_mean=np.zeros((2,int(T/d)))
    resamp_coef=1
    pars_0=np.zeros((SGD_steps+1+ast,4))
    pars_1=np.zeros((SGD_steps+1+ast,4))
    
    pars_0[0,:3]=A
    pars_1[0,:3]=A
    pars_0[0,3]=g_den_par
    pars_1[0,3]=g_den_par
    Grads_test_0=np.zeros((B+K,4))
    Grads_test_1=np.zeros((B+K,4))
    
    # The following parameters are set to 1 since they are not actually used in the function
    # PF_bridge(the only instance where they are called), they are just vestigial parameters from
    # previous implementations.
    A_til=1
    fi_til=1
    r_pars=1
    H_pars=1
    atdp=1
    #g_par
    # The next part might depend on the specific dynamics of the example
    # since we have to define the finite difference for the parameters and 
    # A, fi, g_par might be a list parameters, each. 
    
    # Similarly for the auxiliar parameters, these might be related to 
    # the parameters of the dynamics.
    np.random.seed(seed)
    [log_weights_0,log_weights_1,int_Gs_0,int_Gs_1,x_pr_0,x_pr_1,seeds_0_wp,seeds_1_wp]=\
    bdg.Gen_C_PF_bridge(in_dist,in_dist_pars, b,A,Sig,fi,b_til,A_til,\
    Sig_til,fi_til,r,r_pars,H,H_pars,\
    update_func,max_sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, ind_prop_trans_par,resamp_coef, l, d,N,seed)


    index=np.random.choice(np.array(range(N)))
    
    cond_path_0=x_pr_0[:,index].copy()
    cond_path_1=x_pr_1[:,index].copy()

    #cond_log_weights_0=log_weights_0[:,index].copy()
    #cond_log_weights_1=log_weights_1[:,index].copy()
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond_0=seeds_0_wp[:,:,index].copy()
    seeds_cond_1=seeds_1_wp[:,:,index].copy()

    Grads_mcmc_0=np.zeros((SGD_steps+ast,4))
    Grads_mcmc_1=np.zeros((SGD_steps+ast,4))
    ch_paths_0=np.zeros((B+K,int(T/d)))
    ch_paths_1=np.zeros((B+K,int(T/d)))
    
    n=1
    the1_0=A[0]
    the2_0=A[1]
    the3_0=A[2]
    the4_0=g_den_par
    A_0=A
    fi_0=fi
    g_den_par_0=g_den_par

    the1_1=A[0]
    the2_1=A[1]
    the3_1=A[2]
    the4_1=g_den_par
    A_1=A
    fi_1=fi
    g_den_par_1=g_den_par

    mcmc_step=2**ast
    count=0


    [in_dist_pars,A_fd_0,A_fd_1,fi_fd_0,fi_fd_1,sample_pars,\
        ind_prop_trans_par_0,ind_prop_trans_par_1]=update_pars([A,g_den_par],[A,g_den_par],fd_rate,levels=2)
    gamma_1,gamma_2,gamma_3,gamma_4=gammas

    for b_ind in range(B+K):
        # the varaible int_Gs is meant to have the record of int_G of the 
        # backward sampled path.
        #print("mcmc iteration is:", b)
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(b_ind)

        [log_weights_0,log_weights_1,x_pr_0,x_pr_1, \
        cond_log_weights_0,cond_log_weights_1,cond_int_G_0,cond_int_G_1,\
        cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,Grads_t_0,Grads_t_1]=\
        bdg.Gen_C_Grad_Cond_PF_bridge_back_samp(in_dist,in_dist_pars,Grad_log_in_dist,\
        cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,b,A_0,A_1,A_fd_0,A_fd_1,Sig,\
        fi_0,fi_1,fi_fd_0,fi_fd_1,b_til,Sig_til,r,\
        H,update_func,max_sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par_0,g_den_par_1,\
        aux_trans_den,Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par_0,ind_prop_trans_par_1,\
        Grad_log_G,resamp_coef, l, d,N,seed,fd_rate)


        Grads_test_0[b_ind]=Grads_t_0
        Grads_test_1[b_ind]=Grads_t_1
        ch_paths_0[b_ind]=cond_path_0
        ch_paths_1[b_ind]=cond_path_1


        #if (b_ind+1)%mcmc_links==0:
        if n<=ast:
            if b_ind+1==count+int(mcmc_step*2/2**n):
                count+=int(mcmc_step*2/2**n)
        
                Grad_mcmc_0=np.mean(Grads_test_0[b_ind+1-int(mcmc_step*2/2**n):b_ind+1],axis=0)
                Grad_mcmc_1=np.mean(Grads_test_1[b_ind+1-int(mcmc_step*2/2**n):b_ind+1],axis=0)

                Grads_mcmc_0[n-1]=Grad_mcmc_0
                Grads_mcmc_1[n-1]=Grad_mcmc_1

                the1_0*=np.exp(gamma_1*gamma*Grad_mcmc_0[0]*the1_0/n**(0.5+alpha))
                the1_1*=np.exp(gamma_1*gamma*Grad_mcmc_1[0]*the1_1/n**(0.5+alpha))
                #fi+=gamma*Grad_mcmc[1]/n**(0.5+alpha)
                the2_0*=np.exp(gamma_2*gamma*Grad_mcmc_0[1]*the2_0/n**(0.5+alpha))
                the2_1*=np.exp(gamma_2*gamma*Grad_mcmc_1[1]*the2_1/n**(0.5+alpha))
                #g_den_par+=gamma*Grad_mcmc[2]/n**(0.5+alpha)
                the3_0*=np.exp(gamma_3*gamma*Grad_mcmc_0[2]*the3_0/n**(0.5+alpha))
                the3_1*=np.exp(gamma_3*gamma*Grad_mcmc_1[2]*the3_1/n**(0.5+alpha))

                the4_0*=np.exp(gamma_4*gamma*Grad_mcmc_0[3]*the4_0/n**(0.5+alpha))
                the4_1*=np.exp(gamma_4*gamma*Grad_mcmc_1[3]*the4_1/n**(0.5+alpha))



                pars_0[n]=np.array([the1_0,the2_0,the3_0,the4_0]) 
                A_0=pars_0[n,:3].copy()
                fi_0=pars_0[n,2]
                g_den_par_0=pars_0[n,3]

                pars_1[n]=np.array([the1_1,the2_1,the3_1,the4_1])
                A_1=pars_1[n,:3].copy()
                fi_1=pars_1[n,2]
                g_den_par_1=pars_1[n,3]

                the1s_0=pars_0[n,0]
                the2s_0=pars_0[n,1]
                the3s_0=pars_0[n,2]
                the4s_0=pars_0[n,3]
                w_0=the3s_0**2/2+the1s_0
                xi_0=the2s_0
                sigma_0=the3s_0
                alpha_pars_0=2*w_0/sigma_0**2-1
                theta_0=sigma_0**2/(2*xi_0)

                print(" The pars for 0 are: ",the1s_0,the2s_0,the3s_0,the4s_0)    
                print("The mean for 0 is: ",alpha_pars_0*theta_0)  
                print("The gradeint for 0 is:",Grad_mcmc_0)


                the1s_1=pars_1[n,0]
                the2s_1=pars_1[n,1]
                the3s_1=pars_1[n,2]
                the4s_1=pars_1[n,3]
                w_1=the3s_1**2/2+the1s_1
                xi_1=the2s_1
                sigma_1=the3s_1
                alpha_pars_1=2*w_1/sigma_1**2-1
                theta_1=sigma_1**2/(2*xi_1)
                print(" The pars for 1 are: ",the1s_1,the2s_1,the3s_1,the4s_1)
                print("The mean for 1 is: ",alpha_pars_1*theta_1)
                print("The gradeint for 1 is:",Grad_mcmc_1)
                print("The value of n for 1 is: ",n)





                [in_dist_pars,A_fd_0,A_fd_1,fi_fd_0,fi_fd_1,sample_pars,\
                ind_prop_trans_par_0,ind_prop_trans_par_1]=\
                update_pars([A_0,g_den_par_0],[A_1,g_den_par_1],fd_rate,levels=2)


                n+=1

        else:
            
            if (b_ind+1-count)%mcmc_links==0:
                Grad_mcmc_0=np.mean(Grads_test_0[b_ind+1-mcmc_links:b_ind+1],axis=0)
                Grad_mcmc_1=np.mean(Grads_test_1[b_ind+1-mcmc_links:b_ind+1],axis=0)
    
                Grads_mcmc_0[n-1]=Grad_mcmc_0
                Grads_mcmc_1[n-1]=Grad_mcmc_1
    
                the1_0*=np.exp(gamma_1*gamma*Grad_mcmc_0[0]*the1_0/n**(0.5+alpha))
                the1_1*=np.exp(gamma_1*gamma*Grad_mcmc_1[0]*the1_1/n**(0.5+alpha))
                #fi+=gamma*Grad_mcmc[1]/n**(0.5+alpha)
                the2_0*=np.exp(gamma_2*gamma*Grad_mcmc_0[1]*the2_0/n**(0.5+alpha))
                the2_1*=np.exp(gamma_2*gamma*Grad_mcmc_1[1]*the2_1/n**(0.5+alpha))
                #g_den_par+=gamma*Grad_mcmc[2]/n**(0.5+alpha)
                the3_0*=np.exp(gamma_3*gamma*Grad_mcmc_0[2]*the3_0/n**(0.5+alpha))
                the3_1*=np.exp(gamma_3*gamma*Grad_mcmc_1[2]*the3_1/n**(0.5+alpha))
                
                the4_0*=np.exp(gamma_4*gamma*Grad_mcmc_0[3]*the4_0/n**(0.5+alpha))
                the4_1*=np.exp(gamma_4*gamma*Grad_mcmc_1[3]*the4_1/n**(0.5+alpha))
    
                
                
                pars_0[n]=np.array([the1_0,the2_0,the3_0,the4_0]) 
                A_0=pars_0[n,:3].copy()
                fi_0=pars_0[n,2]
                g_den_par_0=pars_0[n,3]
    
                pars_1[n]=np.array([the1_1,the2_1,the3_1,the4_1])
                A_1=pars_1[n,:3].copy()
                fi_1=pars_1[n,2]
                g_den_par_1=pars_1[n,3]
    
                the1s_0=pars_0[n,0]
                the2s_0=pars_0[n,1]
                the3s_0=pars_0[n,2]
                the4s_0=pars_0[n,3]
                w_0=the3s_0**2/2+the1s_0
                xi_0=the2s_0
                sigma_0=the3s_0
                alpha_pars_0=2*w_0/sigma_0**2-1
                theta_0=sigma_0**2/(2*xi_0)
                
                print(" The pars for 0 are: ",the1s_0,the2s_0,the3s_0,the4s_0)    
                print("The mean for 0 is: ",alpha_pars_0*theta_0)  
                print("The gradeint for 0 is:",Grad_mcmc_0)
    
    
                the1s_1=pars_1[n,0]
                the2s_1=pars_1[n,1]
                the3s_1=pars_1[n,2]
                the4s_1=pars_1[n,3]
                w_1=the3s_1**2/2+the1s_1
                xi_1=the2s_1
                sigma_1=the3s_1
                alpha_pars_1=2*w_1/sigma_1**2-1
                theta_1=sigma_1**2/(2*xi_1)
                print(" The pars for 1 are: ",the1s_1,the2s_1,the3s_1,the4s_1)
                print("The mean for 1 is: ",alpha_pars_1*theta_1)
                print("The gradeint for 1 is:",Grad_mcmc_1)
                print("The value of n for 1 is: ",n)
                
    
                
    
                
                [in_dist_pars,A_fd_0,A_fd_1,fi_fd_0,fi_fd_1,sample_pars,\
                ind_prop_trans_par_0,ind_prop_trans_par_1]=\
                update_pars([A_0,g_den_par_0],[A_1,g_den_par_1],fd_rate,levels=2)
        
    
                n+=1
            
        
    return ch_paths_0,ch_paths_1,pars_0,pars_1 , Grads_mcmc_0, Grads_mcmc_1



def Prl_Gen_Unbiased_comparison(args):


    [sing_in_dist,sing_in_dist_pars,in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
    r,H,update_func,  sing_sample_funct, sing_sample_pars,max_sample_funct,sample_pars,\
    obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
    Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, d,N,seed,fd_rate,\
    mcmc_links,gamma,gammas, alpha,ast, update_pars, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]=args

    # new parameters:
    # l0: Base level of the grid.
    # lmax: Maximum level of the grid.
    # pmax: Maximum level of SGD steps.
    # beta: coupling rate of the time discretization
    # CL,CL0,CP,CP0: parameters of the coupling.
    # s0: number of SGD steps at the base level.
    np.random.seed(seed+1)
    # cumulative for the time discretization
    eLes=np.arange(l0,lmax+1)
    beta=beta_l
    q=4
    P0=(1+CL*np.sum((q+eLes[1:]-l0)*np.log(q+eLes[1:]-l0)**2/2**(beta*eLes[1:]))/\
    (CL0*(q+1)*np.log(q+1)**2))**(-1)
    l_cumu=bdg.P_l_cumu_gen(P0,lmax-l0+1,beta,l0)
    l=eLes[bdg.sampling(l_cumu)]
    l=lmax # here we change this to ensure we have the full unbiased estimator
    # cumulative for the number of SGD steps
    beta=beta_p
    ePes=np.arange(0,pmax+1)
    eSes=s0*2**ePes
    P0=(1+CP*np.sum((ePes[1:]+q)*np.log(ePes[1:]+q)**2/eSes[1:]**(beta))\
    /(CP0*(q+1)*np.log(1+q)**2))**(-1)
    
    p_cumu=bdg.P_p_cumu_gen(P0,pmax,beta,s0)
    p=bdg.sampling(p_cumu)
    p=pmax # here we change this to ensure we have the full unbiased estimator
    #print("P0p, p_cumu, p",P0,p_cumu,p)
    #print(CP0/P0,CP/(p_cumu[1]-p_cumu[0])) 
    pars=np.zeros((2,2,4))
    #print("l and p are: ",l,p)
    #print("The parameters are: ",l,p)
    # The first dimensions of pars are for the two levels of time discretization. 
    
    if l==l0:
        SGD_steps=eSes[p]

        """
        Gen_SGD_bridge(in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,sample_funct,sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, gammas,alpha,ast, update_pars)
        """

        


        parameters=bdg.Gen_SGD_bridge(sing_in_dist,sing_in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,sing_sample_funct,sing_sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, gammas,alpha,ast, update_pars)[1]
        pars[1,1]=parameters[eSes[p]]
        if p!=0:
            pars[1,0]=parameters[eSes[p-1]]
    else:
        """
        Gen_C_SGD_bridge(in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,max_sample_funct,sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma,gammas, alpha, update_pars)   
        """

        SGD_steps=eSes[p]
        parameters=bdg.Gen_C_SGD_bridge(in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,max_sample_funct,sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma,gammas, alpha,ast, update_pars)   
        pars[1,1]=parameters[3][eSes[p]]
        pars[0,1]=parameters[2][eSes[p]]
        if p!=0:
            pars[1,0]=parameters[3][eSes[p-1]]
            pars[0,0]=parameters[2][eSes[p-1]]
            
    return pars,np.array([l,p])



def Prl_Gen_Unbiased(args):


    [sing_in_dist,sing_in_dist_pars,in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
    r,H,update_func,  sing_sample_funct, sing_sample_pars,max_sample_funct,sample_pars,\
    obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
    Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, d,N,seed,fd_rate,\
    mcmc_links,gamma,gammas, alpha,ast, update_pars, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]=args

    # new parameters:
    # l0: Base level of the grid.
    # lmax: Maximum level of the grid.
    # pmax: Maximum level of SGD steps.
    # beta: coupling rate of the time discretization
    # CL,CL0,CP,CP0: parameters of the coupling.
    # s0: number of SGD steps at the base level.
    np.random.seed(seed+1)
    # cumulative for the time discretization
    eLes=np.arange(l0,lmax+1)
    beta=beta_l
    q=4
    P0=(1+CL*np.sum((q+eLes[1:]-l0)*np.log(q+eLes[1:]-l0)**2/2**(beta*eLes[1:]))/\
    (CL0*(q+1)*np.log(q+1)**2))**(-1)
    
    l_cumu=bdg.P_l_cumu_gen(P0,lmax-l0+1,beta,l0)
    l=eLes[bdg.sampling(l_cumu)]
    # cumulative for the number of SGD steps
    beta=beta_p
    ePes=np.arange(0,pmax+1)
    eSes=s0*2**ePes
    P0=(1+CP*np.sum((ePes[1:]+q)*np.log(ePes[1:]+q)**2/eSes[1:]**(beta))\
    /(CP0*(q+1)*np.log(1+q)**2))**(-1)
    
    p_cumu=bdg.P_p_cumu_gen(P0,pmax,beta,s0)
    p=bdg.sampling(p_cumu)
    #print("P0p, p_cumu, p",P0,p_cumu,p)
    #print(CP0/P0,CP/(p_cumu[1]-p_cumu[0])) 
    pars=np.zeros((2,2,4))
    #print("l and p are: ",l,p)
    #print("The parameters are: ",l,p)
    # The first dimensions of pars are for the two levels of time discretization. 
    
    if l==l0:
        SGD_steps=eSes[p]

        """
        Gen_SGD_bridge(in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,sample_funct,sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, gammas,alpha,ast, update_pars)
        """

        


        parameters=bdg.Gen_SGD_bridge(sing_in_dist,sing_in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,sing_sample_funct,sing_sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, gammas,alpha,ast, update_pars)[1]
        pars[1,1]=parameters[eSes[p]]
        if p!=0:
            pars[1,0]=parameters[eSes[p-1]]
    else:
        """
        Gen_C_SGD_bridge(in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,max_sample_funct,sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma,gammas, alpha, update_pars)   
        """

        SGD_steps=eSes[p]
        parameters=bdg.Gen_C_SGD_bridge(in_dist,in_dist_pars,Grad_log_in_dist,b,A,Sig,fi,b_til,Sig_til,\
        r,H,update_func,max_sample_funct,sample_pars,\
        obs,obs_times,log_g_den,g_den_par, aux_trans_den,\
        Grad_log_aux_trans,prop_trans_den,ind_prop_trans_par, Grad_log_G,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma,gammas, alpha,ast, update_pars)   
        pars[1,1]=parameters[3][eSes[p]]
        pars[0,1]=parameters[2][eSes[p]]
        if p!=0:
            pars[1,0]=parameters[3][eSes[p-1]]
            pars[0,0]=parameters[2][eSes[p-1]]
            
    return pars,np.array([l,p])


def Prl_EM_logistic(args ):

    [t0,x0,T,the1,the2,the3,l,N,seed,in_c]=args
    stat=time.time()
    Dt=1/(2**l)
    steps=int((T-t0)/(Dt))
    
    x=np.zeros((steps+1,N))
    x[0]=x0+np.zeros(N)
    if in_c==True:
        np.random.seed(seed)
        
        dW=np.sqrt(Dt)*np.random.normal(0,1,(steps,N))
        for i in range(1,steps+1):
            #x[i]=x[i-1]+(the3**2/2+the1-the2*x[i-1])*x[i-1] *Dt+dW[i-1]*the3*x[i-1]
            x[i]=x[i-1]+(the1/the3-the2/the3*np.exp(the2*x[i-1])*Dt+dW[i-1])
    else:
        for i in range(1,steps+1):
            x[i]=x[i-1]+(the1/the3-the2/the3*np.exp(the2*x[i-1]))*Dt+np.sqrt(Dt)*np.random.normal(0,1,N)
        
    c_time= time.time()-stat
    return c_time


#%%

#%%
#"""

if __name__ == '__main__':
    
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=500
    l0=5
    Lmax=10
    N=50
    mcmc_links=1
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_xiii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")


    
#%%
"""
    40538783
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=500
    l0=5
    Lmax=10
    N=50
    mcmc_links=1
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_xiii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")

    
    40538742

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(63)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxx"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    40538736
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(62)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    40538654

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(61)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    


    
    40538650

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(60)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    40538645

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(59)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    

    40538642

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(57)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    40538640



    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(58)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")




    
    40538619

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(56)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    40538613
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(55)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    

    40538610

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(54)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxxi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    40538557

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(53)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxx"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    40538553

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(52)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    40538548


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(51)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    

    40538543
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(50)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
        


    40538496

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(49)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    40538494

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(48)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    

    40538486
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(47)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    40538485

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(46)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    40538369

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(45)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    40538324


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l)+3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan12_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    40538261
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l)+3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan12_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    40538213
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l)+3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan12_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    40538165

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l)+3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan12_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    

    40538093

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l)+3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan12_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    40537035
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]

    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0

    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(44)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]

    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xxi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")



    
    40452711
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(43)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="
    "+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    
    
    40452708

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(42)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xvix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    
    40452706

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(41)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    
    40452702

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(40)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    40452676

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(39)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    
    40452671
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(38)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    40452$

        fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(37)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    40451553

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    
    40451551


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    40451549
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    40451536
    40451538
    40451542
    40451546

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    40451536
    40451538
    40451542

    40451542

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    40451538

    40451538
    40451536

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



        40451536

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3)+4)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan11_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data27/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    40445362

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=1.70291650e+00 ,3.32587500e-03, 7.38051400e-01, 1.91349128e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=50
    l0=10
    Lmax=10
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf5_ip_v"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    40437408

    40434434

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=50
    l0=5
    Lmax=5
    SGD_steps=2**10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_ePes_i"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    40433643

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=500
    l0=5
    Lmax=9
    N=50
    mcmc_links=1
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_xii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")


    40432167

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=200
    l0=5
    Lmax=9
    N=50
    mcmc_links=1
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_xii"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
       @
    
    

    40424533

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(36)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    40424530

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(35)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    

    40424529

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(34)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_xi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    

    40424528
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(33)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_x"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
        40424526
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(32)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_ix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    


    40424523
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(31)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_viii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    
    40424520

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=1000
    l0=5
    Lmax=9
    N=50
    mcmc_links=1
    SGD_steps=2**3
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_xi"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
       


    
    40424445

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=50
    l0=5
    Lmax=5
    SGD_steps=2**10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_ePes_i"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

       

    40424081

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=100
    l0=4
    Lmax=9
    N=50
    mcmc_links=1
    SGD_steps=2**3
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_x"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
       



   40424064

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=100
    l0=4
    Lmax=4
    N=50
    mcmc_links=1
    SGD_steps=2**1
    B=mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_test"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n") 
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
       
    



    40423350

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=50
    l0=4
    Lmax=9
    SGD_steps=2**3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_eLes_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    40423329

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=50
    l0=4
    Lmax=7
    SGD_steps=2**3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_eLes_tests_2"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    40423293

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=50
    l0=4
    Lmax=5
    SGD_steps=2**3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_eLes_tests"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    40423285



    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=100
    N=50
    l0=5
    Lmax=5
    SGD_steps=2**10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_ePes"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    40423280


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=50
    l0=5
    Lmax=5
    SGD_steps=2**1
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan10_test"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data26/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    40421268

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=1.90031095e+00 ,3.69975000e-03 ,7.67187725e-01 ,1.92413204e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=50
    l0=10
    Lmax=10
    mcmc_links=1
    SGD_steps=400
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf5_ip_iii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    40398505

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=50
    l0=6
    Lmax=6
    mcmc_links=1
    SGD_steps=400
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf5_ip_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    40377139

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(30)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_vii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
       
    
    
    40377128

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(29)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_vi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    

    40377126
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(28)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_v"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    40346114
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(27)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_iv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    

    40346075
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(26)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_iii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    40320173

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(25)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_ii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


40320129


fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8

    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


40320068
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    samples=1
    N=50
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    lmax=3
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU8_ip_test"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data25/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data25/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    



40320023

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=50
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=400
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf5_ip_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



40320006

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=50
    l0=4
    Lmax=4
    mcmc_links=1
    SGD_steps=2
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf5_ip_test"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    


X1_flat = reshape(X1_sm, [], size(X1_sm, 3));  % or reshape(A, [], 1) for full vector
X2_flat = reshape(X2_sm, [], size(X2_sm, 3)); 
h1_flat= reshape(hs1, [], size(hs1, 3));
h2_flat= reshape(hs2, [], size(hs2, 3));
thetas_flat= reshape(thetas, [], size(thetas, 3));
writematrix(h1_flat, 'Observationsdata_matlab/Grads1_sm_31.txt');
writematrix(X1_flat, 'Observationsdata_matlab/X1_sm_31.txt');
writematrix(h2_flat, 'Observationsdata_matlab/Grads2_sm_31.txt');
writematrix(X2_flat, 'Observationsdata_matlab/X2_sm_31.txt');
writematrix(thetas_flat, 'Observationsdata_matlab/thetas_sm_31.txt');
writematrix(thetas_p_level,'Observationsdata_matlab/thetas_p_level_31.txt');


kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
Y_data = kangaroo_Data(:,1:2);
% MLUSMA parameters
L_max = 9;

L_min = 8;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =1000;
mcmc_link=500;
gammas=[2,3,0,10]*5e-4;
alpha=0.5;
particleCount = 500;
iterCount = 500; % replicates count
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
theta0_p=log(theta0(:));
X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
hs1=zeros(iterCount,N_0,4);
hs2=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;

end_0=toc(start_0);
tic;
X_sm=zeros(iterCount,N_0,length(DataTimes));
X1_sm=zeros(iterCount,N_0,length(DataTimes));
X2_sm=zeros(iterCount,N_0,length(DataTimes));


X1_flat = reshape(X1_sm, [], size(X1_sm, 3));  % or reshape(A, [], 1) for full vector
X2_flat = reshape(X2_sm, [], size(X2_sm, 3)); 
h1_flat= reshape(hs1, [], size(hs1, 3));
h2_flat= reshape(hs2, [], size(hs2, 3));
thetas_flat= reshape(thetas, [], size(thetas, 3));
writematrix(h1_flat, 'Observationsdata_matlab/Grads1_sm_30.txt');
writematrix(X1_flat, 'Observationsdata_matlab/X1_sm_30.txt');
writematrix(h2_flat, 'Observationsdata_matlab/Grads2_sm_30.txt');
writematrix(X2_flat, 'Observationsdata_matlab/X2_sm_30.txt');
writematrix(thetas_flat, 'Observationsdata_matlab/thetas_sm_30.txt');
writematrix(thetas_p_level,'Observationsdata_matlab/thetas_p_level_30.txt');


kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
Y_data = kangaroo_Data(:,1:2);
% MLUSMA parameters
L_max = 9;

L_min = 8;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =1000;
mcmc_link=500;
gammas=[2,3,0,10]*5e-4;
alpha=0.5;
particleCount = 10;
iterCount = 10; % replicates count
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
theta0_p=log(theta0(:));
X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
hs1=zeros(iterCount,N_0,4);
hs2=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;


X1_flat = reshape(X1_sm, [], size(X1_sm, 3));  % or reshape(A, [], 1) for full vector
X2_flat = reshape(X2_sm, [], size(X2_sm, 3)); 
h1_flat= reshape(hs1, [], size(hs1, 3));
h2_flat= reshape(hs2, [], size(hs2, 3));
thetas_flat= reshape(thetas, [], size(thetas, 3));
writematrix(h1_flat, 'Observationsdata_matlab/Grads1_sm_27.txt');
writematrix(X1_flat, 'Observationsdata_matlab/X1_sm_27.txt');
writematrix(h2_flat, 'Observationsdata_matlab/Grads2_sm_27.txt');
writematrix(X2_flat, 'Observationsdata_matlab/X2_sm_27.txt');
writematrix(thetas_flat, 'Observationsdata_matlab/thetas_sm_27.txt');
writematrix(thetas_p_level,'Observationsdata_matlab/thetas_p_level_27.txt');
%X_mean=mean(X_sm,[1,2]);
%X_est=mean(X_sm,2);
%X_var=var(X_est,1);


kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
Y_data = kangaroo_Data(:,1:2);
% MLUSMA parameters
L_max = 9;

L_min = 8;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =1000;
mcmc_link=500;
gammas=[2,3,0,10]*5e-4;
alpha=0.5;
particleCount = 500;
iterCount = 100; % replicates count
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
theta0_p=log(theta0(:));
X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
hs1=zeros(iterCount,N_0,4);
hs2=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;

end_0=toc(start_0);
tic;
X_sm=zeros(iterCount,N_0,length(DataTimes));
X1_sm=zeros(iterCount,N_0,length(DataTimes));
X2_sm=zeros(iterCount,N_0,length(DataTimes));



X1_flat = reshape(X1_sm, [], size(X1_sm, 3));  % or reshape(A, [], 1) for full vector
X2_flat = reshape(X2_sm, [], size(X2_sm, 3)); 
h1_flat= reshape(hs1, [], size(hs1, 3));
h2_flat= reshape(hs2, [], size(hs2, 3));
thetas_flat= reshape(thetas, [], size(thetas, 3));
writematrix(h1_flat, 'Observationsdata_matlab/Grads1_sm_25.txt');
writematrix(X1_flat, 'Observationsdata_matlab/X1_sm_25.txt');
writematrix(h2_flat, 'Observationsdata_matlab/Grads2_sm_25.txt');
writematrix(X2_flat, 'Observationsdata_matlab/X2_sm_25.txt');
writematrix(thetas_flat, 'Observationsdata_matlab/thetas_sm_25.txt');
writematrix(thetas_p_level,'Observationsdata_matlab/thetas_p_level_25.txt');







    kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
Y_data = kangaroo_Data(:,1:2);
% MLUSMA parameters
L_max = 9;

L_min = 8;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =200;
mcmc_link=200;
gammas=[2,3,0,10]*5e-4;
alpha=0.5;
particleCount = 500;
iterCount = 100; % replicates count
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
theta0_p=log(theta0(:));

X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
hs1=zeros(iterCount,N_0,4);
hs2=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;

end_0=toc(start_0);
tic;
X_sm=zeros(iterCount,N_0,length(DataTimes));
X1_sm=zeros(iterCount,N_0,length(DataTimes));
X2_sm=zeros(iterCount,N_0,length(DataTimes));









    X1_flat = reshape(X1_sm, [], size(X1_sm, 3));  % or reshape(A, [], 1) for full vector
X2_flat = reshape(X2_sm, [], size(X2_sm, 3)); 
h1_flat= reshape(hs1, [], size(hs1, 3));
h2_flat= reshape(hs2, [], size(hs2, 3));
thetas_flat= reshape(thetas, [], size(thetas, 3));
writematrix(h1_flat, 'Observationsdata_matlab/Grads1_sm_22.txt');
writematrix(X1_flat, 'Observationsdata_matlab/X1_sm_22.txt');
writematrix(h2_flat, 'Observationsdata_matlab/Grads2_sm_22.txt');
writematrix(X2_flat, 'Observationsdata_matlab/X2_sm_22.txt');
writematrix(thetas_flat, 'Observationsdata_matlab/thetas_sm_22.txt');
writematrix(thetas_p_level,'Observationsdata_matlab/thetas_p_level_22.txt')


    kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
Y_data = kangaroo_Data(:,1:2);
% MLUSMA parameters
L_max = 6;

L_min = 5;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =400;
mcmc_link=400;
gammas=[2,3,0,10]*5e-4;
alpha=0.5;
particleCount = 500;
iterCount = 100; % replicates count
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
theta0_p=log(theta0(:));

X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
hs1=zeros(iterCount,N_0,4);
hs2=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;

end_0=toc(start_0);
tic;
X_sm=zeros(iterCount,N_0,length(DataTimes));
X1_sm=zeros(iterCount,N_0,length(DataTimes));
X2_sm=zeros(iterCount,N_0,length(DataTimes));


    40266109

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=5
    Lmax=5
    N=10
    mcmc_links=500
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_ix"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
       


    40265156

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=5
    Lmax=5
    N=500
    mcmc_links=500
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_viii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")


    40260393

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=1000
    l0=5
    Lmax=5
    mcmc_links=500
    SGD_steps=1
    B= mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xx"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")

    
    40260313

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=1000
    l0=5
    Lmax=5
    mcmc_links=500
    SGD_steps=2
    B= mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xix"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")  

    

    40260281

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=1000
    l0=5
    Lmax=5
    mcmc_links=500
    SGD_steps=2
    B= mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xix"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data24/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    
    
    



    40251553

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=6
    Lmax=6
    N=500
    mcmc_links=100
    SGD_steps=1
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_vii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
    




    writematrix(h_flat, 'Observationsdata/Grads_sm_19.txt');
writematrix(X_flat, 'Observationsdata/X_sm_19.txt');
writematrix(thetas_flat, 'Observationsdata/thetas_sm_19.txt');
writematrix(thetas_p_level,'Observationsdata/thetas_p_level_19.txt')


    kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
%DataTimes=zeros(2,1);
%disp(DataTimes);
%DataTimes(1,1)=DataTimes_or(1,1);
%DataTimes( 2,1)=DataTimes_or(end-1,1);
%DataTimes( 3,1)=DataTimes_or(end-1,1)+30;
%disp(DataTimes);
Y_data = kangaroo_Data(:,1:2);
%Y_data=zeros(2,2);
%disp(Y_data);
%Y_data(1,:)=Y_data_or(1,:);
%Y_data(2,:)=Y_data_or(end-1,:);
%Y_data(3,:)=Y_data_or(end,:);
%disp(Y_data)
% MLUSMA parameters
L_max = 7;
%L_start = 3;
L_min = 6;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =1000;
mcmc_link=500;
gammas=[2,3,0,10]*5e-4;
alpha=0.5;
particleCount = 500;
%iterCount = 300;                % replicates count
iterCount = 1200;
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
%theta0 = [1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01];
%theta0 = [0.160496933034803 ,  0.000506506513065, 6.71310500e-01, 1.86623609e+01];
%theta0 = [1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01];
%theta0 = [2, 3.429e-03, 0.840, 10];
%theta0_p = [log(2.397), log(4.429e-03), log(0.840), log(17.631)];
theta0_p=log(theta0(:));
%theta0_p = [1.860247644028022,  -5.675069792663762 , -0.257493022287784  , 2.924471348097306];
%X0 = log(Y_data(1,1))/theta0(3);
X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
hs1=zeros(iterCount,N_0,4);
hs2=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;


    40240487

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=5
    Lmax=5
    N=1000
    mcmc_links=1000
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_vi"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
    


    40239722

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=5
    Lmax=5
    N=500
    mcmc_links=500
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_v"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
    

    

    40239692

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=6
    Lmax=6
    N=100
    mcmc_links=50
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_iv"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
     
    

    40239616

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=4
    Lmax=4
    N=100
    mcmc_links=50
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_0=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    Grads_1=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            Grads_0[sample,i]=pool_outputs[sample*len(eLes)+i][4]
            Grads_1[sample,i]=pool_outputs[sample*len(eLes)+i][5]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    Grads_0=Grads_0.flatten()
    Grads_1=Grads_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_iii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_0_v"+v+".txt",Grads_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_Grads_1_v"+v+".txt",Grads_1,fmt="%f")
    

    40236405

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=4
    Lmax=4
    N=100
    mcmc_links=50
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")


    40235631

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=5
    Lmax=5
    N=500
    mcmc_links=100
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_i"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")




40235537

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=1
    #the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=5
    Lmax=5
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0005
    gammas=[2,3,0.0,10]
    ast=0
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_comparison_test"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_C_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")




40232013

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=0.8 
    ##the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=500
    mcmc_links=100
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.0005
    gammas=[2,3,0.0,10]

    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=4
    lmax=5
    pmax=1
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased_comparison,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU_comparison_iii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data23/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
     


40231967
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=0.8 
    ##the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=10
    #samples=1
    N=100
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.0005
    gammas=[2,3,0.0,10]

    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=4
    lmax=5
    pmax=1
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_Unbiased_comparison,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU_comparison_iii_test"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data23/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data23/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
     



kangaroo_Data = readmatrix(sprintf('%s%s', folder_read,'Kangaroo Data.txt'));
DataTimes = kangaroo_Data(:,3);
%DataTimes=zeros(2,1);
%disp(DataTimes);
%DataTimes(1,1)=DataTimes_or(1,1);
%DataTimes( 2,1)=DataTimes_or(end-1,1);
%DataTimes( 3,1)=DataTimes_or(end-1,1)+30;
%disp(DataTimes);
Y_data = kangaroo_Data(:,1:2);
%Y_data=zeros(2,2);
%disp(Y_data);
%Y_data(1,:)=Y_data_or(1,:);
%Y_data(2,:)=Y_data_or(end-1,:);
%Y_data(3,:)=Y_data_or(end,:);
%disp(Y_data)
% MLUSMA parameters
L_max = 6;
%L_start = 3;
L_min = 5;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =200;
mcmc_link=100;
gammas=[2,3,0.6,6]*5e-4;
alpha=0.5;
particleCount = 1000;
%iterCount = 300;                % replicates count
iterCount = 100;
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
%theta0 = [1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01];
%theta0 = [0.160496933034803 ,  0.000506506513065, 6.71310500e-01, 1.86623609e+01];
%theta0 = [1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01];
%theta0 = [2, 3.429e-03, 0.840, 10];
%theta0_p = [log(2.397), log(4.429e-03), log(0.840), log(17.631)];
theta0_p=log(theta0(:));
%theta0_p = [1.860247644028022,  -5.675069792663762 , -0.257493022287784  , 2.924471348097306];
%X0 = log(Y_data(1,1))/theta0(3);
X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,N_0+1,4);
disp(size(thetas(:,1,:)));
thetas(:,1,1) = theta0(1);
thetas(:,1,2) = theta0(2);
thetas(:,1,3) = theta0(3);
thetas(:,1,4) = theta0(4);
hs=zeros(iterCount,N_0,4);
ch_me=zeros(iterCount);
thetas_p_level=zeros(iterCount,4);
start_0=tic;
X_in=zeros(particleCount,1)+X0;

# In this run we check if the siddigs algorithm as I made is comparable to the backward one.



# This run is designed to compute the unbiased estimator of our algorithm and compare it to Siddigs's. 
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=0.8 
    ##the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=10
    mcmc_links=100
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.0005
    gammas=[2,3,0.0,10]

    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=4
    lmax=5
    pmax=1
    beta_l=1/2
    beta_p=1
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=19
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_Unbiased_comparison,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU_comparison_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("Observationsdata/data22/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("Observationsdata/data22/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
     



 #This run is designed to obtain the true parameters unbiased estimator for a certain level and number of SGD steps. 
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    #the1=2
    #the2=the1/smean
    #the3=0.8 
    ##the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    #the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=100
    mcmc_links=100
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.0005
    gammas=[2,3,0.0,10]

    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=4
    lmax=5
    pmax=1
    beta_l=1/2
    beta_p=1
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=19
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_Unbiased_comparison,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU_comparison_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("Observationsdata/data22/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("Observationsdata/data22/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
     



    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the1=2
    the2=the1/smean
    the3=1
    the4=1

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=200
    N=20
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=2**8
    B= mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),SGD_steps+ast+1,4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xvii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("Observationsdata/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("Observationsdata/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("Observationsdata/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    



40214688

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the1=2
    the2=the1/smean
    the3=1
    the4=1

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=20
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=2**8
    B= mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),SGD_steps+ast+1,4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xv"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


40214688

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the1=2
    the2=the1/smean
    the3=1
    the4=1

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=20
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=2**8
    B= mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,SGD_steps+ast+1,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xv"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    



40214551

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the1=2
    the2=the1/smean
    the3=1
    the4=1

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=20
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=2**8
    B= mcmc_links*SGD_steps
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xv"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

GSb_comparison_xiv.txt can be used as comparison in the computation of the gradients. 

    Grads_sm_15.txt
    X_sm_15.txt

    L_max = 6;
%L_start = 3;
L_min = 6;

p_max = 0;      % useless now
p_min = 0;      % p_min is always 1.
N_0 =51;

particleCount = 10;
%iterCount = 300;                % replicates count
iterCount = 40*30;
theta0 = [2.397, 4.429e-03, 0.840, 17.631];
%theta0 = [1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01];
%theta0 = [0.160496933034803 ,  0.000506506513065, 6.71310500e-01, 1.86623609e+01];
%theta0 = [1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01];
%theta0 = [2, 3.429e-03, 0.840, 10];
%theta0_p = [log(2.397), log(4.429e-03), log(0.840), log(17.631)];
theta0_p=log(theta0(:));
%theta0_p = [1.860247644028022,  -5.675069792663762 , -0.257493022287784  , 2.924471348097306];
%X0 = log(Y_data(1,1))/theta0(3);
X0 = gamrnd(2*theta0(1)/theta0(3)^2,theta0(3)^2/(2*theta0(2)),1);
Cost_MSA_try_iter = zeros(iterCount, 1);
Time_MSA_try_iter = zeros(iterCount, 1);
used_L_try_iter =   zeros(iterCount, 1);
used_p_try_iter = zeros(iterCount, 1);
used_p_prob_try_iter = zeros(iterCount, 1);
used_L_prob_try_iter = zeros(iterCount, 1);
theta_iter = zeros(iterCount, 4);
theta_weighted_iter = zeros(iterCount, 4);
%iterCounts = zeros(1, L_max - L_start + 1);
theta_trace1 = cell(iterCount, 1);
theta_trace2 = cell(iterCount, 1);
theta_p_trace1 = cell(iterCount, 1);
theta_p_trace2 = cell(iterCount, 1);
theta_trace = cell(iterCount, 1);
theta_p_trace = cell(iterCount, 1);
thetas=zeros(iterCount,2*N_0-1,4);
hs=zeros(iterCount,2*N_0-1,4);
ch_me=zeros(iterCount);
start_0=tic;
X_in=zeros(particleCount,1)+X0;

    40213979

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=10
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=100
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xiv"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


        40213232

    fd=1e-5
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=1000
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=100
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xiii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

    

    40212942

    fd=1e-5
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=500
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=100
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    
    
    40212734

    fd=1e-5
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    #the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=100
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=300
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_xi"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")   
    

    40211571
fd=1e-5
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    #the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    the1,the2,the3,the4=2+00 ,2/530, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=100
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=300
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_x"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    40211413

    fd=1e-5
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=100
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=300
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_ix"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

    
    40210916

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the1,the2,the3,the4=1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=100
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=300
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_viii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

    

    40195851

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.631
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=100
    l0=7
    Lmax=7
    mcmc_links=1
    SGD_steps=1000
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_vii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    
    40195602


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=5
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=200
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_vi"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data21/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    
    40195390

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=10
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=100
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_v"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    
40195335

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=10
    l0=7
    Lmax=7
    mcmc_links=1
    SGD_steps=50
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_iv"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


40192374

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=10
    l0=7
    Lmax=7
    mcmc_links=1
    SGD_steps=50
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][1][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_mcmc=Grads_mcmc.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_iii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_Grads_mcmc_v"+v+".txt",Grads_mcmc,fmt="%f")
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    



    40192279

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=10
    l0=7
    Lmax=7
    mcmc_links=1
    SGD_steps=50
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][1][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_iii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    



    
    40192156

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=20
    l0=4
    Lmax=4
    mcmc_links=1
    SGD_steps=2
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    Grads_mcmc=np.zeros((samples,len(eLes),SGD_steps+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][1]
            Grads_mcmc[sample,i]=pool_outputs[sample*len(eLes)+i][1][2]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_test"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    

    39936015
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=20
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=400
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_ii"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    39935883

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=50
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=2000
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_long"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

    
    39935822

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=50
    l0=5
    Lmax=5
    mcmc_links=1
    SGD_steps=400
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_i"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

    

    39935782

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=50
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=3
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_test"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    



    39935709

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=50
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=3
    B= mcmc_links*SGD_steps
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_test"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    

    
    39935641

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=50
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=3
    gamma=0.000
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_comparison_test"+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    
    np.savetxt("data/data20/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")    


    39921011

    t0=0
    T=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    theta=[the1,the2,the3]
    the4=np.array([[10]])
    theta_aux=1
    sigma=the3
    sigma_aux=sigma
    l=10
    d=1
    N=50
    x0=0+np.zeros(N)
    x_p=0+np.zeros(N)

    dim=1
    seed=1 
    # t0,x0,T,x_p,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,l,d,N,seed
    #start=time.time()
    n_tests=500
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    samples= 10000
    inputs=[]
    times=np.zeros(samples)
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[t0,x0,T,the1,the2,the3,l,N,seed,True]
        inputs.append(args)

    start=time.time()
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_EM_logistic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        times[sample]=pool_outputs[sample]
    v="test4"
    np.savetxt("data/data19/EM_computations"+v+".txt",times,fmt="%f")

    39920993

    t0=0
    T=10
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    theta=[the1,the2,the3]
    the4=np.array([[10]])
    theta_aux=1
    sigma=the3
    sigma_aux=sigma
    l=10
    d=1
    N=50
    x0=0+np.zeros(N)
    x_p=0+np.zeros(N)

    dim=1
    seed=1 
    # t0,x0,T,x_p,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,l,d,N,seed
    #start=time.time()
    n_tests=500
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    samples= 1000
    inputs=[]
    times=np.zeros(samples)
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[t0,x0,T,the1,the2,the3,l,N,seed,True]
        inputs.append(args)

    start=time.time()
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_EM_logistic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        times[sample]=pool_outputs[sample]
    v="test3"
    np.savetxt("data/data19/EM_computations"+v+".txt",times,fmt="%f")



    39683641
    39683643
    39683658
    39683666
    39683672
    39683690
    39683701
    39683701

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(38)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39683690

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(37)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39683672

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(36)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    39683666

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(35)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    
    39683658
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(34)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    
    39683643

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(33)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    39683641

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(32)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39682098
    39683540
    39683545
    39683598
    39683609
    39683627
    39683637
    39683637

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(31)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39683627

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(30)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxxi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    
    39683609


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(29)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxx"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    39683598

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(28)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39683545

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(27)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39683540

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(26)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    39682098

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(25)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39682041

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=296
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_vii_b"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39681983

    # This version is very similar to GSbVan9_vii_a, the only change we made is the
    # seed number

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=2
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_vii_a"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39639709

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39639704

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(23)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
        39639696

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(22)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39639693

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(21)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    
    39639691

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(20)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xxi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    39639682

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(19)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xx"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39639678

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(18)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")



    39639123

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    

    
    39636658

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(17)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    39636652

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(16)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    39636631

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(15)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    39635621

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39635612

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39635601

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    
    39635574

    
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    
    39635569

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39635567

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39635547

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(2*(l-3))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan9_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




        39635455

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_xi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    


    39635445

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=12
    Lmax=12
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_x"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    

    

    39635406

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=11
    Lmax=11
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_ix"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    

    
    
            39635395

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=10
    Lmax=10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_viii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    


    39635356

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    


    
    39635076

    
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    



    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    


    
    

    39635056

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    


    
    39635033
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    

    

    39634997

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    


    39634972
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    


    


    39634951
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=0
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan8_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39626149
39626155
39626161
39626168
39626174    
    
    39626174

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(14)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")



39626149
39626155
39626161
39626168

    39626168

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(13)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

39626149
39626155
39626161

    39626161

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(12)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

39626149
39626155
    
39626155
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(11)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


39626149

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(10)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_xi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")



    39624308

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=111
    Lmax=11
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_ix"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    

    39624300

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=10
    Lmax=10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_viii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    
    39624292

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



        39624289

        fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39624274

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39624270

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    39624259


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39624243

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39624214

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-3)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan7_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39624120

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(9)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_x"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    39624102

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=11
    Lmax=11
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_ix"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39624094

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=10
    Lmax=10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_viii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39624085

     fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39624080

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39624067

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39624054

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39624045

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39624035

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    39624028

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(l-l0)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan6_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    lost x

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(9)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_x"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    39621427

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=11
    Lmax=11
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_ix"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39621421

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=10
    Lmax=10
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_viii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39621412

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    39621386

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    39621336

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    39621292
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39621228

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39621221
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    39621187

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=2**(i)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan5_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39620862

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(8)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_ix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    
    39597398
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(6)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_vii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    39548488
    39548519
    39568214
    39568660
    39583719
    39597363

    
    39597363

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(5)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_vi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    39583719

     
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(4)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_v"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    

    39569857

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan4_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    
    39569836


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan4_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39569825

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan4_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39569810

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan4_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    

    39569785

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan4_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39569770

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan4_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39548488
    39548519
    39568214
    39568660


    39568660
    
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(3)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_iv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    
    39548488
    39548519
    39568214


    39568214

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(2)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_iii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")



    39548569

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(1)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_ii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    39548568

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=500
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v=" "#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39548565

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=400
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf4_ip_iv"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39548563

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=300
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf4_ip_iii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39548488
    39548519
    39548519


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(1)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()

    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_ii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    39548488

        fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=100
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU7_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39544997

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=8
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU6_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    39540677

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8 
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU5_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


    
    39540665
 
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf4_ip_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39540649

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=0.8
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=80
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf4_ip_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    


    39519548  

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(24)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xxi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    
    39519529

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(23)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xx"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39519535

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(22)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
        

    39516502

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(21)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xviii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    


    39516483

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(20)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xvii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    



    39516464

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=1.45754903e+00, 2.72797500e-03, 6.71310500e-01, 1.86623609e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf3_ip_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    iit


    39498399

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(19)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xvi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    
    39498347

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(18)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    39498328

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(17)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xiv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    
    39496847


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(0)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU4_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    



    39496711

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=300
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=36445
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf2_ip_iii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39496706

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=3644
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf2_ip_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    
    39496689

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=10
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf2_ip_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39486209
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(16)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xiii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    39474955

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(15)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    
    39470426

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(14)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_xi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    39469773

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(13)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_x"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    
    39469746

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(12)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_ix"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    39469712

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=400
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf_ip_iii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    
    39469710

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=300
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf_ip_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    

    39469709

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    #the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=364
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_hf_ip_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    


    39446844

         
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(11)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_viii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    
    39446820
        
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(10)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_vii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


    
    39440066    

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=13
    Lmax=13
    N=200
    mcmc_links=1
    SGD_steps=40
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,1,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.1
    #arg_cm=int(sys.argv[1])
    arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=942
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_True_ip_ii"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


    39439693

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=13
    Lmax=13
    N=200
    mcmc_links=1
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,1,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.1
    #arg_cm=int(sys.argv[1])
    arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1232
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_True_ip_i"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")

    

    39439690

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=4
    Lmax=4
    N=200
    mcmc_links=1
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,1,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.1
    #arg_cm=int(sys.argv[1])
    arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1232
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_True_ip_test"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


    39437749

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=4
    Lmax=4
    N=200
    mcmc_links=1
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,1,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.1
    #arg_cm=int(sys.argv[1])
    arg_cm=0
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_True_ip_test"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")




    
    39440340

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(9)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_vi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

    39437636

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(8)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_v"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    39432275

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(9)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):x
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_vi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    x


    
    39432254

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(8)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):x
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_v"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    
    39430072

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=10
    Lmax=10
    mcmc_links=1

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_viii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
 

    

    39430070

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
 
    


    39429939

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(7)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):x
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_iv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    

    39423246

    
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(6)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_iii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    



39419610
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=8
    Lmax=8
    mcmc_links=1

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39419601

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=7
    Lmax=7
    mcmc_links=1

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39419560
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=6
    Lmax=6
    mcmc_links=1

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39419552

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39419550

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39419539
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=eNes=2**(3+l)
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan3_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39416075
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(5)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_ii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    

39416062
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params

    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=10
    SGD_steps=24
    B=SGD_steps*mcmc_links
    gamma=0.003
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=987134
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_xv"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39410485
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1.5e-7
    CL0=3e-8
    CP=3e-8
    CP0=3.28e-8
    s0=2**0
    
    l0=3
    lmax=10
    pmax=lmax+3
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(4)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU3_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    


39406211
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=80
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=10
    CP0=1
    s0=2**4
    pmax=10
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(4)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU2a_ip_v"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

39405748

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4 =1.45742747e+00 ,2.74535000e-03, 6.71387000e-01, 1.86269606e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params

    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=10
    SGD_steps=8
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=913
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_xiv"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39405691


fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4 =1.45742747e+00 2.74535000e-03 6.71387000e-01 1.86269606e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params

    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=10
    SGD_steps=8
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=913
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_xiv"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39379016


fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=10
    Lmax=10
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_viii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39379012

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=9
    Lmax=9
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_vii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39375721
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4 =1.47761682e+00 ,2.77612500e-03, 6.75328450e-01, 1.86235184e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params

    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=80
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=83491
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_xiii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39355858

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=80
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=10
    CP0=1
    s0=2**4
    pmax=10
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(3)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU2a_ip_iv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")

    



39355026

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4 = 1.5e+00 ,2.8e-03, 6.94350275e-01 ,1.87962188e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params

    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=80
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=459
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_xii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

39341115

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=80
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=10
    CP0=1
    s0=2**4
    pmax=10
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(2)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU2a_ip_iii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")


39340863

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4 = 1.71938902e+00 ,3.20652500e-03, 7.09435950e-01, 1.89756025e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=80
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=1398246
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_xi"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




39340202

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.73275005e+00 ,3.23095000e-03 ,7.10832200e-01, 1.90700065e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=3000
    B=SGD_steps*mcmc_links
    gamma=0.02
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=13982435
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_ll_iv"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39336803

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.73275005e+00 ,3.23095000e-03 ,7.10832200e-01, 1.90700065e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=3000
    B=SGD_steps*mcmc_links
    gamma=0.05
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=13982435
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_ll_iii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")





39334689

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.73275005e+00 ,3.23095000e-03 ,7.10832200e-01, 1.90700065e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=5000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=13982435
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_ll_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




39333022

This iteration is made to check the algorithm at low leves. It appears to wo to large number of theta1
and theta2 while at large levels these parameters decrease.

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.73275005e+00 ,3.23095000e-03 ,7.10832200e-01, 1.90700065e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=1000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=13982435
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_ll_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




39329018
39328979
39328974   39329075  39329117
39328714
39328636
39328613

39329117

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_iv_test2"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    

39329075
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_iv_test1"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39329018

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=8
    Lmax=8
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_vi"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39328979
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39328974
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39328714

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39328636
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39328613

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((1/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan2_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")





39320347
39320332
39320316
39319962
39319909



39320347

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=7
    Lmax=7
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan1_v"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39320332

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=6
    Lmax=6
    mcmc_links=1
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan1_iv"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39320316

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=5
    Lmax=5
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan1_iii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39319962
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=4
    Lmax=4
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan1_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")









39319909

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=20
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan1_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39312617

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.73275005e+00 ,3.23095000e-03 ,7.10832200e-01, 1.90700065e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=3
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.005*2e-1
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=1398246
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_x"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39311272

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=10
    N=200
    l0=7
    Lmax=8
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan_ii"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39307431
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=10
    N=200
    l0=3
    Lmax=6
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    #ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1][-1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan_i"#+str(arg_cm)
    end=time.time()
    print("Total time:",end-start,"\n")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39307131

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=10
    N=200
    l0=2
    Lmax=4
    mcmc_links=1
    

    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            SGD_steps=int((50/22)*2**(2*l))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSbVan_i"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data18/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39305706

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.73275005e+00 ,3.23095000e-03 ,7.10832200e-01, 1.90700065e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=3
    SGD_steps=40
    B=SGD_steps*mcmc_links
    gamma=0.005*2e-1
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=13982
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_ix"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39288275

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=1.74471793e+00, 3.24305000e-03, 7.11010375e-01, 1.90343294e+01
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=3
    SGD_steps=20
    B=SGD_steps*mcmc_links
    gamma=0.005*2e-1
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=14928
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_viii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39287993

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40*3*2
    l0=4
    Lmax=7
    N=200
    mcmc_links=1
    SGD_steps=100
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,10]
    ast=2
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb8_ip_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


39274000

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40*3
    l0=3
    Lmax=6
    N=200
    mcmc_links=1
    SGD_steps=200
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,10]
    ast=2
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb8_ip_i"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


    39273897

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=10000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,10]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb7_ip_i"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data15/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    39273553

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=1000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[12,3,0.6,14]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_xiii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    39273216

    fd=1e-10x
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=1000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[12,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_xii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    
    39273106

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[12,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_xi"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    39273038

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[10,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_x"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39263884

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=2000
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[8,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_ix"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39263270

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=2000
    B=SGD_steps*mcmc_links
    gamma=0.012
    gammas=[7,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_viii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




39263161

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.012
    gammas=[7,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_vii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




39262855
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.015
    gammas=[7,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_vi"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39261892
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[5,3,1,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_v"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39261527

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[5,3,2,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_iv"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39257326
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=1000
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[5,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_iii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39257254

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    start=time.time()

    samples=40
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[5,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_ii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39257110

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    #the2=the1/smean
    the2=0.01
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=200
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[3,3,0.6,6]
    alpha=0.5
    ast=1
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb6_ip_i"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data17/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




    39241550
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=2.01901483e+00, 3.77027500e-03, 7.88904675e-01, 1.95046647e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=150*3
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=14928
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_vii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    



39112196

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    
    the1,the2,the3,the4=2.01901483e+00, 3.77027500e-03, 7.88904675e-01, 1.95046647e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=150*4
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=1
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_vi"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39106138
39106108

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=3
    Lmax=6
    N=200
    mcmc_links=1
    SGD_steps=200
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb7_ip_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data16/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")



39095557


# This iteration is made in order to qunatify the bias of 
# the estimation. 

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=3
    Lmax=6
    N=200
    mcmc_links=1
    SGD_steps=200
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()

    
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb7_ip_i"#+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    
    np.savetxt("data/data16/Prl_Gen_C_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_C_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")

    

39093338

# In this realization we hcange the timestep of the varaible theta3

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=1.97707570e+00, 3.67902500e-03, 7.68023450e-01, 1.94123391e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=150
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=1
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_v"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



39093329

# This file  is imade in order to check wether the increase of level of
# time discretization change much the results of GSb_True_test.

# We also change the parameter alpha to make it run faster (this could have some
# unwanted effects on the varaince of the algorithm)


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=2.35417118e+00, 4.35565000e-03, 8.20236300e-01, 1.95978003e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
   
    samples=40
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=150
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.1
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=1
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_iv"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    


39047286
fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=2.01901483e+00, 3.77027500e-03, 7.88904675e-01, 1.95046647e+01
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=1
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_iii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


39039289

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=10
    CP0=1
    s0=2**4
    pmax=10
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(1)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU2_ip_ii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")





39038825


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=10
    CP0=1
    s0=2**4
    pmax=10
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(0)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU2_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

39037513
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the1,the2,the3,the4=2.0, 4.0e-03, 8.0981455e-01 ,1.9565340e1
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=0
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_ii"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38954820

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(6)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_vii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38954812

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(5)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_vi"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38954790

 fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(4)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_v"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




38954778

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(3)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_iv"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38954769

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(2)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_iii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




38954633


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=13
    Lmax=13
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=0
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_i"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38954619


38948949

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=1
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=0
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb_True_test"#+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




38927398

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(1)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_ii"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




38918017

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    #samples=1
    N=200
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=8
    l0=3
    lmax=10
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    seed=1+1146702*(arg_cm-1)+  1146702*30*(0)
    #seed=0
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        print(seed)
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)




    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38917951

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=400
    N=200
    l0=3
    Lmax=10
    mcmc_links=1
    #SGD_steps=10000
    #B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    CL=1
    CL0=1
    CP=1
    CP0=1
    s0=2**4
    pmax=2
    l0=3
    lmax=4
    beta_l=1/2
    beta_p=1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*(arg_cm-1)+  17*samples*30*(0)
    pars=np.zeros((samples,2,2,4))
    levels=np.zeros((samples, 2))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        
        seed+=sympy.prime(2*(sample+2))
        #print("The sample is",sample)
        args=[bdg.gamma_sampling,in_dist_pars,bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        bdg.Grad_log_gamma_in_dist,bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs, bdg.sampling_prop_log_normal_2,\
        [the1,the2,the3],bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4, bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,[the1,the2,the3],\
        bdg.Grad_log_g_nbino,resamp_coef, d,N,seed,fd,\
        mcmc_links,gamma,gammas, alpha,ast, bdg.update_pars_logis,\
        CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]


       

        inputs.append(args)




    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_Unbiased,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        pars[sample]=pool_outputs[sample][0]
        levels[sample]=pool_outputs[sample][1]
        
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levels=levels.flatten()
    #Grads_test=Grads_test.flatten()
    v="GU1_ip_test"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data16/Prl_Gen_Unbiased_levels_v"+v+".txt",levels,fmt="%f")
    np.savetxt("data/data16/Prl_Gen_Unbiased_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




38859790

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=10000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=2
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb5_ip_ii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data15/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




    38859584

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1

    start=time.time()
    samples=40
    l0=3
    Lmax=9
    N=200
    mcmc_links=1
    SGD_steps=5
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            #ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            #ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    #ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    #ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_x"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    #np.savetxt("data/data15/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("data/data15/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data15/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data15/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")

    38859078
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()
    samples=40
    l0=3
    Lmax=9
    N=200
    mcmc_links=1
    SGD_steps=5
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,1,6]
    ast=2
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_ix"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")



    38839366

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=10000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.5
    ast=5
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb5_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




38839348
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()
    samples=40
    l0=3
    Lmax=9
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,1,6]
    ast=5
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_viii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")



fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=8
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=100
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,1,6]
    alpha=0.5
    ast=5
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb4_ip_iv"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=40
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=1000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,6]
    alpha=0.5
    ast=5
    K=2*(2**ast-1)  
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb5_ip_i"+str(arg_cm)
    end=time.time()
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=8
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=1000
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,4]
    alpha=0.5
    ast=5
    K=2*(2**ast-1)  
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths=np.zeros((samples,len(eLes),B+K,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb4_ip_ii"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    38837856

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()
    samples=40
    l0=3
    Lmax=9
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,4]
    ast=5
    K=2*(2**ast-1)
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_vii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")





38836817

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=4
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.007
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    ast=2
    K=2*(2**ast-1)  
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+ast,4))
    ch_paths_0=np.zeros((samples,len(eLes),B+K,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B+K,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B+K,4))
    Grads_test_1=np.zeros((samples,len(eLes),B+K,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_test"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


38829895

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=4
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.007
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    ast=2
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha,ast, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_test"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")




38807297


    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=9
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.007
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_vi"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")



38807234


fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=5
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.007
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_v"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


####################################################################



    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    samples=8
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=2
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[2,3,0.6,3]
    alpha=0.5
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    #Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    #Grads_test=Grads_test.flatten()
    v="GSb3_ip_i"+str(arg_cm)
    end=time.time()
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    
    np.savetxt("Observationdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


    38806754

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=5
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.01
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_iv"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")




    38806375

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.01
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_iii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


    38806279

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.05
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")



      38806168

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1+5,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb6_ip_i"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


    38804989

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.005
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb5_ip_i"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")




38804923

In this sample we change the function of the step size in the SGD, this is done linearly changing
alpha as a funciton of n (the number of steps)

fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40
    start=time.time()
    N=200
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=20
    B=SGD_steps*mcmc_links
    gamma=0.005
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb2_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



# Increasing from N=50 to N=200 had an strange effect, instead of giving error it made 
# the run slower, for some reason it wouldn't finish.

    38804308
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=200
    mcmc_links=1
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.01
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb4_ip_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")

    38803699

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=50
    mcmc_links=1
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.01
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb4_ip_i"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")




    38803644

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=2
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40

    start=time.time()
    N=50
    l0=3
    Lmax=3
    mcmc_links=1
    SGD_steps=20
    B=SGD_steps*mcmc_links
    gamma=0.01
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_v"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")




    38802870

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
   
    start=time.time()
    samples=40

    start=time.time()
    N=50
    l0=3
    Lmax=3
    mcmc_links=10
    SGD_steps=20
    B=SGD_steps*mcmc_links
    gamma=0.1
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_iv"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    38801179
    In this run we use the same parameters that gave an error for the coupled version 38799817

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    N=50
    l0=3
    Lmax=3
    mcmc_links=10
    SGD_steps=20
    B=SGD_steps*mcmc_links
    gamma=0.1
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_iii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


38800180

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=50
    mcmc_links=20
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb3_ip_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")



    38799817

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=50
    mcmc_links=10
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    #Grads_test=Grads_test.flatten()
    v="GCSb3_ip_ii"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    



    La corrida que nunca fue

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    mcmc_links=10
    SGD_steps=20
    B=SGD_steps*mcmc_links
    gamma=0.1
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_iii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data14/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



    38773653

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    #the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()

    start=time.time()

    samples=40
    l0=3
    Lmax=3
    N=50
    mcmc_links=3
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb3_ip_i"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    

    38760642

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=200

    start=time.time()
    l0=3
    Lmax=9
    N=50
    mcmc_links=5
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_4"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    



    38759050

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=200

    start=time.time()
    l0=3
    Lmax=9
    N=50
    mcmc_links=3
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_3"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    



    38756973

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=100

    start=time.time()
    l0=3
    Lmax=7
    N=50
    mcmc_links=3
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_1"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    


    38756324

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=5
    N=50
    mcmc_links=3
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test9"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    



    38756288
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=4
    N=50
    mcmc_links=3
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test8"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    




    38756238

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    N=50
    mcmc_links=3
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test7"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    


    38756195

    N=100
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=0.5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    N=500
    mcmc_links=1
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.09
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[2,3,0.6,3]
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test6"+str(arg_cm)
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    

    38755728


    38749421

    N=200
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test4"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")


    38749295
N=200
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test3"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    




38749240

N=100
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test2"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    


38749161

N=100
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=5
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test2"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")

38749125

N=100
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=5
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test2"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    


38749090


38749047

    N=200
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb2_ip_test"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    




38710522
    N=200
    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=1
    the2=the1/smean
    the3=1

    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))
    the4=1 
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=7
    N=100
    mcmc_links=2
    SGD_steps=20
    B=mcmc_links*SGD_steps
    gamma=0.1
    #gammas=[1,5e-1,2e-1,1]
    #gammas=[0,3e-1,5e-1,3] # This set gave good resutls
    gammas=[6e-1,3e-1,5e-1,0.5]
    alpha=0.1
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("data/data13/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

38707250

N=200

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=3
    mcmc_links=100
    SGD_steps=1
    B=SGD_steps*mcmc_links
    gamma=0.05
    gammas=[1,3e-1,5e-1,0.5]

    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars_0=np.zeros((samples,len(eLes),SGD_steps+1,4))
    pars_1=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths_0=np.zeros((samples,len(eLes),B,int(T/d)))
    ch_paths_1=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test_0=np.zeros((samples,len(eLes),B,4))
    Grads_test_1=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.rej_max_coup_gamma_in_dist,\
            coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,\
            bdg.b_log, dist_params,bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
            bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
            bdg.rej_max_coup_log_normal_2,[the1,the2,the3,the1,the2,the3],\
            obs,obs_times,bdg.log_g_nbino_den,the4,bdg.trans_log_normal,\
            bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal_2,[the1,the2,the3],\
            bdg.Grad_log_g_nbino, resamp_coef, l, d,N,seed,fd,mcmc_links,\
            SGD_steps,gamma,gammas, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths_0[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            ch_paths_1[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            pars_0[sample,i]=pool_outputs[sample*len(eLes)+i][2]
            pars_1[sample,i]=pool_outputs[sample*len(eLes)+i][3]
            #Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths_0=ch_paths_0.flatten()
    pars_0=pars_0.flatten()
    ch_paths_1=ch_paths_1.flatten() 
    pars_1=pars_1.flatten()
    
    #Grads_test=Grads_test.flatten()
    v="GCSb_ip_test"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_0_v"+v+".txt",ch_paths_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_1_v"+v+".txt",ch_paths_1,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_0_v"+v+".txt",pars_0,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_1_v"+v+".txt",pars_1,fmt="%f")
    #np.savetxt("data/data13/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")


38618735

N=200

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=7
    mcmc_links=30
    SGD_steps=12*4
    B=SGD_steps*mcmc_links
    gamma=0.0
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma, alpha, bdg.update_pars_logis]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_ii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")



38616851
N=200

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=5
    mcmc_links=30
    SGD_steps=12
    B=SGD_steps*mcmc_links
    gamma=0.0
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")    




38616221
N=200

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    l0=3
    Lmax=5
    mcmc_links=30
    SGD_steps=6
    B=SGD_steps*mcmc_links
    gamma=0.0
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_test"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")

38609629

N=200

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    l0=5
    Lmax=6
    mcmc_links=30
    SGD_steps=3
    B=SGD_steps*mcmc_links
    gamma=0.0
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    Grads_test=np.zeros((samples,len(eLes),B,4))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
            Grads_test[sample,i]=pool_outputs[sample*len(eLes)+i][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads_test=Grads_test.flatten()
    v="GSb_ip_test"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_Grads_v"+v+".txt",Grads_test,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    




38609579
    N=200

    fd=1e-10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    smean=np.mean(obs)  
    svar=np.var(obs)
    the1=5
    the2=the1/smean
    the3=1
    the4=np.sqrt(smean+the3**2/(2*the2))/np.sqrt(svar/smean-1-the3**2/(2*the2))    

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    
    d=1
    H_pars=1
    seed=6
    start=time.time()
    samples=40

    start=time.time()
    l0=5
    Lmax=5
    mcmc_links=30
    SGD_steps=3
    B=SGD_steps*mcmc_links
    gamma=0.05
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    pars=np.zeros((samples,len(eLes),SGD_steps+1,4))
    ch_paths=np.zeros((samples,len(eLes),B,int(T/d)))
    inputs=[]
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
            bdg.b_log,dist_params, bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
            bdg.H_log_normal, bdg.update_log_functs,\
            bdg.sampling_prop_log_normal_2,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
            the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal_2,\
            bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd,mcmc_links,SGD_steps,gamma, alpha, bdg.update_pars_logis]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_SGD_bridge,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            ch_paths[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pars[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    v="GSb_ip_test"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data13/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_SGD_bridge_pars_v"+v+".txt",pars,fmt="%f")
    

38464564



def Prl_Gen_C_Grad_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,\
    the1,the2,the3,the4,the3_fd,fd]=args
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    

    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]

    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
 

    d=1
    T=len(obs_times)*d
    resamp_coef=1
    A_til=dist_params
    fi_til=the3
    r_pars=1
    H_pars=1

    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    
    ch_paths=np.zeros((2,B,int(T/d)))
    Grads=np.zeros((2,B,4))
    ch_weights=np.zeros((2,B,int(T/d)))
    
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        print(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,\
        Grads_0,Grads_1]=\
        bdg.Gen_C_Grad_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,\
        coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,dist_params_fd_0,\
        dist_params_fd_1,bdg.Sig_gbm_1d,the3,the3,the3_fd,the3_fd_1,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3],\
        bdg.Grad_log_g_nonin, resamp_coef, l, d,N,seed,fd)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        Grads[:,b]=np.array([Grads_0,Grads_1])
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    
    return Grads,ch_paths 


N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    fd=1e-10
    the3_fd=the3+fd 

    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    

    start=time.time()
    l0=4
    Lmax=8
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),2,B,int(T/d)))
    Grads=np.zeros((samples,len(eLes),2,B,4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,\
            the1,the2,the3,the4,the3_fd,fd]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_Grad_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GGCS_ip_ii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data12/Prl_Gen_C_Grad_logarithmic_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data12/Prl_Gen_C_Grad_logarithmic_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    


38453976


def Prl_Gen_C_Grad_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,\
    the1,the2,the3,the4,the3_fd,fd]=args
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]

    the3_fd=the3+fd
    the3_fd_1=the3+fd/2
    

    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]

    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
 

    d=1
    T=len(obs_times)*d
    resamp_coef=1
    A_til=dist_params
    fi_til=the3
    r_pars=1
    H_pars=1

    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    
    ch_paths=np.zeros((2,B,int(T/d)))
    Grads=np.zeros((2,B,4))
    ch_weights=np.zeros((2,B,int(T/d)))
    
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        print(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1,\
        Grads_0,Grads_1]=\
        bdg.Gen_C_Grad_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,\
        coup_in_dist_pars,bdg.Grad_log_gamma_in_dist,cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,dist_params_fd_0,\
        dist_params_fd_1,bdg.Sig_gbm_1d,the3,the3,the3_fd,the3_fd_1,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.Grad_trans_log_normal, bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3],\
        bdg.Grad_log_g_nonin, resamp_coef, l, d,N,seed,fd)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        Grads[:,b]=np.array([Grads_0,Grads_1])
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    
    return Grads,ch_paths 



N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    fd=1e-10
    the3_fd=the3+fd 

    dist_params=np.array([the1,the2,the3])
    dist_params_fd=np.array([[the1+fd,the2,the3],[the1,the2+fd,the3],[the1,the2,the3+fd]])
    dist_params_fd_1=np.array([[the1+fd/2,the2,the3],[the1,the2+fd/2,the3],[the1,the2,the3+fd/2]])
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    dist_params_0=dist_params
    dist_params_fd_0=dist_params_fd
    dist_params_1=dist_params
    
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]
    #t0=1.2
    #T=3.2
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    

    start=time.time()
    l0=3
    Lmax=7
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),2,B,int(T/d)))
    Grads=np.zeros((samples,len(eLes),2,B,4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,\
            the1,the2,the3,the4,the3_fd,fd]
            
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_Grad_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GGCS_ip_test"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data12/Prl_Gen_C_Grad_logarithmic_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data12/Prl_Gen_C_Grad_logarithmic_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    
----------------------
38439061


def Prl_Gen_Grad_smooth_2(args):


    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the4,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)




        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean




N=50
    samples=40
    B=2000*7
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_xi"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
---------------------------
38435524

def Prl_Gen_Grad_smooth_2(args):


    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the4,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_nbino_den,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd)




        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean



N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    inputs=[]
    start=time.time()
    l0=3
    Lmax=5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)        
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,\
            dist_params,dist_params_fd,the1,the2,the3,the4,the3_fd,fd]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Grad_smooth_2,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GGS2_4_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    

--------------------
38435103


def Prl_Gen_Grad_smooth_2(args):


    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the4,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_nbino_den,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        the4, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nbino,resamp_coef, l, d,N,seed,fd)




        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean



N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:8,:2]
    obs_times=rkdata[:8,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    inputs=[]
    start=time.time()
    l0=3
    Lmax=5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)        
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,\
            dist_params,dist_params_fd,the1,the2,the3,the4,the3_fd,fd]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Grad_smooth_2,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GGS2_3_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    

-----------------
38434339



def Prl_Grad_smooth(args):

    [N,seed,B,l,obs,obs_times]=args
    #the1=2
    #the2=0.1
    #the3=0.5
    #the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean


    N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_x"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    
-----------------------
38434030


def Prl_Gen_Grad_smooth_2(args):


    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1
    the4=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean



    N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    the1,the2,the3,the4=2.397, 4.429e-2, 0.84, 17.36
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)        
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,\
            dist_params,dist_params_fd,the1,the2,the3,the3_fd,fd]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Grad_smooth_2,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GGS2_2_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    



-------------------
38433975

    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1
    the4=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean    


N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)        
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,\
            dist_params,dist_params_fd,the1,the2,the3,the3_fd,fd]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Grad_smooth_2,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="cd pf"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    
------------------------------------------------------------------------------------------------------

38433822

def Prl_Gen_Grad_smooth_2(args):


    [N,seed,B,l,obs,obs_times,in_dist_pars,dist_params,dist_params_fd,the1,the2,the3,the3_fd,fd]=args
    the3=dist_params[2]
    A_til=dist_params # This parameters is vestigial, it is used in the function Gen_PF_bridge
    # but it doens't have any effect there.
    fi_til=the3 # same as the parameter above
    r_pars=1
    H_pars=1
    the4=1

    d=1

    T=len(obs_times)*d
    resamp_coef=1
    
    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    cond_whole_path=cond_path
    cwpn=cond_path
    cond_whole_log_weights=cond_log_weights
    Grad_sum=0
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    return Gradss,mcmc_mean


    
    
    N=50
    samples=40
    B=200
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    inputs=[]
    start=time.time()
    l0=3
    Lmax=5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)        
            args=[N,seed,B,l,obs,obs_times,in_dist_pars,\
            dist_params,dist_params_fd,the1,the2,the3,the3_fd,fd]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_Grad_smooth_2,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GGS2_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data11/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Plr_Gen_Grad_smooth_2_Grads_v"+v+".txt",Grads,fmt="%f")
    




--------------------------------------
38409759

    
def Prl_Grad_smooth(args):

    [N,seed,B,l,obs,obs_times]=args
    #the1=2
    #the2=0.1
    #the3=0.5
    #the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean



    
    N=50
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_ix"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    

-----------------
38409499

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=0.1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean



N=50
    samples=40
    B=10000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_viii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    

---------------------------------

38409443


def Prl_Grad_smooth(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=0.1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean

    N=50
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_vii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    


--------------
38409394

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=0.1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-1, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean

N=50
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_vi"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    
------------------------------------
38408193

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=0.1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-1, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-10
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean


N=50
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_v"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    

------------------------------------

38396076

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=0.1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-1, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-6
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean


    N=50
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_iv"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")


----------------------------
38395756

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means


N=50
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_iii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data10/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data10/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")



38395564

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means


    N=5000
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    N0=30
    p=13
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=124879+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    l=8
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            #l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL4_ip_iv"+str(arg_cm)
    end=time.time()
    print(end-start)

    np.savetxt("data/data9/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data9/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)


------------------
38395469

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means






N=5000
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    N0=30
    p=13
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=124879+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    l=8
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            #l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL4_ip_iii"+str(arg_cm)
    end=time.time()
    print(end-start)

    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)


-------------------------------
38395200

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means




N=5000
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    N0=30
    p=10
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=124879+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    l=8
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            #l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL4_ip_ii"+str(arg_cm)
    end=time.time()
    print(end-start)

    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)

----------------------------------
38395150

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means




N=5000
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    N0=30
    p=8
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=124879+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    l=6
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            #l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL4_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)

    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)


---------------------------------------
38393313



def Prl_Gen_C_smoother_logarithmic(args):


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means



N=50
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=124879+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_ix"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)




---------------------
start=time.time()
    N=50
    samples=100
    B=100*25
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=9
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_ii"#+str(arg_cm)
    end=time.time()
    print(end-start)
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")

---------------------------
38386008

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means


start=time.time()
    N=50
    samples=40
    B=2000*2
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_viii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)


-------------------
    start=time.time()
    N=50
    samples=100
    B=100
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=5
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="GS_ip_i"#+str(arg_cm)
    end=time.time()
    print(end-start)
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")


-----------------
38383395

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return ch_paths,pf_means



    start=time.time()
    start=time.time()
    N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=8
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    ch_paths=np.zeros((samples,2,B,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            ch_paths[sample,:,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    ch_paths=ch_paths.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_vii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)

------------
38382083


def Prl_Gen_C_smoother_logarithmic(args):


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means



    start=time.time()
    N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=8
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_vi"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)
-------------------------


38381946

start=time.time()
    N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=8
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_vi"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   

    mcmc_mean=np.mean(ch_paths,axis=1)
    return mcmc_mean,pf_means

-------------------------
38380896
start=time.time()
    N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=8
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_v"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)


def Prl_Gen_C_smoother_logarithmic(args):


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means



-------------------------
38379584



def Prl_Gen_C_smoother_logarithmic(args):


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means

N=50
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=6
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_iv"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)



-------------------------

38379231


def Prl_Gen_C_smoother_logarithmic(args):


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means



start=time.time()
    N=50
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=6
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_iii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)

--------------------
38378355

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means


start=time.time()
    N=50
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=6
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed_count,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time() 
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_ii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    print("The time is",end-start)

------------------
38364688


        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL16_ip_ii.py "$ARGUMENT"

Z

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]

    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


    start=time.time()
    N=100
    samples=40
    B=5000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=4
    Lmax=10
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL16_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


----------------
38364005

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GCSL3_ip_i.py "$ARGUMENT"




[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means


start=time.time()
    N=50
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=6
    #N0=30
    #p=10
    #eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    #l=6
    
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            #N=eNes[i]
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL3_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")


    print("The time is",end-start)
---------------
38362397
    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GCSL2_ip_i.py "$ARGUMENT"




def Prl_Gen_C_PF_bridge(args):

    [in_dist,in_dist_pars, b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    update_func,max_sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp,\
    prop_trans_den, ind_prop_trans_par,resamp_coef, l, d,N,seed]=args
    #(t0,x0,T,b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,\
    #max_sample_funct,sample_pars,obs,log_g_den,g_den_par, aux_trans_den,atdp,\
    #prop_trans_den,ind_prop_trans_par, resamp_coef, l, d,N,seed,crossed=False):
    
    # ARGUMENTS: 

    T=len(obs_times)*d
    log_weights_0=np.zeros((int(T/d),N))
    log_weights_1=np.zeros((int(T/d),N))
    x_pr_0=np.zeros((int(T/d),N))
    x_pr_1=np.zeros((int(T/d),N))
    #xs=np.zeros((int(T*2**l*d),N))
    int_Gs_0=np.zeros((int(T/d),N))                      
    int_Gs_1=np.zeros((int(T/d),N))
    
    np.random.seed(seed)                   
    x_new_0=in_dist(in_dist_pars,N)
    x_pr_0[0]= copy.deepcopy(x_new_0)
    log_weights_0[0]=log_g_den(obs[0],x_pr_0[0],g_den_par,crossed=False)
    x_new_1=copy.deepcopy(x_new_0)
    x_pr_1[0]= copy.deepcopy(x_new_1)
    log_weights_1[0]=log_g_den(obs[0],x_pr_1[0],g_den_par,crossed=False)

    # resampling
    weights_0=pff.norm_logweights(log_weights_0[0])
    [part_resamp, x_new_0]=bdg.multi_samp_exp(weights_0,N,x_new_0,1)
    log_weights_0[0]=log_weights_0[0,part_resamp]
    #int_Gs[0]=int_Gs[0,part_resamp]
    x_pr_0[0]=x_pr_0[0,part_resamp]
    log_weights_1[0]=copy.deepcopy(log_weights_1[0,part_resamp])

    x_pr_1[0]= copy.deepcopy(x_new_0)
    x_new_1= copy.deepcopy(x_new_0)

    meanss=np.zeros((2,int(T/d)))
    smss=np.zeros((2,int(T/d)))

    seeds_0_wp=np.zeros((int(T/d)-1,2,N),dtype=int)
    seeds_1_wp=np.zeros((int(T/d)-1,2,N),dtype=int)


    meanss[:,0]=np.sum(x_pr_0[0]*weights_0)
    smss[:,0]=np.sum(x_pr_0[0]**2*weights_0)
    
    Deltas=bdg.get_deltas(obs_times)
    
    for i in range(int(T/d)-1):
        tf=obs_times[i+1]
        ti=obs_times[i]
        x_pr_0[i+1],x_pr_1[i+1]=max_sample_funct(x_new_0,x_new_1,N,tf-ti,sample_pars)

        np.random.seed(seed+i*int(2**l*d-1))
        pars_0=[A,fi]
        pars_1=[A,fi]
        [A_til_0,A_til_1,fi_til_0,fi_til_1,r_pars_0,r_pars_1,H_pars_0,H_pars_1,\
         atdp_0,atdp_1]=update_func(pars_0,pars_1,ti,x_new_0,x_new_1,tf,x_pr_0[i+1],x_pr_1[i+1],levels=2)
        #[A_til,fi_til,r_pars,H_pars,\
        #atdp]=update_func(pars_0,pars_0,ti,x_new,x_new,tf,x_pr[i+1],x_pr[i+1],levels=1)

        # what parameters do we need in order to make the auxiliar density general?
        # x_new,  d, t, x_pr,tf
        # aux_trans_den(t0,x0,T,x_pr,atdp)
        # atdp stands for auxiliar transition density parameters. 
        seeds_0_wp[i,1,:]=np.copy(np.arange(N))
        seeds_1_wp[i,1,:]=np.copy(np.arange(N))
        seeds_0_wp[i,0,:]=seed+i*int(2**l*d-1)
        seeds_1_wp[i,0,:]=seed+i*int(2**l*d-1)
     
        int_G_0,int_G_1=bdg.C_Bridge_1d(ti,x_new_0,x_new_1,tf,x_pr_0[i+1],x_pr_1[i+1],b,A,A,Sig,fi,fi,b_til,A_til_0,A_til_1,\
        Sig_til,fi_til_0,fi_til_1,r,r_pars_0,r_pars_1,H,H_pars_0,H_pars_1,l,d,N,seed+i*int(2**l*d-1),Dt=Deltas[i]/2**l)
        #int_G=Bridge_1d(ti,x_new,tf,x_pr[i],b,A,Sig,fi,b_til,A_til,Sig_til,fi_til,\
        #r,r_pars,H,H_pars,l,d,N, seed+i*int(int(2**l*d-1)),crossed=False)
        #int_Gs[i]=int_G
        #print(xi.shape)
        #print(yi,obti-i*d,)
        #print(x_new,xi)
        #print(xi)
        #Things that could be wrong
        # observations, x_new, weights
        #observations seem to be fine

        #xs[2**l*d*i:2**l*d*(i+1)]=x[:-1]

        #print("other parameteres are:",ti,x_new,tf,x_pr[i] )
        #print("atdp is ", atdp)
        #print("object is: ", aux_trans_den(ti,x_new,tf,x_pr[i],atdp))

        log_weights_0[i+1]=int_G_0+log_g_den(obs[i+1],x_pr_0[i+1],g_den_par)\
        +np.log(aux_trans_den(ti,x_new_0,tf,x_pr_0[i+1],atdp_0))-np.log(prop_trans_den(ti,x_new_0,tf,x_pr_0[i+1],ind_prop_trans_par))
        weights_0=pff.norm_logweights(log_weights_0[i+1])
        
        log_weights_1[i+1]=int_G_1+log_g_den(obs[i+1],x_pr_1[i+1],g_den_par)\
        +np.log(aux_trans_den(ti,x_new_1,tf,x_pr_1[i+1],atdp_1))-np.log(prop_trans_den(ti,x_new_1,tf,x_pr_1[i+1],ind_prop_trans_par))
        weights_1=pff.norm_logweights(log_weights_1[i+1])

       
        #print(yi,weights)
        #seed_val=i 
        #print(weights.shape)
        x_last_0=x_pr_0[i+1]
        x_last_1=x_pr_1[i+1]
        
        #ESS=1/np.sum(weights**2)
        #ESS=1/np.sum(weights**2)
        
        meanss[0,i+1]=np.sum(x_pr_0[i+1]*weights_0)
        smss[0,i+1]=np.sum(x_pr_0[i+1]**2*weights_0)
        meanss[1,i+1]=np.sum(x_pr_1[i+1]*weights_1)
        smss[1,i+1]=np.sum(x_pr_1[i+1]**2*weights_1)

       
        [part_resamp_0,part_resamp_1, x_new_0,x_new_1]=bdg.max_coup_multi\
        (weights_0,weights_1,N,x_last_0,x_last_1,1)
        #print(part_resamp_0,part_resamp_1)
        #log_weights_0[:i+2]=log_weights_0[:i+2,part_resamp_0]
        #int_Gs_0[:i+2]=int_Gs_0[:i+2,part_resamp_0]
        #x_pr_0[:i+2]=x_pr_0[:i+2,part_resamp_0]
        #log_weights_1[:i+2]=log_weights_1[:i+2,part_resamp_1]
        #int_Gs_1[:i+2]=int_Gs_1[:i+2,part_resamp_1]
        #x_pr_1[:i+2]=x_pr_1[:i+2,part_resamp_1]
#
        #seeds_0_wp[:i+1]=np.copy(seeds_0_wp[:i+1,:,part_resamp_0])
        #seeds_1_wp[:i+1]=np.copy(seeds_1_wp[:i+1,:,part_resamp_1])
        
        #x_new=multi_samp_exp(weights,N,x_last,1)[1]
        #print(x_new.shape)
        
        
       #x_new=sr(weights,N,x_pf[i],dim)[1]
    #weights=np.reshape(norm_logweights(log_weights,ax=1),(int(T/d),N,1))
    #pf=np.sum(weights*x_pf,axis=1)
    #Filter
    #spots=np.arange(d_steps,2**l*T+1,d_steps,dtype=int)
    #x_pf=x[spots]
    #weights=norm_logweights(log_weights,ax=1)

    #print(x_pf.shape,weights.shape)
    #suma=np.sum(x_pf[:,:,1]*weights,axis=1)
    return [meanss,smss]




start=time.time()
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    #l0=4
    #Lmax=9
    N0=30
    p=10
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    l=6
    
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL2_ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")


    print("The time is",end-start)
----------------
38352577


    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL16_ip_i.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]

    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means





    N=100
    samples=40
    B=5000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL16_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-------------------
    38350807

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GCSL1_ip_ii.py "$ARGUMENT"

s

def Prl_Gen_C_smoother_logarithmic(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means

    start=time.time()
    
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    #l0=4
    #Lmax=9
    N0=30
    p=14
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    l=9
    
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL1_ip_ii"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")



38349716

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GCSL1_ip_i.py "$ARGUMENT"



def Prl_Gen_C_smoother_logarithmic(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d
    seed+=1
    start=time.time() 
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    weights=pff.norm_logweights(log_weights[-1])
    
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    Deltas=bdg.get_deltas(obs_times)
    intervals=obs_times[1:]-obs_times[:-1]
    seeds_cond[0,0]=seed
    for i in range(int(T/d)-2):
        seeds_cond[i+1,0]=seeds_cond[i,0]+int(2**l*intervals[i]/Deltas[i]) -1

    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    pf_means=np.zeros((2,B,int(T/d)))


    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
        weights_0=pff.norm_logweights(log_weights_0,ax=1)
        weights_1=pff.norm_logweights(log_weights_1,ax=1)
        pf_means[:,b]=np.array([np.sum(x_pr_0*weights_0,axis=1),np.sum(x_pr_1*weights_1,axis=1)])   


    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean,pf_means

samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3,4.7])
    #obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    #l0=4
    #Lmax=9
    N0=30
    p=14
    eNes=N0*2**np.array(range(p))
    arg_cm=int(sys.argv[1])
    l=8
    
    d=1
    T=len(obs_times)*d
    #eLes=np.array(range(l0,Lmax+1))
    eLes=eNes
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,2,B,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            N=eNes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples][0]
            pf_means[sample,:,:,i]=pool_outputs[sample+i*samples][1]
    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    v="GCSL1_ip_vi"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data8/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")



-------------------
38342162

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL15_ip_vii.py "$ARGUMENT"





    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means



    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5
    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=15
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=10
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_vii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-------------------------



38339486


    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL15_ip_vi.py "$ARGUMENT"




 
def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means

start=time.time()
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=14
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=9
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_vi"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


---------------


38338877

         #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL15_ip_v.py "$ARGUMENT"

 s


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


start=time.time()
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=13
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=8
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_v"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)
------------------------------------

38337933

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL15_ip_iv.py "$ARGUMENT"




[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


start=time.time()
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=15
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=6
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_iv"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


--------------
38337799



[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means






samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=12
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=6
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_iii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


--------------
38337522


        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL15_ip_ii.py "$ARGUMENT"



[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


start=time.time()
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=8
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=6
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

----------------------------
38337319

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL15_ip_i.py "$ARGUMENT"




[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
                
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means




start=time.time()
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=8
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=4
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-------------------
38337048

start=time.time()
    
    samples=40
    B=1
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    d=1
    T=len(obs_times)*d
    p=2
    N0=30
    eNes=N0*2**np.array(range(p))
    
    seed=1+samples*len(eNes)*30*(4010)+samples*len(eNes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eNes),int(T/d)))
    l=6
    pf_means=np.zeros((samples,len(eNes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eNes)):
            N=eNes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eNes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eNes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eNes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL15_ip_test"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


---------------------------
38317235

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL13_ip_ii.py "$ARGUMENT"



 [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means



start=time.time()
    N=500000
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(4010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL14_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)



------------------
38316020

#!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL14_ip_i.py "$ARGUMENT"





    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means



start=time.time()
    N=500000
    samples=40
    B=10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL14_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)



----------------
38315939
    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL13_ip_i.py "$ARGUMENT"





    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means



start=time.time()
    N=100000
    samples=40
    B=10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL13_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)



-------------------
38314763


    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL12_ip_i.py "$ARGUMENT"



    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


start=time.time()
    N=1000
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL12_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)
-----------------
38314285

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL11_ip_i.py "$ARGUMENT"




    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means





    start=time.time()
    N=100
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL10_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


------------------------
38312253

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL10_ip_i.py "$ARGUMENT"




[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)

    end=time.time()

    return mcmc_mean,pf_means


start=time.time()
    N=30
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()
    v="GSL10_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

------------------------

38311941

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL10_ip_test.py "$ARGUMENT"



    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    pf_means=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        weights=pff.norm_logweights(log_weights,ax=1)
        pf_means[b]=np.sum(x_pr*weights,axis=1)
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    
    end=time.time()

    return mcmc_mean,pf_means

start=time.time()
    N=30
    samples=40
    B=100
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    pf_means=np.zeros((samples,len(eLes),B,int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            pf_means[sample,i]=pool_outputs[sample*len(eLes)+i][1]

    mcmcs=mcmcs.flatten()
    pf_means=pf_means.flatten()
    end=time.time()

    v="GSL10_ip_test"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_pf_means_v"+v+".txt",pf_means,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-------------
38298896

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL9_ip_ii.py "$ARGUMENT"




def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=30
    samples=40
    B=1000*36
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL9_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


------------
38298864

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL9_ip_i.py "$ARGUMENT"




def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=30
    samples=40
    B=1000*36
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:3,:2]
    obs_times=rkdata[:3,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(2010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL9_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

-----------------
38280530 # Done with the alternative backward sampling

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL8_ip_i.py "$ARGUMENT"




    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean

    start=time.time()
    N=30
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(2010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL8_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-----------------------

38280604 # Done with the alternative backward sampling

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL7_ip_i.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean




start=time.time()
    N=100
    samples=40
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(2010)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL7_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

---------
38272350

#!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL6_ip_ii.py "$ARGUMENT"



[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean




start=time.time()
    N=30
    samples=40
    B=8000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(1)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL6_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)




-----------------------
38272290

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL6_ip_i.py "$ARGUMENT"



[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean




start=time.time()
    N=30
    samples=40
    B=8000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(0)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL6_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


----------------------
38269630

      #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL5_ip_ii.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean






start=time.time()
    N=30
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(1)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL5_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

---------------
38269617

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL5_ip_i.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean






start=time.time()
    N=30
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(0)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL5_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)



-------------------
38264464


    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL4_ip_iv.py "$ARGUMENT"




    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(7)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL4_ip_iv"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)



--------------

38263001

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL4_ip_ii.py "$ARGUMENT"




    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(6)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL4_ip_iii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)
-----------------------
38262119


    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL4_ip_ii.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(5)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL4_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)





--------------------
38262033

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL4_ip_i.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=2000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:10,:2]
    obs_times=rkdata[:10,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(4)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL4_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)




---------------------------
38256462

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL3_ip_iii.py "$ARGUMENT"




def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=5000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(6)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL3_ip_iii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


---------------------
38256449

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL3_ip_ii.py "$ARGUMENT"




def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=5000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(5)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL3_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


---------------------------
38243977
    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL3_ip_i.py "$ARGUMENT"




def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    
    
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    #seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=40
    B=5000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3])
    #obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    inputs=[]
    start=time.time()
    l0=1
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(4)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",wsample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL3_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)
------------------
38240084


        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL2_ip_vi.py "$ARGUMENT"


    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=80*5
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3])
    obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(5)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL2_ip_vi"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)




------------------

38238582

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL2_ip_v.py "$ARGUMENT"


    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3])
    obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(4)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL2_ip_v"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)



------------------
38238488
    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL2_ip_iv.py "$ARGUMENT"


    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3])
    obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL2_ip_iv"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


------------
38234761


        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL2_ip_iii.py "$ARGUMENT"


    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3])
    obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(2)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL2_ip_iii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

---------------
38234689



        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL2_ip_ii.py "$ARGUMENT"


    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3])
    obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(1)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL2_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

-----------------
38234576

        #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL2_ip_i.py "$ARGUMENT"
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean



start=time.time()
    N=30
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3])
    obs_times=np.array([1.2,2,3.5])/5

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(0)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL2_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data7/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


--------------
38222069

     #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40

    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL_ip_x.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(7)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_x"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)




-------------------------
38222066

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL_ip_ix.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(6)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_ix"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-----------------
38222062

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL_ip_viii.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(5)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_viii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

------------
38222060

    #!/bin/bash
    #SBATCH --array=1-30
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=5-00:00:00
    #SBATCH --mem=200G
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=40


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python pfiles/Par_loc_GSL_ip_vii.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(4)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_vii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

--------------------

38219641
38219626

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_vi.py "$ARGUMENT"


def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


    start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(3)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_vi"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

-------------------
38219626

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_v.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


    start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(2)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_v"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


-----------------------
38219583

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_iv.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*(1)+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_iv"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)


--------------
38205858

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_iii.py "$ARGUMENT"




    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=80
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*0+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_iii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

------------
38202956

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_ii.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=60
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*0+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_ii"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

-----------------
38202756

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_i.py "$ARGUMENT"




def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean


start=time.time()
    N=50
    samples=40
    B=500
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*0+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time()
    
    v="GSL_ip_itest"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    print("The time is",end-start)

-------------

38197624

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_GSL_ip_i.py "$ARGUMENT"



def Prl_Gen_smoother_logarithmic(args):

    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        r_pars,bdg.H_log_normal,H_pars,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean

    start=time.time()
    N=50
    samples=40*4
    B=1000
    rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3

    arg_cm=int(sys.argv[1])
    
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+samples*len(eLes)*30*0+samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
    end=time.time
    print("The time is",end-start)
    v="GSL_ip_i"+str(arg_cm)
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")




---------------

38191104



def Prl_Grad_smooth(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=0.1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    dist_params=[the1,the2,the3]

    fd=1e-6
    the1_fd=the1+fd
    the2_fd=the2+fd
    the3_fd=the3+fd

    dist_params_fd=np.array([[the1_fd,the2,the3],[the1,the2_fd,the3],[the1,the2,the3_fd]])
    in_dist_pars=dist_params
    A_til=dist_params
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=2
    d=1
    H_pars=1
    
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    
    resamp_coef=1
    d=1
    T=len(obs_times)*d

    
    
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    cond_int_G=int_Gs[:,index]

    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    Grad_sum=0

    
    for b in range(B):
        #print("sample iteration: ",i," chain iteration: ",b)
        seed+=int((int(T/d))*int(int(2**l)))
        np.random.seed(seed)
        
        #[log_weights,x_pr, cond_log_weights,cond_int_G,cond_path,seeds_cond,Grads]
        #(in_dist,in_dist_pars,Grad_log_in_dist,\
        #x_cond,seeds_cond,b,A,A_fd,Sig,fi,fi_fd,b_til,Sig_til,r,\
        #H,update_func,sample_funct,sample_pars,obs,obs_times,log_g_den,g_den_par,\
        #aux_trans_den,Grad_log_aux_trans,prop_trans_den, Grad_log_G,resamp_coef, l, d,N,seed,\
        #fd_rate)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond,Grads]=\
        bdg.Gen_Grad_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, bdg.Grad_log_gamma_in_dist, \
        cond_path,seeds_cond,bdg.b_log,dist_params,dist_params_fd,\
        bdg.Sig_gbm_1d,the3,the3_fd,bdg.b_log_aux,bdg.Sig_aux_gbm_1d,bdg.r_log_normal,\
        bdg.H_log_normal, bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_den_nonin,\
        1, bdg.trans_log_normal, bdg.Grad_trans_log_normal,bdg.trans_prop_log_normal,\
        bdg.Grad_log_g_nonin,resamp_coef, l, d,N,seed,fd)
        Grad_sum+=Grads
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    Gradss=Grad_sum/B
    mcmc_mean=np.mean(ch_paths,axis=0)
    

    return Gradss,mcmc_mean

    start=time.time()
    N=50
    samples=40
    B=5000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    #seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="2ip_vi1"+str(arg_cm)
    end=time.time()
    print(end-start)
    #np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("data/data6/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("data/data6/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")

------
38190445


#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_4ip_i.py "$ARGUMENT"



def Prl_Gen_C_smoother_logarithmic(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):

   
        
        seed+=int((int(T/d))*int(int(2**(l+1)-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)
        

start=time.time()
    N=50
    samples=40
    B=2000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7])
    obs_times=np.array([1.2,2,3.5,4.7])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for i in range(len(eLes)):
        seed_count=seed
        for sample in range(samples):
        
            l=eLes[i]
            seed_count+=sympy.prime(2*(i+2))
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for i in range(len(eLes)):
        for sample in range(samples):

            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample+i*samples]
    mcmcs=mcmcs.flatten()
    v="4ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")

------





    38184211

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_3ip_i.py "$ARGUMENT"




    start=time.time()
    N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="3ip_i"+str(arg_cm)
    end=time.time()
    print(end-start)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")



    
    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):


        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,bdg.Sig_aux_gbm_1d,\
        bdg.r_log_normal,bdg.H_log_normal,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)
        
    





    start=time.time()
    N=50
    samples=24*8
    B=10000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=14)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):


        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="ip_vi4"#+str(arg_cm)
    end=time.time()
    print(end-start)
    #np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")



start=time.time()
    N=50
    samples=24
    B=1000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3

    Lmax=5
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=14)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):


        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="ip_vi3"#+str(arg_cm)
    end=time.time()
    print(end-start)
    #np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",Grads,fmt="%f")


start=time.time()
    N=50
    samples=24
    B=1000
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=3

    Lmax=7
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    #seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    seed=101203
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    Grads=np.zeros((samples,len(eLes),4))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=14)
    pool_outputs = pool.map(Prl_Grad_smooth,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            Grads[sample,i]=pool_outputs[sample*len(eLes)+i][0]
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i][1]
    mcmcs=mcmcs.flatten()
    Grads=Grads.flatten()
    v="ip_vi3"#+str(arg_cm)
    end=time.time()
    print(end-start)
    #np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_mcmc_v"+v+".txt",mcmcs,fmt="%f")
    np.savetxt("Observationsdata/Prl_Grad_smooth_Grads_v"+v+".txt",mcmcs,fmt="%f")



38131802
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_2ip_vi.py "$ARGUMENT"



    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)


    N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=10
    Lmax=11
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(0)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="2ip_vi"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")



38131795
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_2ip_v.py "$ARGUMENT"



[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)


N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=10
    Lmax=11
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(1)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="2ip_v"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")


38131765
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_2ip_iv.py "$ARGUMENT"


[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)




    N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=10
    Lmax=11
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(2)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="2ip_iv"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")




38110248
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_2ip_iii.py "$ARGUMENT"

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)


N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(2)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="2ip_iii"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")


38110217
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_2ip_ii.py "$ARGUMENT"

[N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)

N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)+  17*samples*len(eLes)*30*(1)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="2ip_ii"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    


    38027593

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_2ip_i.py "$ARGUMENT"




    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the4=1
    #the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)
        
    return mcmc_mean


N=50
    samples=40
    B=500*10
    #rkdata= np.loadtxt("Kangaroo_data.txt")
    #obs=rkdata[:,:2]
    #obs_times=rkdata[:,2]
    
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=4
    Lmax=9
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="2ip_i"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    


    38014742

    
    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_ip_i.py "$ARGUMENT"




def Prl_Gen_C_smoother_logarithmic(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)
        


    N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    inputs=[]
    start=time.time()
    l0=5
    Lmax=8
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    seed=1+17*samples*len(eLes)*(arg_cm-1)
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
    v="ip_i"+str(arg_cm)
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    


    
    38011813

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)


    N=50
    samples=40
    B=500*4
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    seed=2
    inputs=[]
    start=time.time()
    l0=5
    Lmax=8
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
   
    v="1_3"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")


    

    38006074

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_den_nonin,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)



    N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    seed=2
    inputs=[]
    start=time.time()
    l0=5
    Lmax=8
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
   
    v="1_2"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")



    38005701


    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)

    N=50
    samples=40
    B=500
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    seed=2
    inputs=[]
    start=time.time()
    l0=5
    Lmax=8
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
   
    v="1_2"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")

    

    38005597

    
def Prl_Gen_C_smoother_logarithmic(args):

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    dist_params_0=dist_params
    dist_params_1=dist_params
    coup_in_dist_pars=[the1,the2,the3,the1,the2,the3]   
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    T=len(obs_times)*d

    
    seed+=1
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
    the4, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #print(log_weights)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path_0=x_pr[:,index]
    cond_path_1=np.copy(cond_path_0)
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    seeds_cond_0=np.copy(seeds_cond)
    seeds_cond_1=np.copy(seeds_cond)
    scn=np.copy(seeds_cond)
    ch_paths=np.zeros((2,B,int(T/d)))
    ch_weights=np.zeros((2,B,int(T/d)))
    ch_whole_paths=np.zeros((B,int(T/d)))
    ch_whole_weights=np.zeros((B,int(T/d)))
    
    for b in range(B):
        
        seed+=int((int(T/d))*int(int(2**l-1)))
        np.random.seed(seed)
        [log_weights_0,log_weights_1,x_pr_0,x_pr_1,cond_log_weights_0,cond_log_weights_1,\
        int_Gs_cond_0,int_Gs_cond_1,cond_path_0,cond_path_1,seeds_cond_0,seeds_cond_1]=\
        bdg.Gen_C_Cond_PF_bridge_back_samp(bdg.rej_max_coup_gamma_in_dist,coup_in_dist_pars,\
        cond_path_0,cond_path_1 ,seeds_cond_0,\
        seeds_cond_1,bdg.b_log,dist_params_0, dist_params_1,bdg.Sig_gbm_1d,the3,the3,\
        bdg.b_log_aux,A_til,A_til,bdg.Sig_aux_gbm_1d,fi_til,fi_til,\
        bdg.r_log_normal,r_pars,r_pars,bdg.H_log_normal,H_pars,H_pars,bdg.update_log_functs,\
        bdg.rej_max_coup_log_normal,[the1,the2,the3,the1,the2,the3],\
        obs,obs_times,bdg.log_g_nbino_den,the4,the4,bdg.trans_log_normal,1,1,\
        bdg.trans_prop_log_normal,[the1,the2,the3],[the1,the2,the3], resamp_coef, l, d,N,seed)
        #[log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        #bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        #bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,\
        #bdg.update_log_functs,\
        #bdg.sampling_prop_log_normal,[the1,the2,the3],obs,obs_times,bdg.log_g_nbino_den,\
        #the4, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        
        ch_paths[:,b]=np.array([cond_path_0,cond_path_1])
        ch_weights[:,b]=np.array([cond_log_weights_0,cond_log_weights_1])
               
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=1)
        
    

    N=50
    samples=4
    B=5
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3

    seed=2
    inputs=[]
    start=time.time()
    l0=5
    Lmax=6
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,2,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_C_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,:,i]=pool_outputs[sample*len(eLes)+i]
    mcmcs=mcmcs.flatten()
   
    v="1_1"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")


37999267
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv4_unb_ip_vii.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*6)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_vii"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    




    37968427

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv4_unb_ip_vi.py "$ARGUMENT"




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*5)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_vi"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    
    




    37948382

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40

#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv4_unb_ip_v.py "$ARGUMENT"





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*4)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_v"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    



    37948371

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        

        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        r_pars,bdg.H_log_normal,H_pars,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean





    N=50
    samples=40*4
    B=100*17*10
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3

    seed=2
    inputs=[]
    start=time.time()
    l0=5
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
   
    v="1_5"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")



    37946754
    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=2
    Lmax=9
    eLes=np.array(range(l0,Lmax+1))
    samples=100
    meanss=np.zeros((2,len(eLes),samples,len(obs)))
    smss=np.zeros((2,len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.rej_max_coup_CIR,rej_dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare,dist_params, resamp_coef, l, d,N,seed]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[:,i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[:,i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="v1_3"
    np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")



    37946137

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        r_pars,bdg.H_log_normal,H_pars,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean




    N=50
    samples=40
    B=100*17
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3

    seed=2
    inputs=[]
    start=time.time()
    l0=5
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
   
    v="1_4"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")

    
    


    

    N=50
    samples=20
    B=100*17
    rkdata= np.loadtxt("Kangaroo_data.txt")
    obs=rkdata[:,:2]
    obs_times=rkdata[:,2]
    
    #obs=np.array([1.4,2,3,4.7,5.3,6.5])
    #obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3

    seed=2
    inputs=[]
    start=time.time()
    l0=3
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
   
    v="1_3"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")

    [N,seed,B,l,obs,obs_times]=args
    the1=2
    the2=1
    the3=0.5
    the1,the2,the3,the4=2.397, 4.429e-3, 0.84, 17.36
    w=the3**2/2+the1
    xi=the2
    sigma=the3
    alpha=2*w/sigma**2-1
    theta=sigma**2/(2*xi)
    
    dist_params=[the1,the2,the3]
    
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    H_pars=1
    d=1
    
    start=time.time()    
    T=len(obs_times)*d
    np.random.seed(seed)
    #print("Seed feeded to PF_bridge is: ",seed)
    [log_weights,int_Gs,x_pr]=bdg.Gen_PF_bridge(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
    bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
    r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
    bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
    1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
    #x_mean=np.sum(pff.norm_logweights(log_weights,ax=1)*x_pr,axis=-1)
    weights=pff.norm_logweights(log_weights[-1])
    #print(weights.shape)
    index=np.random.choice(np.array(range(N)))
    cond_path=x_pr[:,index]
    cond_log_weights=log_weights[:,index]
    clwn=np.copy(cond_log_weights)
    cond_int_G=int_Gs[:,index]
    cign=np.copy(cond_int_G)
    seeds_cond=np.zeros((int(T/d)-1,2),dtype=int)
    seeds_cond[:,0]=seed+np.array(range(int(T/d)-1))*int(int(2**l-1))
    seeds_cond[:,1]=index*np.ones(int(T/d)-1)
    ch_paths=np.zeros((B,int(T/d)))
    ch_weights=np.zeros((B,int(T/d)))
    
    seed+=(int(T/d))*int(int(2**l-1))
    for b in range(B):
        
        
        seed+=int((int(T/d))*int(int(2**(l+2)-1)))
        # In this case the seed in increasing by an ammount almost proportional to 2**2, we increase
        # it by two bcs the function get deltas divides by two all intervals, the other two is put there
        # given the fact that not all intervals are the same, this is a bit of overkill, but it does the job.
        np.random.seed(seed)
        [log_weights,x_pr,cond_log_weights,int_Gs_cond,cond_path,seeds_cond]=\
        bdg.Gen_Cond_PF_bridge_back_samp(bdg.gamma_sampling,in_dist_pars, cond_path,seeds_cond,\
        bdg.b_log,dist_params,\
        bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        r_pars,bdg.H_log_normal,H_pars,\
        bdg.update_log_functs,\
        bdg.sampling_prop_log_normal,dist_params,obs,obs_times,log_g_den_nonin,\
        1, bdg.trans_log_normal,1, bdg.trans_prop_log_normal, resamp_coef, l, d,N,seed)
        #(in_dist,in_dist_pars,x_cond,seeds_cond,t0,x0,T,b,A,Sig,\
        #fi,b_til,A_til,Sig_til,fi_til,r,r_pars,H,H_pars,update_func,sample_funct,sample_pars,\
        #obs,obs_times,log_g_den,g_den_par, aux_trans_den,atdp, prop_trans_den, resamp_coef, l, d,N,seed):
        #(bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
        #bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
        #r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
        #bdg.sampling_CIR,dist_params,obs,obs_times,log_g_den_nonin,\
        #1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed)
        #seed=int((int(T/d))*int(int(2**l*d-1)))
        ch_paths[b]=cond_path
        ch_weights[b]=cond_log_weights
        #print("seed conditionals are:",seeds_cond)
    mcmc_mean=np.mean(ch_paths,axis=0)
    end=time.time()

    return mcmc_mean

    


    ############################################################################


    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=8
    Lmax=9
    eLes=np.array(range(l0,Lmax+1))
    samples=100
    meanss=np.zeros((2,len(eLes),samples,len(obs)))
    smss=np.zeros((2,len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.rej_max_coup_CIR,rej_dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare,dist_params, resamp_coef, l, d,N,seed]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_Gen_C_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[:,i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[:,i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="v1_3"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")




    N=50
    samples=2*9
    B=10000
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    seed=2
    inputs=[]
    start=time.time()
    l0=2
    Lmax=7
    d=1
    T=len(obs_times)*d
    eLes=np.array(range(l0,Lmax+1))
    mcmcs=np.zeros((samples,len(eLes),int(T/d)))
    for sample in range(samples):
        for i in range(len(eLes)):
            l=eLes[i]
            seed+=1
            #print("The sample is",sample)
            
            args=[N,seed,B,l,obs,obs_times]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_smoother_logarithmic,inputs)
    pool.close()
    pool.join()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):

        for i in range(len(eLes)):
            print(i,sample)
            mcmcs[sample,i]=pool_outputs[sample*len(eLes)+i]
            

    mcmcs=mcmcs.flatten()
   
    v="1_2"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_smoother_logarithmic_v"+v+".txt",mcmcs,fmt="%f")


    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=2
    Lmax=7
    eLes=np.array(range(l0,Lmax+1))
    samples=100
    meanss=np.zeros((2,len(eLes),samples,len(obs)))
    smss=np.zeros((2,len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.rej_max_coup_CIR,rej_dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare,dist_params, resamp_coef, l, d,N,seed]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[:,i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[:,i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="v1_2"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")



    37895863

    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=2
    Lmax=7
    eLes=np.array(range(l0,Lmax+1))
    samples=400
    meanss=np.zeros((2,len(eLes),samples,len(obs)))
    smss=np.zeros((2,len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.rej_max_coup_CIR,rej_dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare,dist_params, resamp_coef, l, d,N,seed]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[:,i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[:,i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="v1_2"
    np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")




    N=100000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    rej_dist_params=[the1,the2,the3,the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=2
    Lmax=6
    eLes=np.array(range(l0,Lmax+1))
    samples=40
    meanss=np.zeros((2,len(eLes),samples,len(obs)))
    smss=np.zeros((2,len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.rej_max_coup_CIR,rej_dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1, bdg.trans_noncentral_chisquare,dist_params, resamp_coef, l, d,N,seed]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_C_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[:,i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[:,i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="test1"
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    #np.savetxt("data/data6/Prl_Gen_C_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_C_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")

    37870318

#from Un_cox_PF_functions_def import *
#from PF_functions_def import
import PF_functions_def as pff
import bridge as bdg
import multiprocessing
import time
import math
from scipy.special import iv, gamma
import numpy as np
import matplotlib.pyplot as plt 
#import progressbar
from scipy import linalg as la
from scipy.sparse import identity
from scipy.sparse import rand
from scipy.sparse import diags
from scipy.sparse import triu

import copy
#from sklearn.linear_model import LinearRegression
from scipy.stats import ortho_group
from scipy.stats import multivariate_normal

x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*3)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")4
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_iv"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    37870285


     #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv4_unb_ip_iii.py "$ARGUMENT"



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*2)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_iii"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





    37870246


    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=8
    eLes=np.array(range(l0,Lmax+1))
    samples=400
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_log_normal,0.5,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_log_normal_test, resamp_coef, l, d,N,seed]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="mean_check_lognormal_prop_1"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")




    37863023

    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=8
    eLes=np.array(range(l0,Lmax+1))
    samples=400f
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="mean_check_8"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")



    37857723

    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=8
    eLes=np.array(range(l0,Lmax+1))
    samples=40
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="mean_check_7"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")




    37856898

    N=1000000

    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    #t0=1.2
    #T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=5
    eLes=np.array(range(l0,Lmax+1))
    samples=40
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]

    meanss=meanss.flatten()
    smss=smss.flatten()
    v="mean_check_6"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")







    N=10000

    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    t0=1.2
    T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=5
    eLes=np.array(range(l0,Lmax+1))
    samples=32
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]


    meanss=meanss.flatten()
    smss=smss.flatten()
    v="mean_check_4"
    #np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    #np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%i")
    np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%f")



    37848956
    N=1000000

    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    t0=1.2
    T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=7
    eLes=np.array(range(l0,Lmax+1))
    samples=200
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)


    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]
            
    meanss=meanss.flatten()
    smss=smss.flatten()
    
    v="mean_check_2"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%i")


    37846222

    N=1000000

    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    t0=1.2
    T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=7
    eLes=np.array(range(l0,Lmax+1))
    samples=40 
    meanss=np.zeros((len(eLes),samples,len(obs)))
    smss=np.zeros((len(eLes),samples,len(obs)))    
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)


    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            meanss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            smss[i,sample]=pool_outputs[sample*len(eLes)+i][1]
            


    meanss=meanss.flatten()
    smss=smss.flatten()
    
    v="mean_check_1"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_meanss"+v+".txt",meanss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_smss"+v+".txt",smss,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_v"+v+".txt",meanss,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Gen_PF_bridge_levels_v"+v+".txt",smss,fmt="%i")
    

    37835832
    N=1000000
    the1=2
    the2=1
    the3=0.5
    dist_params=[the1,the2,the3]
    in_dist_pars=dist_params
    t0=1.2
    T=3.2
    A_til=1
    fi_til=the3
    r_pars=1
    resamp_coef=1
    l=1
    d=1
    H_pars=1
    seed=6
    obs=np.array([1.4,2,3,4.7,5.3,6.5])
    obs_times=np.array([1.2,2,3.5,4.7,5.3,6.5])/3
    start=time.time()
    l0=1
    Lmax=8
    eLes=np.array(range(l0,Lmax+1))
    samples=200
    log_weightss=np.zeros((len(eLes),samples,len(obs),N))
    int_Gss=np.zeros((len(eLes),samples,len(obs),N))    
    x_prs=np.zeros((len(eLes),samples,len(obs),N))
    inputs=[]

    for sample in range(samples):
        seed+=1
        #print("The sample is",sample)
        for i in range(len(eLes)):
            #print("The level is",eLes[i])
            l=eLes[i]
            args=[bdg.gamma_sampling,in_dist_pars, bdg.b_log,dist_params,\
            bdg.Sig_gbm_1d,the3,bdg.b_log_aux,A_til,bdg.Sig_aux_gbm_1d,fi_til,bdg.r_log_normal,\
            r_pars,bdg.H_log_normal,H_pars,bdg.update_log_functs,\
            bdg.sampling_CIR,dist_params,obs,obs_times,bdg.log_g_den_nonin,\
            1, bdg.trans_log_normal,1,bdg.trans_noncentral_chisquare, resamp_coef, l, d,N,seed]
            inputs.append(args)


    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Gen_PF_bridge,inputs)
    pool.close()
    pool.join()

    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
        seed+=1
        for i in range(len(eLes)):
            l=eLes[i]
            print(i,sample)
            log_weightss[i,sample]=pool_outputs[sample*len(eLes)+i][0]
            int_Gss[i,sample]=pool_outputs[sample*len(eLes)+i][1]
            x_prs[i,sample]=pool_outputs[ sample*len(eLes)+i][2]

    log_weightss=log_weightss.flatten()
    int_Gss=int_Gss.flatten()
    x_prs=x_prs.flatten()
    v="mean_check_1"
    np.savetxt("data/data6/Prl_Gen_PF_bridge_v"+v+".txt",log_weightss,fmt="%f")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_v"+v+".txt",int_Gss,fmt="%i")
    np.savetxt("data/data6/Prl_Gen_PF_bridge_levels_v"+v+".txt",x_prs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    37831949

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv4_unb_ip_ii.py "$ARGUMENT"



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*1)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_ii"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




    37831923

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv4_unb_ip_i.py "$ARGUMENT"





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=14
    l0=4
    lmax=11  
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*0)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv4_unb_ip_i"+str(arg_cm)
    np.savetxt("data/data6/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data6/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





    37806969

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_vii.py "$ARGUMENT"   
    
        
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*6)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_vii"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




37806898

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_vi.py "$ARGUMENT"



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*5)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_vi"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




37794671
    
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=5-00:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_v.py "$ARGUMENT"




x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*4)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_v"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




37787977
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_iv.py "$ARGUMENT"

x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*3)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_iv"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")






37787895

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_iii.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*2)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_iii"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





    37737994

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_ii.py "$ARGUMENT"




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+40*(3000*1)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_ii"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")



    37722719

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python pfiles/Par_loc_rcv3_unb_ip_i.py "$ARGUMENT"






    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    
    l0=4
    lmax=13
    
    beta_l=1
    beta_p=1
    samples=3000
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_i"+str(arg_cm)
    np.savetxt("data/data5/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data5/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





37719589

#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=4
    lmax=13
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=1+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv3_unb_ip_i_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





37715850


#!/bin/bash
#SBATCH --array=1-10
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("21","22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"



x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*4)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_xi_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




37715834

#!/bin/bash
#SBATCH --array=1-9
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*2)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_ix_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





    37665285

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"




x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*5)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_xii_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    


    37665214

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*4)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_xi_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




    37653777

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*3)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_x_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    




    37652144    

    #!/bin/bash
#SBATCH --array=1-8
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"



x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=1000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*3)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_vi_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    







37647123

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*2)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_ix_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    




37637963
    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40

#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4+3000*1)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_viii_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    



    37626266

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"






    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=3000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*4)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_vii_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    




37624605

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=1000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*3)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_vi_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    




37621237

    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"



x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=1000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000*2)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_v_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    


37617786 This has also a number of samples 1000
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N) 
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=1000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+(40+200+1000)*30+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0
    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_iv_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    





    37613065 for this one we increase the number of samples to 1000


    #!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=1000
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+40*(30)+200*(30)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_lll_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    




    37606663

 #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=200G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=200
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+40*(30-1)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_ll_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    




    37606630

    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=100G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=200
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+40*(30-1)+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_ll_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    



    37605353
    THis job computes the same as the previous job 37605139 but with a different seed.

    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=50G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("21" "22" "23" "24" "25" "26" "27" "28" "29" "30") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    





    37605139
This job is made to compute as many as possible jobs with ibex. The slurm file is

#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=50G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





37604792  This is a test with the following slurm file and exactly the same Par_loc.py as the
two previous iterations  37604581, 37601288. What I want here is to check how this configuration works out s

#!/bin/bash
#SBATCH --array=1-5
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=50G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=40


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("2" "2" "2" "2" "2") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"




37604581, this is the same system as in 37601288, the diffrence
is that the file par_unb.slurm is different, instead of having 
--ntasks-per-core=1, it has --ntasks-per-node=1.
How this affects the computation time? 


#!/bin/bash
#SBATCH --array=1-5
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=50G
#SBATCH --ntasks=5
#SBATCH --ntasks-per-core=1


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"






37601288 This process took 44 mins to be completed, what about the others? How many 
of them can I take simultaneusly?



    #!/bin/bash
#SBATCH --array=1-5
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=50G
#SBATCH --ntasks=5
#SBATCH --ntasks-per-node=1


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb_ip_"+str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





 THis run rpocesses the unbiased estimator
 The files in terms of the seeds are 
 samples 1:40

37601224 rcv2_unb1:40 done 22 mins

37601230 rcv2_unb41:80 done 10 mins

37601233 rcv2_unb81:120 done 4 mins

37601243 rcv2_unb121:140 <--- typo, it should have been 160 instead of 140, the samples are correct tho
32 mins

37601266 rcv2_unb161:200 done 16 mins

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.2
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=11
    beta_l=1
    beta_p=1
    samples=40
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253#+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.08
    CL0=0.08
    CP=0.2
    CP0=0.07
    s0=2**0

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_unb1:40"#str(arg_cm)
    np.savetxt("data/data4/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data4/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")







37555303 This is the third iteration of this version, the first one outputted an error
37594787


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2*14
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.2
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_12"
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




37536554 I run this iteration to find out wether the memory is the responsible for ibex taking only two 
jobs simultaneously


x0_sca=1.2
x0=x0_sca
l=10
T=10
t0=0
l_d=0
d=2**(l_d)
theta_true=-0.3
sigma_true=0.8
sd_true=0.8
np.random.seed(7)
collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
resamp_coef=1
l_max=10
x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
times=np.array(range(t0,int(T/d)+1))*d
l_times=np.arange(t0,T,2**(-l))
l_max_times=np.arange(t0,T,2**(-l_max))
np.random.seed(1007)
d_times=np.array(range(t0+d,int(T/d)+1))*d
obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
resamp_coef=1
N=50
start=time.time()
mcmc_links=5
SGD_steps=32*16*2*14
#SGD_steps=2
B=mcmc_links*SGD_steps
fd=1e-8
theta_in=-1
sigma_in=1
sd_in=1
theta_in_fd=theta_in+fd
sigma_in_fd=sigma_in+fd
sigma_in_aux=sigma_in
theta_in_aux=theta_in+0.2
sigma_in_aux_fd=sigma_in_aux+fd
samples=80
gamma=0.2
alpha=0.5
seed=2393
x0=x0_sca+np.zeros(N)
pars=np.zeros((2,SGD_steps+1,3))
Grads=np.zeros((2,B,3))
ch_paths=np.zeros((2,B,int(T/d)))
inputs=[]
l=6
arg_cm=int(sys.argv[1])
for i in range(arg_cm):
    seed+=sympy.prime(2*(i+2))
args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
bdg.H_quasi_normal,\
[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
alpha]
ch_paths_0,ch_paths_1,pars_0 ,pars_1, Grads_test_0,Grads_test_1=Prl_C_SGD_bridge(args)
ch_paths[:]=[ch_paths_0,ch_paths_1]
pars[:]=[pars_0,pars_1]
Grads[:]=[Grads_test_0,Grads_test_1]
xend1=time.time()
end=time.time()
print("Parallelized processes time:",end-start,"\n")
        #lps[sample,2]=sample
#tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
#log_weightss=log_weightss.flatten()
ch_paths=ch_paths.flatten()
pars=pars.flatten()
Grads=Grads.flatten()
v=str(arg_cm)
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_vipeSestest"+v+".txt",ch_paths,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_vipeSestest"+v+".txt",pars,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_vipeSestest"+v+".txt",Grads,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


37536476 
    This run is made to get the constants for the second moment in terms of 
the time discretization, where we increased the sgd step and changed the initioal conditions



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=200*3*20
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.2
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(2*(i+2))
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_13"
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")






37536406
This run is made in order to compare to the PP version in the computation of the constants 
37534084



x0_sca=1.2
x0=x0_sca
l=10
T=10
t0=0
l_d=0
d=2**(l_d)
theta_true=-0.3
sigma_true=0.8
sd_true=0.8
np.random.seed(7)
collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
resamp_coef=1
l_max=10
x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
times=np.array(range(t0,int(T/d)+1))*d
l_times=np.arange(t0,T,2**(-l))
l_max_times=np.arange(t0,T,2**(-l_max))
np.random.seed(1007)
d_times=np.array(range(t0+d,int(T/d)+1))*d
obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
resamp_coef=1
N=50
start=time.time()
mcmc_links=5
SGD_steps=32*16*2*14
#SGD_steps=2
B=mcmc_links*SGD_steps
fd=1e-8
theta_in=-1
sigma_in=1
sd_in=1
theta_in_fd=theta_in+fd
sigma_in_fd=sigma_in+fd
sigma_in_aux=sigma_in
theta_in_aux=theta_in+0.2
sigma_in_aux_fd=sigma_in_aux+fd
samples=80
gamma=0.2
alpha=0.5
seed=2393
x0=x0_sca+np.zeros(N)
pars=np.zeros((2,SGD_steps+1,3))
Grads=np.zeros((2,B,3))
ch_paths=np.zeros((2,B,int(T/d)))
inputs=[]
l=6
arg_cm=int(sys.argv[1])
for i in range(arg_cm):
    seed+=sympy.prime(2*(i+2))
args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
bdg.H_quasi_normal,\
[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
alpha]
ch_paths_0,ch_paths_1,pars_0 ,pars_1, Grads_test_0,Grads_test_1=Prl_C_SGD_bridge(args)
ch_paths[:]=[ch_paths_0,ch_paths_1]
pars[:]=[pars_0,pars_1]
Grads[:]=[Grads_test_0,Grads_test_1]
xend1=time.time()
end=time.time()
print("Parallelized processes time:",end-start,"\n")
        #lps[sample,2]=sample
#tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
#log_weightss=log_weightss.flatten()
ch_paths=ch_paths.flatten()
pars=pars.flatten()
Grads=Grads.flatten()
v=str(arg_cm)
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_vipeSes"+v+".txt",ch_paths,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_vipeSes"+v+".txt",pars,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_vipeSes"+v+".txt",Grads,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

37534173

x0_sca=1.2
x0=x0_sca
l=10
T=10
t0=0
l_d=0
d=2**(l_d)
theta_true=-0.3
sigma_true=0.8
sd_true=0.8
np.random.seed(7)
collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
resamp_coef=1
l_max=10
x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
times=np.array(range(t0,int(T/d)+1))*d
l_times=np.arange(t0,T,2**(-l))
l_max_times=np.arange(t0,T,2**(-l_max))
np.random.seed(1007)
d_times=np.array(range(t0+d,int(T/d)+1))*d
obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
resamp_coef=1
N=50
start=time.time()
mcmc_links=5
SGD_steps=4*32
#SGD_steps=2
B=mcmc_links*SGD_steps
fd=1e-8
theta_in=-0.8
sigma_in=1
sd_in=1
theta_in_fd=theta_in+fd
sigma_in_fd=sigma_in+fd
sigma_in_aux=sigma_in
theta_in_aux=theta_in+0.2
sigma_in_aux_fd=sigma_in_aux+fd
samples=80
gamma=0.15
alpha=0.5
seed=2393
x0=x0_sca+np.zeros(N)
pars=np.zeros((2,SGD_steps+1,3))
Grads=np.zeros((2,B,3))
ch_paths=np.zeros((2,B,int(T/d)))
inputs=[]
l=6
arg_cm=int(sys.argv[1])
for i in range(arg_cm):
    seed+=sympy.prime(2*(i+2))
args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
bdg.H_quasi_normal,\
[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
alpha]
    
ch_paths_0,ch_paths_1,pars_0 ,pars_1, Grads_test_0,Grads_test_1=Prl_C_SGD_bridge(args)

ch_paths[:]=[ch_paths_0,ch_paths_1]
pars[:]=[pars_0,pars_1]
Grads[:]=[Grads_test_0,Grads_test_1]


xend1=time.time()
end=time.time()
print("Parallelized processes time:",end-start,"\n")

        
        #lps[sample,2]=sample
#tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
#log_weightss=log_weightss.flatten()
ch_paths=ch_paths.flatten()
pars=pars.flatten()
Grads=Grads.flatten()
v=str(arg_cm)
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



37534084

# This test is made in order to get the constants for the unbiased estimator


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2*14
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.2
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_12"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")











x0_sca=1.2
x0=x0_sca
l=10
T=10
t0=0
l_d=0
d=2**(l_d)
theta_true=-0.3
sigma_true=0.8
sd_true=0.8
np.random.seed(7)
collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
resamp_coef=1
l_max=10
x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
times=np.array(range(t0,int(T/d)+1))*d
l_times=np.arange(t0,T,2**(-l))
l_max_times=np.arange(t0,T,2**(-l_max))
np.random.seed(1007)
d_times=np.array(range(t0+d,int(T/d)+1))*d
obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
resamp_coef=1
N=50
start=time.time()
mcmc_links=5
SGD_steps=4
#SGD_steps=2
B=mcmc_links*SGD_steps
fd=1e-8
theta_in=-0.8
sigma_in=1
sd_in=1
theta_in_fd=theta_in+fd
sigma_in_fd=sigma_in+fd
sigma_in_aux=sigma_in
theta_in_aux=theta_in+0.2
sigma_in_aux_fd=sigma_in_aux+fd
samples=80
gamma=0.15
alpha=0.5
seed=2393
x0=x0_sca+np.zeros(N)
pars=np.zeros((2,SGD_steps+1,3))
Grads=np.zeros((2,B,3))
ch_paths=np.zeros((2,B,int(T/d)))
inputs=[]
l=6
arg_cm=int(sys.argv[1])
for i in range(arg_cm):
    seed+=sympy.prime(2*(i+2))
args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
bdg.H_quasi_normal,\
[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
alpha]
    
ch_paths_0,ch_paths_1,pars_0 ,pars_1, Grads_test_0,Grads_test_1=Prl_C_SGD_bridge(args)

ch_paths[:]=[ch_paths_0,ch_paths_1]
pars[:]=[pars_0,pars_1]
Grads[:]=[Grads_test_0,Grads_test_1]


xend1=time.time()
end=time.time()
print("Parallelized processes time:",end-start,"\n")

        
        #lps[sample,2]=sample
#tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
#log_weightss=log_weightss.flatten()
ch_paths=ch_paths.flatten()
pars=pars.flatten()
Grads=Grads.flatten()
v=str(arg_cm)
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




37519619 is the same as 37519600 but with 

#!/bin/bash
#SBATCH --array=1-10
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=10 # comment: here I changed the number of tasks from 40 to 10
#SBATCH --ntasks-per-core=1


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


37519600

x0_sca=1.2
x0=x0_sca
l=10
T=10
t0=0
l_d=0
d=2**(l_d)
theta_true=-0.3
sigma_true=0.8
sd_true=0.8
np.random.seed(7)
collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
resamp_coef=1
l_max=10
x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
times=np.array(range(t0,int(T/d)+1))*d
l_times=np.arange(t0,T,2**(-l))
l_max_times=np.arange(t0,T,2**(-l_max))
np.random.seed(1007)
d_times=np.array(range(t0+d,int(T/d)+1))*d
obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
resamp_coef=1
N=50
start=time.time()
mcmc_links=5
SGD_steps=32*4
#SGD_steps=2
B=mcmc_links*SGD_steps
fd=1e-8
theta_in=-0.8
sigma_in=1
sd_in=1
theta_in_fd=theta_in+fd
sigma_in_fd=sigma_in+fd
sigma_in_aux=sigma_in
theta_in_aux=theta_in+0.2
sigma_in_aux_fd=sigma_in_aux+fd
samples=80
gamma=0.15
alpha=0.5
seed=2393
x0=x0_sca+np.zeros(N)
pars=np.zeros((2,SGD_steps+1,3))
Grads=np.zeros((2,B,3))
ch_paths=np.zeros((2,B,int(T/d)))
inputs=[]
l=6
arg_cm=int(sys.argv[1])
for i in range(arg_cm):
    seed+=sympy.prime(2*(i+2))
args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
bdg.H_quasi_normal,\
[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
alpha]
    
ch_paths_0,ch_paths_1,pars_0 ,pars_1, Grads_test_0,Grads_test_1=Prl_C_SGD_bridge(args)

ch_paths=[ch_paths_0,ch_paths_1]
pars=[pars_0,pars_1]
Grads=[Grads_test_0,Grads_test_1]


xend1=time.time()
end=time.time()
print("Parallelized processes time:",end-start,"\n")

        
        #lps[sample,2]=sample
#tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
#log_weightss=log_weightss.flatten()
ch_paths=ch_paths.flatten()
pars=pars.flatten()
Grads=Grads.flatten()
v=str(arg_cm)
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
#np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37519548
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*4
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=80
    gamma=0.15
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="par_test1"
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data3/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    #Grads=Grads.flatten()
    #v=str(arg_cm)
    #np.savetxt("data/Prl_Unbiased_toy_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("data/Prl_Unbiased_toy_levels_v"+v+".txt",levs,fmt="%i")




    37519464

    # This is made in order to test the python parallelization compared to the full parametrization

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*4
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.15
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="par_test"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    #Grads=Grads.flatten()
    #v=str(arg_cm)
    #np.savetxt("data/Prl_Unbiased_toy_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("data/Prl_Unbiased_toy_levels_v"+v+".txt",levs,fmt="%i")


    37505231 # parallelized version

    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"



    37505063

    Unbiased estimatros for rcv2

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.1
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=12
    l0=3
    lmax=10
    beta_l=1
    beta_p=1
    samples=200
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253#+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.04
    CL0=0.02
    CP=0.004
    CP0=0.003
    s0=2**1

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="rcv2_1"#str(arg_cm)
    np.savetxt("data/data3/Prl_Unbiased_va"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data3/Prl_Unbiased_levels_va"+v+".txt",levs,fmt="%i")
    #np.savetxt("Observationsdata/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    37503629
     # This iteration is made in order to increase the stepsize of  the SGD for the SGD steps discretization constant
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2*14
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.15
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_11"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37503622

    # This iteration is made in order to increase the stepsize of  the SGD for the time discretization constant

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=200*3*20
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.15
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(2*(i+2))
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_10"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    37490243
    # This version is made in order to compare iwth the parallelized version (bellow)
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.55
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    arg_cm=1#int(sys.argv[1])
    #arg_cm=32
    seed=2397#+samples*(arg_cm-1)
    gamma=0.1
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed_p=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single"#str(arg_cm)
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_vrcv2_par"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_vrcv2_par"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_vrcv2_par"+v+".txt",Grads,fmt="%f")


    37490197

    # This run is made in order to compare with the single(above) version
    # as opposed to the parallelized.

    #!/bin/bash
    #SBATCH --array=1-15
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=72:00:00
    #SBATCH --mem=500G
    #SBATCH --ntasks=40
    #SBATCH --ntasks-per-core=1


    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15") 
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python Par_loc.py "$ARGUMENT"



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.55
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=2397+samples*(arg_cm-1)
    gamma=0.1
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed_p=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_vrcv2_par"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_vrcv2_par"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_vrcv2_par"+v+".txt",Grads,fmt="%f")

    This one is made to compare with the previous iteration rcv2_8

    37490055
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*7
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_9"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    # This is made in order to obtain the coupling constant for the distcretization in terms 
    # of the sgd steps.

    37488403

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2*14
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    
    for i in range(samples):
        seed+=sympy.prime(2*(i+2))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_8"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    # This run is made in order to get a different version of the previous experiment rcv2_6, here we 
    # change the number of mcmc_links and samples so they last approx the same time 
    37484250

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=200*3*10
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(2*i)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_7"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37487763
    # This run is made in order to check the coupling constant for the time discretization

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=200*3*20
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(2*(i+2))
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_6"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    This run is made in order to obtain in order to compare to the single version singleb3v1

    37480506
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=600*10
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_5"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")#
    




    37468888
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=200*3
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_4"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")#
    



    37468861
    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=600*10
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb3v1"
    np.savetxt("data/data2/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    # This version is made in order to check the coupling constant to use in the Unbiased function

    37462226
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=200
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_3"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")#
    




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=8
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=4
    L_max=5
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_3"
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    #mcmc_links=10
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=16
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=4
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_2"
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=16
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=4
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_1"
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.8
    sd_true=0.8
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    dim=1
    np.random.seed(1007)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=50
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=8
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=4
    L_max=6
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv2_1test"
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observationsdata/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37425845
    We increase the samples in order to get a better estimate of the variance of the estimators. 
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=100
    start=time.time()
    mcmc_links=50
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=40*6
    #samples=2
    gamma=0.7
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_7"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37409561
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=100
    start=time.time()
    mcmc_links=50
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=40
    #samples=2
    gamma=0.7
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_5"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37409472
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    for i in range(samples):
        seed+=sympy.prime(10*i+1000)
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_6"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")







    37407834

    

    37407170

    This computation is made in order to get the constants of the coupling.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=100
    start=time.time()
    mcmc_links=20
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=40
    #samples=2
    gamma=0.7
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_4"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37407167

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    for i in range(samples):
        seed+=sympy.prime(10*i+1000)
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_3"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37405738

    This sample is made in orderr to check the discretization in terms of the mcmc links.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=10
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    for i in range(samples):
        seed+=sympy.prime(10*i+1000)
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_2"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    37405943

    # This sample is made in order to check the discretization in terms of time of 
    # the process

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=100
    start=time.time()
    mcmc_links=10
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=40
    #samples=2
    gamma=0.7
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rcv1_1"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37405169


    This is made to compare to the singleb2v batch, specifically to singleb2v4, the difference between
    these two are that the current one has larger number of mcmcm_links (even larger than singleb2v10)

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*60
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v11"
    np.savetxt("data/data2/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37405163

    This is made to compare to the singleb2v batch, specifically to singleb2v4, the difference between
    these two are that the current one has larger number of mcmcm_links
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*18
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v9"
    np.savetxt("data/data2/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37404158

    Same experiment as before but with a larger number of mcmc steps. compare to Prl_Grad_chain_ch_paths_v5v2

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*3*2*20
    samples=40
    # interactive 1 samples=100
    N=200
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))
    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/data1/Prl_Grad_chain_ch_paths_v5v5.txt",ch_paths,fmt="%f")

    np.savetxt("data/data1/Prl_Grad_chain_Grads_v5v5.txt",Grads,fmt="%f")

    37404153
    This version is made to compare with Prl_Grad_chain_ch_paths_v5v2

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*3*2
    samples=40
    # interactive 1 samples=100
    N=200
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))
    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/data1/Prl_Grad_chain_ch_paths_v5v4.txt",ch_paths,fmt="%f")

    np.savetxt("data/data1/Prl_Grad_chain_Grads_v5v4.txt",Grads,fmt="%f")


    37403923

    This version is done in order to check the constant for the discretization in the level
    of time.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]




            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rc2"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    

    37404905

    # This version is done in order to compute necessary constants for the coupling in
    # the number of steps discretization.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    
    for i in range(samples):
        seed+=sympy.prime(10*i+1000)
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="rc1"
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    37403038

    This iteration is made in order to get fast result for the corrected version of r and compare to
    previous versions.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=200
    start=time.time()
    mcmc_links=int(600)
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec10"
    np.savetxt("data/data2/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37392061

# This iteration is made to compare the Gradient with the corrected function r, the comparation
# of this file is suposed to be with lower versions of the code. i.e., singlec8, singlec7

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=int(600*6*8*2*0.3)
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec9"
    np.savetxt("data/data2/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37391493

    # This sample is made to compare with the version with the correct r funcction.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=5000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    x_pf=np.zeros((len(eLes),samples,int(T/d)))

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    x_pf=x_pf.flatten()
    v="11"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")



    This iteration is made with a change in the r_quasinormal_1d funciton. 
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=36
    # interactive 1 samples=100
    N=500000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    x_pf=np.zeros((len(eLes),samples,int(T/d)))

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    x_pf=x_pf.flatten()
    v="10"
    #np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")
    np.savetxt("Observationsdata/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")


    37388345

    We run this process making the auxiliary process and the original process more similar.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.01
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=500000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    
    x_pf=np.zeros((len(eLes),samples,int(T/d)))
    

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            



            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    
    x_pf=x_pf.flatten()
    v="9"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")
    


    37384718

    We run this experiment with the same model for the auxiliary process and the hidden model.


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=int(600*6*8*2*0.4)
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec8"
    np.savetxt("data/data2/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data2/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    



    37384436

    # This iteration is made in order to compare to v=7, this one has the auxiliary process as the true process.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=50000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    
    x_pf=np.zeros((len(eLes),samples,int(T/d)))
    

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    
    x_pf=x_pf.flatten()
    v="8"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")

    37384425

    # This iteration is done to compare to v=6, we increase teh number of particle by 10 in order to decrease the variance.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=50000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    
    x_pf=np.zeros((len(eLes),samples,int(T/d)))
    

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            



            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    
    x_pf=x_pf.flatten()
    v="7"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")


    37380920

    # This iteration was made just to increase the number of particles and the maximum level of discretization
    # of a normal iteration, compare to version 5.
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=5000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    
    x_pf=np.zeros((len(eLes),samples,int(T/d)))
    

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            



            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    
    x_pf=x_pf.flatten()
    v="6"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")

    
    37380799
    This iteration is made to increase the particles and Lmax in the sample
    where the aux process and the original process are the same.
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=5000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    
    x_pf=np.zeros((len(eLes),samples,int(T/d)))
    

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            



            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    
    x_pf=x_pf.flatten()
    v="5"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")




    37380542

    # In this experiment we make the parameters of the auxiliary process the same as the original process.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=1000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10


    37380530
    # This sample is made in order to correct an error that was giving the same seed to all samples.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=1000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    
    x_pf=np.zeros((len(eLes),samples,int(T/d)))
    

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            



            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed_p]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                x_pf[k,sample]=pool_outputs[samples*k+sample]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    
    x_pf=x_pf.flatten()
    v="3"
    np.savetxt("data/data2/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")
    
    #np.savetxt("Observationsdata/Prl_PF_bridge_x_pf_v"+v+".txt",x_pf,fmt="%f")  



    37366882
    37370991

    # This iteration is made in order to test the PF_bridge function with a la rge number of particles.
    # and large nubmer Lmax
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=1000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    B=0
    inputs=[]
    log_weights=np.zeros((len(eLes),samples,int(T/d),N))
    x_pr=np.zeros((len(eLes),samples,int(T/d),N))
    int_Gs=np.zeros((len(eLes),samples,int(T/d),N))   

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]




            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                log_weights[k,sample]=pool_outputs[samples*k+sample][0]
                int_Gs[k,sample]=pool_outputs[samples*k+sample][1]
                x_pr[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    log_weights=log_weights.flatten()

    int_Gs=int_Gs.flatten()
    x_pr=x_pr.flatten()
    v="2"
    np.savetxt("data/data1/Prl_PF_bridge_log_weight_v"+v+".txt",log_weights,fmt="%f")
    np.savetxt("data/data1/Prl_PF_bridge_int_Gs_v"+v+".txt",int_Gs,fmt="%f")
    np.savetxt("data/data1/Prl_PF_bridge_x_pr_v"+v+".txt",x_pr,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_log_weight_v"+v+".txt",log_weights,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_int_Gs_v"+v+".txt",int_Gs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_x_pr_v"+v+".txt",x_pr,fmt="%f")  

    


    This iteration is made to discern wether the mistake is in the function PF_bridge or in the Cond_PF_bridge_back_samp.
    37366855
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    samples=40
    # interactive 1 samples=100
    N=50000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    B=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    log_weights=np.zeros((len(eLes),samples,int(T/d),N))
    x_pr=np.zeros((len(eLes),samples,int(T/d),N))
    int_Gs=np.zeros((len(eLes),samples,int(T/d),N))   

    for j in range(len(eLes)):
        l=eLes[j]
        seed_p=seed+sympy.prime(10*j+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)

            if seed_p>2**31:
                raise ValueError("Seed is too large")
            

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]




            args=[t0,x0,T,bdg.b_ou_1d,theta,bdg.Sig_ou_1d,sigma,bdg.b_ou_aux,theta_aux,\
            bdg.Sig_ou_aux,sigma_aux,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],bdg.sampling_ou, [theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.ou_trans_den,resamp_coef,l,d, N,seed]


            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PF_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                log_weights[k,sample]=pool_outputs[samples*k+sample][0]
                int_Gs[k,sample]=pool_outputs[samples*k+sample][1]
                x_pr[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    log_weights=log_weights.flatten()

    int_Gs=int_Gs.flatten()
    x_pr=x_pr.flatten()
    v="1"
    np.savetxt("data/data1/Prl_PF_bridge_log_weight_v"+v+".txt",log_weights,fmt="%f")
    np.savetxt("data/data1/Prl_PF_bridge_int_Gs_v"+v+".txt",int_Gs,fmt="%f")
    np.savetxt("data/data1/Prl_PF_bridge_x_pr_v"+v+".txt",x_pr,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_log_weight_v"+v+".txt",log_weights,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_int_Gs_v"+v+".txt",int_Gs,fmt="%f")
    #np.savetxt("Observationsdata/Prl_PF_bridge_x_pr_v"+v+".txt",x_pr,fmt="%f")  

    

    37366145

    This iteration makes larger the number of maximum level.


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=1
   
    samples=40
    # interactive 1 samples=100
    N=5000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=12
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="11"
    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v"+v+".txt",comp_pf_l,fmt="%f")






    # This iteration is different from the previous in the number N(5 times larger) and the max L, L_max=11

    37365101
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=1
   
    samples=40
    # interactive 1 samples=100
    N=5000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="10"
    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v"+v+".txt",comp_pf_l,fmt="%f")



    # we just increase the number N
    37361111
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=1
   
    samples=40
    # interactive 1 samples=100
    N=1000000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="9"
    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v"+v+".txt",comp_pf_l,fmt="%f")
    


    We make this run to check a single PF, not the Particle Gibbs, meaning taht B=1

    37360992

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=1
   
    samples=40
    # interactive 1 samples=100
    N=500000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="8"
    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v"+v+".txt",comp_pf_l,fmt="%f")


    




    we make this run in order to check what is the result whenever we have a large sd. 


    37360414

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=2.1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=10
   
    samples=40
    # interactive 1 samples=100
    N=50000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="7"
    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v"+v+".txt",comp_pf_l,fmt="%f")


    37303141

    #This iteration is made to compare to Prl_PG_chain_ch_paths_v5, we increase the number of N by 10 times increase L_max 
    # by one.

x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=10
   
    samples=40
    # interactive 1 samples=100
    N=500000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=11
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="6"
    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v"+v+".txt",comp_pf_l,fmt="%f")



    37302525
    # this iteration is made in roder to compare to singleb2v7 and the rest, where the only differce is N, we increase it to
    500
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=int(600*60*8)
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v8"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37302495

    # In this iteration we change three parameters, we increase by a sustantial ammount the number B compared to singleb2v6.
    # we also decrease the number N from 500 to 100 and the max level from 10 t0 7.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=100
    start=time.time()
    mcmc_links=int(600*60*8)
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v7"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37284427

    # we make this run to check the PF 

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=10
   
    samples=40
    # interactive 1 samples=100
    N=50000
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    inputs=[]
    comp_pf_l=np.zeros((len(eLes),samples,B,int(T/d)))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1


            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_PG_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                comp_pf_l[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    comp_pf_l=comp_pf_l.flatten()
    v="5"

    np.savetxt("data/data1/Prl_PG_chain_ch_paths_v5.txt",ch_paths,fmt="%f")
    np.savetxt("data/data1/Prl_PG_chain_comp_pf_v5.txt",comp_pf_l,fmt="%f")



# we make this to compare with Prl_Grad_chain_Grads_v5v2 and Prl_Grad_chain_Grads_v5, we increased the mcmc_links and 
# decreased the levels to L_max=7.
    37275212

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3*2*4
    samples=40
    # interactive 1 samples=100
    N=500
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=7
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/data1/Prl_Grad_chain_ch_paths_v5v3.txt",ch_paths,fmt="%f")

    np.savetxt("data/data1/Prl_Grad_chain_Grads_v5v3.txt",Grads,fmt="%f")


    37273388
    # THis iteration is made in order to compare with the results of singlec6 and singlec5. The changes are in the levels 
    # computed, just up to 6, the mcmc link are 6 times w.r.t. singlec5

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6*8*2*6
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=6
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec7"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37273371
    # THis run is made increasing the number of mcmc 3 times w.r..t singlec5(we also modified the seed for bacward sampling recently) and 
    # also going just to level 7
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6*8*2*3
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec6"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37272591

    # This iteration is made in order to compare with versions singleb2v4 an dsingleb2v2,
    # the changes are stricktly in the file bridge.py(second change of the seeding system), meaning that the rest of the systme 
    # including initial condiitons are the same.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*14
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v6"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37255044

    # This version is made in roder to compare with the file Prl_Grad_chain_ch_paths_v5.txt we increase several parameters
    as N, fd_rate, and B.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3*2
    samples=40
    # interactive 1 samples=100
    N=500
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/data1/Prl_Grad_chain_ch_paths_v5v2.txt",ch_paths,fmt="%f")

    np.savetxt("data/data1/Prl_Grad_chain_Grads_v5v2.txt",Grads,fmt="%f")


    37246452

    # This itertion is made to compare with singleb4, we changed three things, the amomunt of mcmc_links, the seed sysyem
    adn the fd_rate.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*10*3

    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.82
    sigma_in=0.13
    sd_in=0.22
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb5"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37246350
    # This iteteration is made in order to have a huge number of mcmc_links and compare with singleb2v3 version, we also have the recent
    # change of the bridge.py function to include more control on the seed.
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=1000
    start=time.time()
    mcmc_links=int(600*30*72/31)
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v5"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37246394
    # This iteration is made with the purpose comparing with singleb2v2, the change implemented in
    # this version is made in the file bridge.py, where we modified the seed of the the Cond_PF_bridge_back_samp funciton.


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*14
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed+sympy.prime(10*k+1000)
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v4"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37226378

    # This run is made to compare the with the single c5 session. We increase the number of 
    # mcmc_links to 600*6*8*2.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6*8*2
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec5"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37217467

    # This code is run with the purpose of having a huge array of samples so I can discart
    # the error being caused by not having enough smaples.s

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=1000
    start=time.time()
    mcmc_links=600*30
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v3"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    # This experiement is done to test wether the run of v="singleb2" was done correctly
    37217406
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*14
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2v2"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")






    37213159

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500

    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3*2
    samples=40
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=2393
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    v="a2"
    np.savetxt("data/Prl_Grad_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_Grad_chain_Grads_v"+v+".txt",Grads,fmt="%f")    




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500

    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3*2
    samples=40
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=2393
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    v="a2"
    np.savetxt("data/Prl_Grad_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_Grad_chain_Grads_v"+v+".txt",Grads,fmt="%f")




    37208616

    # This iteration is made in order to check the coupling of the SGD in terms fo the 
    # time discretization. 


    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=1000
    start=time.time()
    mcmc_links=40
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    theta_in_fd=theta_in+fd
    sigma_in=2
    sd_in=2
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=1+samples*(arg_cm-1)
    #samples=2
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vcoupledd"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vcoupledd"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vcoupledd"+v+".txt",Grads,fmt="%f")
    
    
    



    37208306
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=40
    SGD_steps=32*4
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=2
    sd_in=2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=40
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="coupled_stepsd1"
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37195748

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1
    sigma_in=2
    sd_in=2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=40
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="coupled_stepsd1"
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37195722


    #!/bin/bash
    #SBATCH --array=1-15
    #SBATCH --partition=batch
    #SBATCH -J test
    #SBATCH -o displays/test.%A.%a.out
    #SBATCH -e displays/test.%A.%a.err
    #SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
    #SBATCH --mail-type=ALL
    #SBATCH --time=72:00:00
    #SBATCH --mem=500G
    #SBATCH --ntasks=40
    #SBATCH --ntasks-per-core=1



    #run the application:
    #source ~/miniconda3/bin/activate loc 
    export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
    source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
    Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15")
    ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
    python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=20
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    theta_in_fd=theta_in+fd
    sigma_in=2
    sd_in=2
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=1+samples*(arg_cm-1)
    #samples=2
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vcoupledd"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vcoupledd"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vcoupledd"+v+".txt",Grads,fmt="%f")
    
    
    




    37195702

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=1200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=2
    sd_in=2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=425#+samples*(arg_cm-1)
    #samples=2
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]




            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singled3"
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vc"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vc"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vc"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37191920
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=1200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=425#+samples*(arg_cm-1)
    #samples=2
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10


    37190244
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=300
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=1
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]




            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singled1"
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vc"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vc"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vc"+v+".txt",Grads,fmt="%f")
    np.savetxt("Observationsdata/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observationsdata/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observationsdata/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    



    37187153
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*2
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,\
            bdg.update_pars_ou,resamp_coef,l,d, N,seed_p,fd,mcmc_links,SGD_steps,gamma,\
            alpha]




            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec4"
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vc"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vc"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vc"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observationsdata/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observationsdata/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observationsdata/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    
 


    37176895
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=10
    L_max=13
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec3"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    



    37176886

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.416
    sigma_in=0.683
    sd_in=0.594
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec2"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37165837

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-416
    sigma_in=0.683
    sd_in=0.594
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec2"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37165821

    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=40
    SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1
    theta_in_fd=theta_in+fd
    sigma_in=1
    sd_in=1
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=1+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vc"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vc"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vc"+v+".txt",Grads,fmt="%f")
    



    37165620

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.2
    sd_true=0.8
    np.random.seed(3)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*6
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1
    sigma_in=1
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlec1"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    




    37128433
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*10

    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.82
    sigma_in=0.13
    sd_in=0.22
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb4"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37127985

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=80
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=4

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="b16"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37127769

    
    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=20
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=1+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_vb"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_vb"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_vb"+v+".txt",Grads,fmt="%f")



    37127666

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600

    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.82
    sigma_in=0.13
    sd_in=0.22
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb3"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    



    37126804

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600*18
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb2"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37126787

    # we run this iteration to check(not so many mcmc links in this one) the bias of the gradient in this iteration

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=0.3 
    sd_true=0.2
    np.random.seed(0)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=600
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1.2
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleb1"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    we run the processes 37124258, 37124120 and 37124195 which are exaclty the same except by the 
    number of particles. The processes 37124258, 37124120 and 37124195 have respectively 50, 500 and 1000 particles.
    The times of the processes are 23:34, 27:07 and 32:52.



    37124258

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1
    sd_true=0.55
    np.random.seed(9)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(5)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=300
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd

    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")

            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):

            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleN50"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37124195
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1 
    sd_true=0.55
    np.random.seed(9)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(5)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=1000
    start=time.time()
    mcmc_links=300
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleN1000"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37124120
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1 
    sd_true=0.55
    np.random.seed(9)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(5)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=300
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singleN500"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37116084
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.6
    sigma_true=0.5
    sd_true=0.3
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-1.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sd_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    samples=200
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="a20"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37116062

#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3"  "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.6
    sigma_true=0.5
    sd_true=0.3
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=20
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sd_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=400
    seed=1+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_va"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_va"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_va"+v+".txt",Grads,fmt="%f")



    37116046

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.6
    sigma_true=0.5
    sd_true=0.3
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=500*10
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-1.67
    sigma_in=1.6
    sd_in=0.7
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="singlea1"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    # This iteration is made so we can test the current Grad function with a large number of samples
    # and particles.

    37106447

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=500*10
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1.2
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single17"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    37106178

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=200
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single16"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")






    37106081, 37105968 time= 17:38, 37105859


    37106081

    #!/bin/bash
#SBATCH --array=1-2
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    sd_true=1.3
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=200
    start=time.time()
    mcmc_links=200
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=0.5
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single_par_"+str(13+arg_cm)
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37105968
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    sd_true=1.3
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=200
    start=time.time()
    mcmc_links=200
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=0.5
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253+samples#*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single15"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37105968
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    sd_true=1.3
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=200
    start=time.time()
    mcmc_links=200
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=0.5
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253+samples#*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single15"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37105859

    # This iteration has a large number of particles N=200.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=200
    start=time.time()
    mcmc_links=200
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single13"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



37105798

    # this iteration has a relatrively large mcmc_links, and additionally has 
    # slightly different parameters.

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    sd_true=0.55
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    
    np.random.seed(3+2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=500*45
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    sigma_in=1.6
    sd_in=1
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single12"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37104496

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=500*45
    #mcmc_links=10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=1
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    


    for k in range(len(eLes)):
        l=eLes[k]
        seed_p=seed
        for i in range(samples):
            seed_p+=(B+1)+sympy.prime(10*i+1000)
            # for large simulations we might run out of seeds, thus 
            # we reuse some of them plus a prime number, making sure
            # the samples are not correlated
            if seed_p>2**31:
                raise ValueError("Seed is too large")
                
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed_p,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single11"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37101328

    #!/bin/bash
#SBATCH --array=1-10
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(34)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single10v"+str(arg_cm)
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37101289
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7+2)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(34)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single10"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37101099

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single9"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37101014
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single8"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    


    37094256
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single7"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37094245

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=500*45
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=1
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single6"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37094226

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.67
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50

    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3
    samples=40
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=2393
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    v="a1"
    np.savetxt("data/Prl_Grad_chain_ch_paths_v"+v+".txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v"+v+".txt",Grads,fmt="%f")



    37067241

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=1
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single5"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    ######################################################################
    ######################################################################
    ######################################################################



37066810
x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single4"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    ######################################################################
    ######################################################################
    ######################################################################

    
   






    37066623.0

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single3"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    ######################################################################
    ######################################################################
    ######################################################################



    37065966
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes), samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,B,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[k,sample]=pool_outputs[sample+samples*k][0]
            pars[k,sample]=pool_outputs[sample+samples*k][1]
            Grads[k,sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single2"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    37065627

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    #arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253#+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=10
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        l=eLes[k]
        for sample in range(samples):
            
            ch_paths[sample]=pool_outputs[sample+samples*k][0]
            pars[sample]=pool_outputs[sample+samples*k][1]
            Grads[sample]=pool_outputs[sample+samples*k][2]
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="single2"
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    #np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    ######################################################################
    ######################################################################
    ######################################################################



    37047938

    #!/bin/bash
#SBATCH --array=1-10
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=500
    start=time.time()
    mcmc_links=400
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    samples=40
    seed=4253+samples*(arg_cm-1)
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=11
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm+83)
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_grad_bias/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")






    37042810

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=200
    start=time.time()
    mcmc_links=200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=11
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="83"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")
    


    37040948

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5*8
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-12
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=200
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=11
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="82"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



37039428

    #!/bin/bash
#SBATCH --array=1-10
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=10
    SGD_steps=32*16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    gamma=0.4
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm+61)
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


37039258

#!/bin/bash
#SBATCH --array=1-10
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("11" "12" "13" "14" "15" "16" "17" "18" "19" "20")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=10
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    #samples=2
    gamma=0.3
    alpha=0.5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm+61)
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")





    37039186

#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=10
    SGD_steps=32*16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    gamma=0.4
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v=str(arg_cm+61)
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37028724


    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1

#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.1
    CL0=0.1
    CP0=0.1
    CP=0.1
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_va"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_va"+v+".txt",levs,fmt="%i")


    37028707X
    #!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1


#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15") 
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"
x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5 
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.1
    CL0=0.1
    CP0=0.1
    CP=0.1
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_va"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_va"+v+".txt",levs,fmt="%i")



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    37028658 
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5*8
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=80
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=11
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="61"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    37028326
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5*8
    SGD_steps=1
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=80
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="60"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    37026361
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=8
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=4*200
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="59"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")






37015321

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=200
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="58"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


37015308

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=200
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="57"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    37011762

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=80
    #samples=2
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="56"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=24
    gamma=0.3
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="55"
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=80
    #samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9

    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="54"
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=24
    #samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="53"
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=24
    #samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=7
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="52"
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=5
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=24
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="51"
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")



37004654

#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("76" "77" "78" "79" "80" "81" "82" "83" "84" "85" "86" "87" "88" "89" "90")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=12
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(100+arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




37004654

#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("61" "62" "63" "64" "65" "66" "67" "68" "69" "70" "71" "72" "73" "74" "75")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=12
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(100+arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




36998622

#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("46" "47" "48" "49" "50" "51" "52" "53" "54" "55" "56" "57" "58" "59" "60")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=12
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(100+arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




36998537
#!/bin/bash
#SBATCH --array=1-15
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=72:00:00
#SBATCH --mem=500G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44" "45")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=12
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(100+arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





36980269
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=48:00:00
#SBATCH --mem=1000G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=7
    l0=2
    lmax=12
    beta_l=1
    beta_p=1
    samples=40
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+samples*(arg_cm-1)
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(100+arg_cm)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




36977181

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    fd=1e-4
    gamma=0.5
    alpha=0.5
    theta_in=-0.67
    sigma_in=1.6
    sd_in=0.7
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=20
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=1
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=10
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)


    v="single1"    
    # FROM HERE
    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()

    np.savetxt("data/Prl_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

   


36975521
#!/bin/bash
#SBATCH --array=1-30
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=48:00:00
#SBATCH --mem=100G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44" "45" "46" "47" "48" "49" "50" "51" "52" "53" "54" "55" "56" "57" "58" "59" "60" "61" "62")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5

    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    arg_cm=int(sys.argv[1])
    #arg_cm=32
    seed=4253+ 5*40*5+samples*arg_cm
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm+38)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




36974723

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*2
    #arg_cm=int(sys.argv[1])
    arg_cm=32
    seed=4253+ 5*40*5+samples*arg_cm
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm+38)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

36974678

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*2
    #arg_cm=int(sys.argv[1])
    arg_cm=31
    seed=4253+ 5*40*5+samples*arg_cm
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm+38)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


36974537

#SBATCH --array=1-2
#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=48:00:00
#SBATCH --mem=100G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("31" "32")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"


x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*2
    arg_cm=int(sys.argv[1])
    seed=4253+ 5*40*5+samples*arg_cm
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm+38)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")







 

    36966237

#!/bin/bash
#SBATCH --array=1-30

#SBATCH --partition=batch
#SBATCH -J test
#SBATCH -o displays/test.%A.%a.out
#SBATCH -e displays/test.%A.%a.err
#SBATCH --mail-user=miguelangel.alvarezballesteros@kaust.edu.sa
#SBATCH --mail-type=ALL
#SBATCH --time=48:00:00
#SBATCH --mem=1000G
#SBATCH --ntasks=40
#SBATCH --ntasks-per-core=1



#run the application:
#source ~/miniconda3/bin/activate loc 
export CONDA_PKGS_DIRS=/ibex/users/alvarem/conda_cache
source /ibex/user/alvarem/miniconda3/bin/activate conda activate /home/alvarem/miniconda3/envs/loc
Array=("1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30")
ARGUMENT=${Array[$SLURM_ARRAY_TASK_ID-1]}
python Par_loc.py "$ARGUMENT"

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*2
    arg_cm=int(sys.argv[1])
    seed=4253+ 5*40*5+samples*arg_cm
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm+38)
    np.savetxt("data/data_unb_ou/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/data_unb_ou/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    



    start=time.time()
    inputs=[]
    
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=16*3
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    samples=100
    arg_cm=sys.argv
    print(arg_cm  )
    seed=int(arg_cm)
    K=10
    pars=np.zeros((samples,2,K))
    levs=np.zeros((samples,2),dtype=int)
    
    for i in range(samples):
        seed+=10
        args= [seed,K, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased_test,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v=str(arg_cm)
    np.savetxt("data/Prl_Unbiased_toy_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_toy_levels_v"+v+".txt",levs,fmt="%i")



    36952194
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=4253+ 5*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="38"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")



    
    36952175
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=4253+ 4*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="37"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")



    
    36952156
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=4253+ 3*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="36"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36952150
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=4253+ 2*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="35"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    36952097

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=4253+ 1*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="34"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    

    36952094
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=2
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=4253+ 0*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3

    for i in range(samples):
        seed+=1
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="33"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    
    36933711
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13+6*samples*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="32"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    36933708
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13+5*samples*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="31"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36933707

    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13+4*samples*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="30"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36933706

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13+3*samples*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="29"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36933160
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13+2*samples*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="28"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




    36933139
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13+samples*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="27"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")
    

    36933084

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5*5
    seed=3332+8*40*5*13
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="26"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")



    36933039

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+7*13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="25"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    

    36933016
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+6*13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="24"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")    


    36932982
    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+5*13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="23"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")



    36932949

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+4*13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="22"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36932387
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+3*13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="21"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")




    36932384

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+2*13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="20"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36932380
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    seed=3332+13*samples
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="19"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    36932375
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=3332
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=40*5
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=13
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="18"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")

    toytest

    start=time.time()
    inputs=[]
    
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=16*3
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    samples=10000
    seed=1003
    K=10000
    pars=np.zeros((samples,2,K))
    levs=np.zeros((samples,2),dtype=int)
    
    for i in range(samples):
        seed+=10
        args= [seed,K, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased_test,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="toytest4"
    np.savetxt("Observations&data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")



    36924840

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=3333
    x0=x0_sca+np.zeros(N)
    inputs=[]
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=120
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=10
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="test4"
    np.savetxt("data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")





    start=time.time()
    inputs=[]    
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=16*3
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    samples=100
    seed=1002
    K=1000
    pars=np.zeros((samples,2,K))
    levs=np.zeros((samples,2),dtype=int)
    
    for i in range(samples):
        seed+=10
        args= [seed,K, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased_test,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    v="toytest4"
    np.savetxt("Observations&data/Prl_Unbiased_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_Unbiased_levels_v"+v+".txt",levs,fmt="%i")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    inputs=[]
    
    pmax=5
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=16
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    CL=0.03
    CL0=0.1
    CP0=0.016
    CP=0.01
    s0=2**3
    for i in range(samples):
        seed+=101
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=16)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_Unbiased_vtest3.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_Unbiased_levels_vtest3.txt",levs,fmt="%i")



    36885484
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true=bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd

    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56

    CP0=1.60118386
    CP=0.23076467*2
    s0=2**3
    pmax=8
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    samples=80
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=102
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v17.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v17.txt",levs,fmt="%i")



    36884281

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*2
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="50"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    #samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="49"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")


    36883624
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    #samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="49"
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")

    36870277
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true=bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    mcmc_links=40

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd

    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56

    CP0=1.60118386
    CP=0.23076467*2
    s0=2**3
    pmax=8
    l0=5
    lmax=10
    beta_l=1
    beta_p=1
    samples=30
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=102
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v16.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v16.txt",levs,fmt="%i")



    36870271
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true=bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    mcmc_links=40

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd

    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56

    CP0=1.60118386
    CP=0.23076467*2
    s0=2**3
    pmax=8
    l0=3
    lmax=10
    beta_l=1
    beta_p=1
    samples=100
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=102
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v17.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v17.txt",levs,fmt="%i")



    36870257

    


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true=bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    mcmc_links=40

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd

    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56
    CP0=1.60118386
    CP=0.23076467*2
    s0=2**3
    pmax=8
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=16*10
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=101
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=16)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_Unbiased_vtest2.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_Unbiased_levels_vtest2.txt",levs,fmt="%i")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true=bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    mcmc_links=40

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd

    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56
    CP0=1.60118386
    CP=0.23076467*2
    s0=2**3
    pmax=8
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=16
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=100
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=16)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_Unbiased_vtest1.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_Unbiased_levels_vtest1.txt",levs,fmt="%f")



    36861288

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5
    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**10
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)


    v="37b"    
    # FROM HERE
    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    # TO HERE IT SHOULD BE COMMENTED
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    print("Parallelized processes time:",end-start,"\n")


    36860587

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5
    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**10
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="37a"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    #################################
    # FROM HERE 
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    # TO HERE IT SHOULD BE COMMENTED
    #################################

    print("Parallelized processes time:",end-start,"\n")


    36860386
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5
    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="36"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")

    36828168
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd

    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56
    CP0=1.60118386
    CP=0.23076467*2
    s0=2**0
    pmax=10
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=100
    
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=100
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v15.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v15.txt",levs,fmt="%f")




    36825011

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5
    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="35"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    36825008

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5
    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**10
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="35"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]  
    
    ch_paths=ch_paths.flatten()
    pars=pars.flatten() 
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    36812286
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5
    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="34"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps
    samples=70
    #samples=2

    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0

    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=38)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="34"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")




    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=70
    #samples=2



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**5
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test33"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**3
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test32"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=1.5
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test31"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.04
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test30"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.05
    sigma_in=0.5
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test29"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")

    36811168

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v48.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v48.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v48.txt",Grads,fmt="%f")




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.0001
    sigma_in=0.5
    sd_in=0.11
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test29"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=80
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**1
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test28"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=10
    SGD_steps=2**1
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test26"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")




    36810395
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    #samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    #L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]

            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v47.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v47.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v47.txt",Grads,fmt="%f")




x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    samples=2
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    L_max=2
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v47test.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v47test.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v47test.txt",Grads,fmt="%f")





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**1
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="test25"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**4
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="25"
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    36539253

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.02
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=20
    SGD_steps=2**9
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="24"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    36536411

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.01
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=20
    SGD_steps=2**8
    #SGD_steps=2**1
    B=mcmc_links*SGD_steps

    samples=30
    #samples=2
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    #l=4
    #l=4
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="23"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.01
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=20
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    samples=10
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="22"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    36536375

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15
    #B=50
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=6
    #L_max=3
    seed=0
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain_an,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_an_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_Grad_chain_an_Grads_v3.txt",Grads,fmt="%f")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=40
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=20
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    samples=10
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="21"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=20
    SGD_steps=2**4
    B=mcmc_links*SGD_steps
    samples=10
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="20"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.8
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    samples=10
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="19"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")





x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(4)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=1.2
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    samples=10
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="18"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    36533292
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.2
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**4
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=9
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="17"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")

    36532950
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.2
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**4
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="16"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")






    36532520
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.1
    sigma_in=0.7
    sd_in=0.2
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**2
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="15"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")
    




    36532325

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3
    
    samples=30
    
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    #
    L_max=10
    
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain_new,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_new_ch_paths_v1.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_new_Grads_v1.txt",Grads,fmt="%f")


    
    36532241

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05/7
    alpha=0.5

    theta_in=-0.7
    sigma_in=0.7
    sd_in=0.1
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=100
    SGD_steps=2**7
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=8
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="14"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")

    

    
    36480596
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.7
    sigma_in=0.7
    sd_in=0.1
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=50
    SGD_steps=2**4
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=7
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="13"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    36464344
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.7
    sigma_in=0.5
    sd_in=1.1
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=100
    SGD_steps=2**8
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="12"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")
    



    36464281

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3
    #B=50
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    #L_max=3
    seed=0
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain_an,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_an_ch_paths_v2.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_Grad_chain_an_Grads_v2.txt",Grads,fmt="%f")



    36464277
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v13.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v13.txt",Grads,fmt="%f")



    36464133
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.7
    sigma_in=0.5
    sd_in=1.1
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=100
    SGD_steps=2**4
    B=mcmc_links*SGD_steps
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="11"

    np.savetxt("data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()


    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")





    36450465
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15
    
    samples=30
    
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=7
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1
            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain_an,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]

    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_an_ch_paths_vt1.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_Grad_chain_an_Grads_v1.txt",Grads,fmt="%f")
    



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=theta_true
    sigma_in=sigma_true
    sd_in=sd_true
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=100
    SGD_steps=2**7
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="10"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=theta_true
    sigma_in=sigma_true
    sd_in=sd_true
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=100
    SGD_steps=2**3
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="9"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")




x0_sca=1.2
    x0=x0_sca
    l=10
    T=30
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-6
    gamma=0.05
    alpha=0.5

    theta_in=theta_true
    sigma_in=sigma_true
    sd_in=sd_true
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=200
    SGD_steps=2**0
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    v="8"

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")











    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    #SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-8
    theta_in=-0.8
    sigma_in=1.8
    sd_in=0.9
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    v="46"
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v"+v+".txt",Grads,fmt="%f")




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.8
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.6
    sigma_in=0.8
    sd_in=1.4
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    start=time.time()
    mcmc_links=100
    SGD_steps=2**6
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    v="7"

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=20
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.8
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.05
    alpha=0.5

    theta_in=-0.6
    sigma_in=0.8
    sd_in=1.4
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    



    start=time.time()
    mcmc_links=500
    SGD_steps=2**1
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    v="6"

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.8
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.1
    alpha=0.5

    theta_in=-0.6
    sigma_in=0.8
    sd_in=1.4
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    



    start=time.time()
    mcmc_links=500
    SGD_steps=2**1
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    v="5"

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.8
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.1
    alpha=0.5

    theta_in=-0.6
    sigma_in=0.8
    sd_in=1.4
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    



    start=time.time()
    mcmc_links=100
    SGD_steps=2**1
    B=mcmc_links*SGD_steps
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]
    v="4"

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v"+v+".txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v"+v+".txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v"+v+".txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v"+v+".txt",Grads,fmt="%f")

    print("Parallelized processes time:",end-start,"\n")



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.8
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    fd=1e-10
    gamma=0.1
    alpha=0.5

    theta_in=-0.6
    sigma_in=0.8
    sd_in=1.4
    theta_in_aux=theta_in+0.2
    sigma_in_aux=sigma_in
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux_fd=sigma_in_aux+fd
    



    start=time.time()
    mcmc_links=100
    SGD_steps=2**1
    B=mcmc_links*SGD_steps
    samples=5
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=6
    seed=0
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()

    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v3.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v3.txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v3.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v3.txt",Grads,fmt="%f")


    print("Parallelized processes time:",end-start,"\n")




    x0_sca=1.2
    x0=x0_sca
    l=10
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-10
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    gamma=0.5
    alpha=0.5
    start=time.time()
    mcmc_links=100
    SGD_steps=2**0
    B=mcmc_links*SGD_steps
    samples=5
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l=5
    seed=0
    
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))
    inputs=[]


        
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta,theta_fd,bdg.Sig_ou_1d,\
        sigma,sigma_fd,bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,\
        sigma_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
        [bdg.ou_sd,[theta_aux,sigma_aux_fd],theta_aux],\
        bdg.sampling_ou,[theta_aux,sigma_aux],\
        obs,bdg.log_g_normal_den,sd, bdg.ou_trans_den,[theta_aux,sigma_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge_vanilla,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_ch_paths_v1.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_pars_v1.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_van_comparison_Grads_v1.txt",Grads,fmt="%f")


    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,SGD_steps,3))
    ch_paths=np.zeros((samples,B,int(T/d)))

    pool = multiprocessing.Pool(processes=12)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_ch_paths_v1.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_pars_v1.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_comparison_Grads_v1.txt",Grads,fmt="%f")

    

    print("Parallelized processes time:",end-start,"\n")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.4
    sigma_true=0.6
    np.random.seed(40)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.4
    np.random.seed(67)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=-1e-6
    theta_fd=theta_true+fd
    
    sigma_fd=sigma_true+fd
    
    sigma_aux_fd=sigma_aux+fd

    start=time.time()
    mcmc_links=500*10    
    SGD_steps=2
    B=SGD_steps*mcmc_links
    samples=10
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=6
    
    eLes=np.array(range(l0,L_max+1))
    gamma=0.5
    alpha=0.5
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta,theta_fd,bdg.Sig_ou_1d,sigma,sigma_fd,\
            bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,sigma_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],[bdg.ou_sd,[theta_aux,sigma_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],[bdg.ou_sd,[theta_aux,sigma_aux_fd],theta_aux],\
            bdg.rej_max_coup_ou, [theta_aux,sigma_aux,theta_aux,sigma_aux],obs,bdg.log_g_normal_den,sd,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_aux,sigma_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/new_Prl_C_SGD_ou_bridge_ch_paths_v4.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/new_Prl_C_SGD_ou_bridge_pars_v4.txt",pars,fmt="%f")
    np.savetxt("Observations&data/new_Prl_C_SGD_ou_bridge_Grads_v4.txt",Grads,fmt="%f") 



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.4
    sigma_true=0.6
    np.random.seed(40)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.4
    np.random.seed(67)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=-1e-6
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*10
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=6
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_Grad_chain_ch_paths_v8.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_Grad_chain_Grads_v8.txt",Grads,fmt="%f")





    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.1
    sigma_true=0.4
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1.4
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*10
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_Grad_chain_ch_paths_v12.txt",ch_paths,fmt="%f")

    np.savetxt("Observations&data/Prl_Grad_chain_Grads_v12.txt",Grads,fmt="%f")

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.4
    sigma_true=0.6
    np.random.seed(40)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.4
    np.random.seed(67)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-10
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*10
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_Grad_chain_ch_paths_v7.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_Grad_chain_Grads_v7.txt",Grads,fmt="%f")





    This run is made in order to compare the SGD_bridge and the Prl_Grad_chain functions

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-6
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    gamma=0.5
    alpha=0.5
    start=time.time()
    mcmc_links=500*10
    SGD_steps=1
    B=mcmc_links*SGD_steps
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=6
    seed=0
    eLes=np.array(range(l0,L_max+1))
    
    pars=np.zeros((len(eLes),samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,SGD_steps,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))
    inputs=[]

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta,theta_fd,bdg.Sig_ou_1d,\
            sigma,sigma_fd,bdg.b_ou_aux,theta_aux,bdg.Sig_ou_aux,sigma_aux,\
            sigma_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_aux,sigma_aux]],\
            [bdg.ou_sd,[theta_aux,sigma_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_aux,sigma_aux],theta_aux],\
            [bdg.ou_sd,[theta_aux,sigma_aux_fd],theta_aux],\
            bdg.sampling_ou,[theta_aux,sigma_aux],\
            obs,bdg.log_g_normal_den,sd, bdg.ou_trans_den,[theta_aux,sigma_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                pars[k,sample]=pool_outputs[samples*k+sample][1]
                Grads[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_ch_paths_v5.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_pars_v5.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_ou_bridge_Grads_v5.txt",Grads,fmt="%f")


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-6
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*10
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=6
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=8)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_Grad_chain_ch_paths_v11.txt",ch_paths,fmt="%f")

    np.savetxt("Observations&data/Prl_Grad_chain_Grads_v11.txt",Grads,fmt="%f")


    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.25
    sigma_true=0.8
    np.random.seed(40)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1.2
    np.random.seed(67)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*10
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=7
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_Grad_chain_ch_paths_v6.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_Grad_chain_Grads_v6.txt",Grads,fmt="%f")




    36420088
        x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.25
    sigma_true=0.8
    np.random.seed(40)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1.2
    np.random.seed(67)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-6
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*15*2
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_Grad_chain_ch_paths_v5.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_Grad_chain_Grads_v5.txt",Grads,fmt="%f")

    
    


    36418828

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=4
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-8
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*3
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v10.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v10.txt",Grads,fmt="%f")



    36400981
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(45)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-5
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*4*2
    samples=40
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=10
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v9.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v9.txt",Grads,fmt="%f")



    36395887
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.25
    sigma_true=0.8
    np.random.seed(40)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1.2
    np.random.seed(67)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*15*2
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=10
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_Grad_chain_ch_paths_v4.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_Grad_chain_Grads_v4.txt",Grads,fmt="%f")



    36395677
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(45)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*2
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v8.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v8.txt",Grads,fmt="%f")

    

    36395263

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(45)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15*2
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v7.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v7.txt",Grads,fmt="%f")

    




    36392617

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(2)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*15
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_Grad_chain_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_Grad_chain_Grads_v3.txt",Grads,fmt="%f")

    
    

    36392610

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=3
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd
    sigma_fd=sigma_true+fd
    sigma_aux_fd=sigma_aux+fd
    start=time.time()
    B=500*15
    samples=20
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v6.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v6.txt",Grads,fmt="%f")


    36390164
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd*0.5
    sigma_fd=sigma_true+fd*0.5
    sigma_aux_fd=sigma_aux+fd*0.5
    start=time.time()
    B=500*15*3
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v5.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v5.txt",Grads,fmt="%f")


    36390094

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*15*4
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        seed=0
        for i in range(samples):
            seed+=1

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_Grad_chain_ch_paths_v2.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_Grad_chain_Grads_v2.txt",Grads,fmt="%f")

    



    36383003

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd_0=theta_true+fd
    theta_fd_1=theta_true+fd*0.5
    sigma_fd_0=sigma_true+fd
    sigma_fd_1=sigma_true+fd*0.5
    sigma_aux_fd_0=sigma_aux+fd
    sigma_aux_fd_1=sigma_aux+fd*0.5
    start=time.time()
    B=500*15
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    seed=2985
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,2,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,2,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,2,3))
    #pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    #pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    #ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1

    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,2,int(T/d)))
    Grads=np.zeros((len(eLes),samples,2,3))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

            args=[t0,x0,\
            T,theta,theta_fd_0,theta_fd_1,sigma,sigma_fd_0,sigma_fd_1,\
            theta_aux,sigma_aux,sigma_aux_fd_0,sigma_aux_fd_1,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_Grad_chain_ch_paths_v1.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_Grad_chain_Grads_v1.txt",Grads,fmt="%f")


    36382661
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd*0.5
    sigma_fd=sigma_true+fd*0.5
    sigma_aux_fd=sigma_aux+fd*0.5
    start=time.time()
    B=500*15*3
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=9
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v4.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v4.txt",Grads,fmt="%f")


    36380444
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(9)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=1000
    fd=1e-3
    theta_in=-0.8
    sigma_in=1.8
    sd_in=sd_true-0.2
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.5
    alpha=0.5
    seed=342
    x0=x0_sca+np.zeros(N)
    inputs=[]
    l0=2
    L_max=9
    samples=30
    SGD_steps=1
    B=mcmc_links*SGD_steps
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,SGD_steps,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                pars[k,sample]=pool_outputs[samples*k+sample][1]
                Grads[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_ch_paths_v4.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_pars_v4.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_Grads_v4.txt",Grads,fmt="%f")


    36379828

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd*0.5
    sigma_fd=sigma_true+fd*0.5
    sigma_aux_fd=sigma_aux+fd*0.5
    start=time.time()
    B=500*15
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v3.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v3.txt",Grads,fmt="%f")




    36379407

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd*0.5
    sigma_fd=sigma_true+fd*0.5
    sigma_aux_fd=sigma_aux+fd*0.5
    start=time.time()
    B=500*10
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v2.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v2.txt",Grads,fmt="%f")






    36377400
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=2
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=1
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(d_times, obs,label="Observations")
    theta=theta_true
    sigma=sigma_true
    theta_aux=theta+0.2
    sigma_aux=sigma
    sd=sd_true
    fd=1e-4
    theta_fd=theta_true+fd*0.5
    sigma_fd=sigma_true+fd*0.5
    sigma_aux_fd=sigma_aux+fd*0.5
    start=time.time()
    B=500
    samples=30
    # interactive 1 samples=100
    N=50
    x0=x0_sca+np.zeros(N)
    l0=3
    L_max=8
    seed=0
    eLes=np.array(range(l0,L_max+1))
    mcmc_mean=np.zeros((len(eLes),samples,int(T/d)))
    mcmc=np.zeros((len(eLes),samples,B,int(T/d)))
    grads_mean=np.zeros((len(eLes),samples,3))
    pf_diffs=np.zeros((len(eLes),samples,int(T/d)))
    pf_l=np.zeros((len(eLes),samples,2,int(T/d)))
    ori_pf_l=np.zeros((len(eLes),samples,int(T/d)))
    resamp_coef=1
    inputs=[]

    ch_paths=np.zeros((len(eLes),samples,int(T/d)))
    Grads=np.zeros((len(eLes),samples,3))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

            args=[t0,x0,\
            T,theta,theta_fd,sigma,sigma_fd,\
            theta_aux,sigma_aux,sigma_aux_fd,\
            obs,sd,resamp_coef,l,d, N,seed,fd,B]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Grad_chain,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]

                Grads[k,sample]=pool_outputs[samples*k+sample][1]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()

    Grads=Grads.flatten()
    np.savetxt("data/Prl_Grad_chain_ch_paths_v1.txt",ch_paths,fmt="%f")

    np.savetxt("data/Prl_Grad_chain_Grads_v1.txt",Grads,fmt="%f")



    36371985

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(9)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=4000
    fd=1e-3
    theta_in=-0.8
    sigma_in=1.8
    sd_in=sd_true-0.2
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.5
    alpha=0.5
    seed=342
    x0=x0_sca+np.zeros(N)
    inputs=[]
    l0=2
    L_max=10
    samples=30
    SGD_steps=2
    B=mcmc_links*SGD_steps
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,SGD_steps,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                pars[k,sample]=pool_outputs[samples*k+sample][1]
                Grads[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_pars_v3.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_Grads_v3.txt",Grads,fmt="%f")

    

    36371021

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(8)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=4*200
    SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=0.9
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=30
    gamma=0.5
    alpha=0.5
    seed=23
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=4)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_ch_paths_v2.txt",ch_paths,fmt="%f")
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_pars_v2.txt",pars,fmt="%f")
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_Grads_v2.txt",Grads,fmt="%f")

    36370868
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=4*200
    SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=0.9
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=30
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=4)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_ch_paths_v2.txt",ch_paths,fmt="%f")
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_pars_v2.txt",pars,fmt="%f")
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_Grads_v2.txt",Grads,fmt="%f")



    36363730

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=2000
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=sd_true-0.2
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.5
    alpha=0.5
    seed=342
    x0=x0_sca+np.zeros(N)
    inputs=[]
    l0=2
    L_max=8
    samples=30
    SGD_steps=2
    B=mcmc_links*SGD_steps
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,SGD_steps,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                pars[k,sample]=pool_outputs[samples*k+sample][1]
                Grads[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_ch_paths_v2.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_pars_v2.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_Grads_v2.txt",Grads,fmt="%f")




    36363019

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=sd_true-0.2
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.5
    alpha=0.5
    seed=342
    x0=x0_sca+np.zeros(N)
    inputs=[]
    l0=2
    L_max=8
    samples=30
    SGD_steps=2
    B=mcmc_links*SGD_steps
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,SGD_steps,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))

    for j in range(len(eLes)):
        l=eLes[j]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
            sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
            sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
            obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
            bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
            mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
            inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=pool_outputs[samples*k+sample][0]
                pars[k,sample]=pool_outputs[samples*k+sample][1]
                Grads[k,sample]=pool_outputs[samples*k+sample][2]
                #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_ch_paths_v1.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_pars_v1.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_Grads_v1.txt",Grads,fmt="%f")



    36361062

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=200
    SGD_steps=2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=0.9
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=30
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=4)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_ch_paths_v1.txt",ch_paths,fmt="%f")
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_pars_v1.txt",pars,fmt="%f")
    np.savetxt("data/new_Prl_C_SGD_ou_bridge_Grads_v1.txt",Grads,fmt="%f")




    36360546

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=1000
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=sd_true-0.2
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    gamma=0.5
    alpha=0.5
    seed=342
    x0=x0_sca+np.zeros(N)
    inputs=[]
    l0=2
    L_max=8
    samples=20
    SGD_steps=2
    B=mcmc_links*SGD_steps
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,SGD_steps,3))
    ch_paths=np.zeros((len(eLes),samples,B,int(T/d)))  

   
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,\
        sigma_in,sigma_in_fd,bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,\
        sigma_in_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        obs,bdg.log_g_normal_den,sd_in, bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd,\
        mcmc_links,SGD_steps,gamma, alpha,bdg.update_pars_ou]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_ou_bridge_ch_paths_vtest1.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_pars_vtest1.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_ou_bridge_Grads_vtest1.txt",Grads,fmt="%f")





    36204286

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=0.9
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v45.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v45.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v45.txt",Grads,fmt="%f")
    


    36204255

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.8
    sigma_in=1.8
    sd_in=0.9
    theta_in_fd=theta_in+fd
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    samples=100
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v44.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v44.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v44.txt",Grads,fmt="%f")


    36204191

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    CL=0.10381823
    CL0=2.56
    CP0=1.60118386
    CP=0.23076467*2
    s0=2**0
    pmax=10
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=800
pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v14.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v14.txt",levs,fmt="%f")





    36204172

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    inputs=[]

    CL=0.10381823
    CL0=2.56
    CP0=1.60118386
    CP=0.23076467*2
    s0=2**0
    pmax=10
    l0=0
    lmax=10
    beta_l=1
    beta_p=1
    samples=800
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2),dtype=int)
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v13.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v13.txt",levs,fmt="%f")




    36192631
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*10
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*10
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))

    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v12.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v12.txt",levs,fmt="%f")






    36192619

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*9
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*10
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))

    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v11.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v11.txt",levs,fmt="%f")






    36192617

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*8
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*10
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v10.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v10.txt",levs,fmt="%f")



    36192613
     x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*7
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*10
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v9.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v9.txt",levs,fmt="%f")





    36191923

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*6
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*2
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v8.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v8.txt",levs,fmt="%f")




    36191912


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*2
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v7.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v7.txt",levs,fmt="%f")



    36191857

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*2
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v6.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v6.txt",levs,fmt="%f")




    36191851

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*1
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*2
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
        for i in range(samples):
        seed+=100

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v5.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v5.txt",levs,fmt="%f")




    36191736

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*2
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*2
    CL=0.10381823
    CL0=3.31636298
    CP0=1.78431146
    CP=0.00287355
    s0=2**3
    pmax=7
    l0=4
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100
       
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v4.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v4.txt",levs,fmt="%f")

    
    36181322

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v43.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v43.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v43.txt",Grads,fmt="%f")


    


    36179069

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v42.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v42.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v42.txt",Grads,fmt="%f")




    36178921

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=1.6
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v41.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v41.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v41.txt",Grads,fmt="%f")

    36150693

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*2
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*2
    CL=3.170869039225322
    CL0=3.65411163
    CP0=1.9423342
    CP=0.04938815
    s0=16
    pmax=5
    l0=5
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    for i in range(samples):
        seed+=100
       
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v3.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v3.txt",levs,fmt="%f")



    36150646

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5*2
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*3
    CL=3.170869039225322
    CL0=3.65411163
    CP0=1.9423342
    CP=0.04938815
    s0=16
    pmax=5
    l0=5
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    

    for i in range(samples):
        seed+=100
       
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Unbiased_v4.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Unbiased_levels_v4.txt",levs,fmt="%f")



    36107976
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393+5
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40*4
    CL=3.170869039225322
    CL0=3.65411163
    CP0=1.9423342
    CP=0.04938815
    s0=16
    pmax=5
    l0=5
    lmax=10
    beta_l=1
    beta_p=1
    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))


    for i in range(samples):
        seed+=100
           args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Prl_Unbiased_v3.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Prl_Unbiased_levels_v3.txt",levs,fmt="%f")




    36107976

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100

    fd=1e-4
    theta_in=-0.67
    #obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    inputs=[]
    samples=40
    CL=3.170869039225322
    CL0=3.65411163
    CP0=1.9423342
    CP=0.04938815
    s0=16
    pmax=5
    l0=5
    lmax=10
    beta_l=1
    beta_p=1

    pars=np.zeros((samples,2,2,3))
    levs=np.zeros((samples,2))
    

    for i in range(samples):
        seed+=100
        
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.sampling_ou,[theta_in_aux,sigma_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,bdg.update_pars_ou,resamp_coef,d, N,seed,fd,\
        mcmc_links,gamma, alpha, CL,CL0,CP,CP0,s0,pmax,l0,lmax,beta_l, beta_p]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_Unbiased,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            pars[sample]=pool_outputs[sample][0]
            print(pars[sample])
            levs[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    #ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    levs=levs.flatten()
    #Grads=Grads.flatten()
    np.savetxt("data/Prl_Prl_Unbiased_v2.txt",pars,fmt="%f")
    np.savetxt("data/Prl_Prl_Unbiased_levels_v2.txt",levs,fmt="%i")



    36049964
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=1
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    #obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.4
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v40.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v40.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v40.txt",Grads,fmt="%f")

    


    36049627

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=100
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=9
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v39.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v39.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v39.txt",Grads,fmt="%f")


    This run is made with the log of the standard deviation parameters (for both the diffusion and the observation noise) as the parameters to be estimated. The following run has the particularity of not updating the drift parameter.
    From now on all approximations will be like this.

    36049530
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v38.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v38.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v38.txt",Grads,fmt="%f")

    


    The following run has the particularity of not updating the drift parameter.

    36049452

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=1
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    #obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    obs=np.array([])
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    obs=np.array([theta_in*x0_sca+np.sqrt(1/2)+1])
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v37.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v37.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v37.txt",Grads,fmt="%f")

    36040041
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=4
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v36.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v36.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v36.txt",Grads,fmt="%f")


    

    36040013

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16*4
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v35.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v35.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v35.txt",Grads,fmt="%f")



    36037373

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)

    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v34.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v34.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v34.txt",Grads,fmt="%f")



    36027652

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*16 
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.7
    samples=40
    gamma=0.5
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v34.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v34.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v34.txt",Grads,fmt="%f")
    


    36027647
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=120
    SGD_steps=32*16 
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=40
    gamma=0.6
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v33.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v33.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v33.txt",Grads,fmt="%f")
    


    

    36027636

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=40
    gamma=0.4
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v32.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v32.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v32.txt",Grads,fmt="%f")
    


    36027619
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=80
    SGD_steps=32*16 
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=40
    gamma=0.8
    alpha=0.5
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v33.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v33.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v33.txt",Grads,fmt="%f")

    36007862
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=4
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=100
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v31.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v31.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v31.txt",Grads,fmt="%f")


    36004266

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=40
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6

    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v30.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v30.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v30.txt",Grads,fmt="%f")
    #blocks_pools.append(pool_outputs)




    35974639

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=80
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v29.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v29.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v29.txt",Grads,fmt="%f")





    35969691

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.67
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=80
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v28.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v28.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v28.txt",Grads,fmt="%f")



    35968541

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-1.37
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=80
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v27.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v27.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v27.txt",Grads,fmt="%f")






    35968524

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(tx0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-1.37
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=40
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=50+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-1.37
    theta_in_fd=theta_in+fd
    sigma_in=0.7
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.8
    samples=40
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v26.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v26.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v26.txt",Grads,fmt="%f")


    35935374
    DISCLAIMER: The learning rates of the obsevation noise is changed to 2*gamma instead of gamma. 
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=20
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true+0.7
    samples=40
    gamma=0.5
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v26.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v26.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v26.txt",Grads,fmt="%f")





    35938437

    DISCLAIMER: The learning rates of the obsevation noise is changed to 2*gamma instead of gamma. 

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*32
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.55
    samples=40
    gamma=0.6
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v25.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v25.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v25.txt",Grads,fmt="%f")




    35913206
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=20
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true+0.7
    samples=40
    gamma=0.1
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v24.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v24.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v24.txt",Grads,fmt="%f")
    

    35913700
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*8*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.55
    samples=40
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v23.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v23.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v23.txt",Grads,fmt="%f") 
    

    35909179
    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=10
    SGD_steps=32*16*32
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.55
    samples=20
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v22.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v22.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v22.txt",Grads,fmt="%f")


    35901852
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=40
    SGD_steps=32*8
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.55
    samples=40
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v21.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v21.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v21.txt",Grads,fmt="%f") 



    35901855
    # DISCLAIMER, IN THE FUNCTION Prl_C_SGD_ou_bridge, IN THE SGD STEP I CHANGED 
    # n BY n+2**9 making for a more contant step size.    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=10
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.55
    samples=20
    gamma=0.4
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v20.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v20.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v20.txt",Grads,fmt="%f")



    35900093
    # DISCLAIMER, IN THE FUNCTION Prl_C_SGD_ou_bridge, IN THE SGD STEP I CHANGED 
    # n BY n+2**9 making for a more contant step size.    


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=0.55
    samples=20
    gamma=0.2
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v19.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v19.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v19.txt",Grads,fmt="%f")




    35897013


    # DISCLAIMER, IN THE FUNCTION Prl_C_SGD_ou_bridge, IN THE SGD STEP I CHANGED 
    # n BY n+2**9 making for a more contant step size.    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.2
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v18.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v18.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v18.txt",Grads,fmt="%f")


    35888977

    # DISCLAIMER, IN THE FUNCTION Prl_C_SGD_ou_bridge, IN THE SGD STEP I CHANGED 
    # n BY n+2**9 making for a more contant step size.    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.2
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v17.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v17.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v17.txt",Grads,fmt="%f")

    35888781

    # DISCLAIMER, IN THE FUNCTION Prl_C_SGD_ou_bridge, IN THE SGD STEP I CHANGED 
    # n BY n+2**9 making for a more contant step size.    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.37
    theta_in_fd=theta_in+fd
    sigma_in=1.33
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.2
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v16.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v16.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v16.txt",Grads,fmt="%f")



    35887357

    # DISCLAIMER, IN THE FUNCTION Prl_C_SGD_ou_bridge, IN THE SGD STEP I CHANGED 
    # n BY n+2**9 making for a more contant step size.    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.55
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.38
    theta_in_fd=theta_in+fd
    sigma_in=1.34
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v15.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v15.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v15.txt",Grads,fmt="%f")




    35885310

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=20
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-(0.35+0.375)/2
    theta_in_fd=theta_in+fd
    sigma_in=1.35
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[] 

    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v14.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v14.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v14.txt",Grads,fmt="%f")





    35885265

    

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-(0.35+0.375)/2
    theta_in_fd=theta_in+fd
    sigma_in=1.3
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v13.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v13.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v13.txt",Grads,fmt="%f")




    35881885
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=32*8
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.35
    theta_in_fd=theta_in+fd
    sigma_in=1.275
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v12.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v12.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v12.txt",Grads,fmt="%f")



    35881730

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=-0.35
    theta_in_fd=theta_in+fd
    sigma_in=1.275
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]


    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v11.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v11.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v11.txt",Grads,fmt="%f")





    35875202

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32*16
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v10.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v10.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v10.txt",Grads,fmt="%f")


    35875159

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32*8
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v9.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v9.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v9.txt",Grads,fmt="%f")




    35868262

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=32
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=10
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v8.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v8.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v8.txt",Grads,fmt="%f")




    Prl_C_SGD_ou_bridge_pars_v7.txt
    35868131
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=64
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=8
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v7.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v7.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v7.txt",Grads,fmt="%f")

    35822309



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=64*2*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=10
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]
    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v6.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v6.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v6.txt",Grads,fmt="%f")







    35797322


    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=64*2*2
    B=mcmc_links*SGD_steps
    fd=1e-4
    theta_in=theta_true
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true
    samples=20
    gamma=0.01
    alpha=0.01
    seed=2393
    x0=x0_sca+np.zeros(N)
    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]

    l=5
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v5.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v5.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v5.txt",Grads,fmt="%f")



    35768989
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=64*2

    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true+0.7


    samples=20
    gamma=0.05
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)

    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]


    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

     

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v3.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v3.txt",Grads,fmt="%f")



    35769518

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=64*2*2

    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.1
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.1
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true+0.1


    samples=50
    gamma=0.01
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)

    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]


    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v4.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v4.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v4.txt",Grads,fmt="%f")





    35751032



    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=100
    SGD_steps=64*2

    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true+0.7


    samples=20
    gamma=0.05
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)

    pars=np.zeros((samples,2,SGD_steps+1,3))
    Grads=np.zeros((samples,2,B,3))
    ch_paths=np.zeros((samples,2,B,int(T/d)))
    inputs=[]


    l=6
    for i in range(samples):
        seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))

     

        args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
        bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
        bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
        bdg.H_quasi_normal,\
        [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
        bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
        bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
        alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=[pool_outputs[sample][0],pool_outputs[sample][1]]
            pars[sample]=[pool_outputs[sample][2],pool_outputs[sample][3]]
            Grads[sample]=[pool_outputs[sample][4],pool_outputs[sample][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_ch_paths_v3.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_pars_v3.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_Grads_v3.txt",Grads,fmt="%f")




    35750768
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=1000
    SGD_steps=20
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true+0.7
    samples=20
    gamma=0.05
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)
    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))



    inputs=[]


    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))



            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v2.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v2.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v2.txt",Grads,fmt="%f")






    35738556

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=1000
    SGD_steps=20
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true


    samples=20
    gamma=0.05
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)

    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))



    inputs=[]


    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))



            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=80)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_ch_paths_v1.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_pars_v1.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_Grads_v1.txt",Grads,fmt="%f")
                                                                                 



    35727247

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=500
    SGD_steps=20
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true


    samples=10
    gamma=0.05
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)

    l0=2
    L_max=8
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    inputs=[]


    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))



            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_vtest.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_vtest.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_vest.txt",Grads,fmt="%f")



    35727178

    
    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=2
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true


    samples=10
    gamma=0.05
    alpha=0.01
    seed=2393

    x0=x0_sca+np.zeros(N)

    l0=2
    L_max=3
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))

    inputs=[]
    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))


            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_C_SGD_ou_bridge_vtest.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_vtest.txt",pars,fmt="%f")
    np.savetxt("data/Prl_C_SGD_ou_bridge_vest.txt",Grads,fmt="%f")


    35727025

    x0_sca=1.2
    x0=x0_sca
    l=10
    T=5
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=0.6
    np.random.seed(3)
    d_times=np.array(range(t0+d,int(T/d)+1))*d
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)

    resamp_coef=1
    N=50
    start=time.time()
    mcmc_links=50
    SGD_steps=2
    B=mcmc_links*SGD_steps

    fd=1e-4
    theta_in=theta_true-0.4
    theta_in_fd=theta_in+fd
    sigma_in=sigma_true-0.5
    sigma_in_fd=sigma_in+fd
    sigma_in_aux=sigma_in
    theta_in_aux=theta_in+0.2
    sigma_in_aux_fd=sigma_in_aux+fd
    sd_in=sd_true


    samples=10
    gamma=0.05
    alpha=0.01
    seed=2393
    
    x0=x0_sca+np.zeros(N)

    l0=2
    L_max=3
    eLes=np.array(range(l0,L_max+1))
    pars=np.zeros((len(eLes),samples,2,SGD_steps+1,3))
    Grads=np.zeros((len(eLes),samples,2,B,3))
    ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))


    
    inputs=[]
    

    for k in range(len(eLes)):
        l=eLes[k]
        for i in range(samples):
            seed+=2*(B+1)*int((int(T/d))*(2**l*d-1))
    

          
            
            args=[t0,x0,T,bdg.b_ou_1d,theta_in,theta_in_fd,bdg.Sig_ou_1d,sigma_in,sigma_in_fd,\
            bdg.b_ou_aux,theta_in_aux,bdg.Sig_ou_aux,sigma_in_aux,sigma_in_aux_fd,\
            bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_in_aux,sigma_in_aux]],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd]],\
            bdg.H_quasi_normal,\
            [bdg.ou_sd,[theta_in_aux,sigma_in_aux],theta_in_aux],[bdg.ou_sd,[theta_in_aux,sigma_in_aux_fd],theta_in_aux],\
            bdg.rej_max_coup_ou, [theta_in_aux,sigma_in_aux,theta_in_aux,sigma_in_aux],obs,bdg.log_g_normal_den,sd_in,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_aux_trans_ou_new,\
            bdg.ou_trans_den,[theta_in_aux,sigma_in_aux],bdg.Grad_log_G_new,resamp_coef,l,d, N,seed,fd,mcmc_links,SGD_steps,gamma,\
            alpha]
            inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_C_SGD_ou_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    
    #pars=np.zeros((len(eLes),samples,2,SGD_steps,3))
    #Grads=np.zeros((len(eLes),samples,2,B,3))
    #ch_paths=np.zeros((len(eLes),samples,2,B,int(T/d)))
    
    for k in range(len(eLes)):
        for sample in range(samples):
                ch_paths[k,sample]=[pool_outputs[sample+samples*k][0],pool_outputs[sample+samples*k][1]]
                pars[k,sample]=[pool_outputs[sample+samples*k][2],pool_outputs[sample+samples*k][3]]
                Grads[k,sample]=[pool_outputs[sample+samples*k][4],pool_outputs[sample+samples*k][5]]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_vtest.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_vtest.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_C_SGD_ou_bridge_vest.txt",Grads,fmt="%f")
    

    34831722

    This iteration is made in order to check the SGD with random initial parameters. 
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])
    samples=20
    start=time.time()
    mcmc_links=10*20
    SGD_steps=80
    B=mcmc_links*SGD_steps
    gamma=0.1
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1. 
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))

        theta_0=theta_true+np.random.uniform(-0.5,0.5)
        sigma_0=sigma_true+np.random.uniform(-0.5,0.5) 
        theta_0_fd=theta_0+fd_rate
        sigma_0_fd=sigma_0+fd_rate
        theta_0_aux=theta_0+0.2
        sigma_0_aux=sigma_0
        sigma_0_aux_fd=sigma_0_aux+fd_rate
        sd_0=sd_true


        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest26.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest26.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest26.txt",Grads,fmt="%f")


    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])
    samples=10
    start=time.time()
    mcmc_links=10*20
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1. 
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))

        theta_0=theta_true+np.random.uniform(-0.5,0.5)
        sigma_0=sigma_true+np.random.uniform(-0.5,0.5) 
        theta_0_fd=theta_0+fd_rate
        sigma_0_fd=sigma_0+fd_rate
        theta_0_aux=theta_0+0.2
        sigma_0_aux=sigma_0
        sigma_0_aux_fd=sigma_0_aux+fd_rate
        sd_0=sd_true


        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest25.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest25.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest25.txt",Grads,fmt="%f")




    Gradiend_flow_&_SGD_1.pdf was done with this data.s
    This iteration is made fixing the parameters update of the r_pars, H_pars etc. 
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])
    samples=10
    start=time.time()
    mcmc_links=10*20
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1. 
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge,inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest24.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest24.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest24.txt",Grads,fmt="%f")


    34825161

    This iteration checks the SGD algorithm for the new samples.  

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])
    samples=20
    start=time.time()
    mcmc_links=10*200
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1. 
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        seed=0
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest23.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest23.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest23.txt",Grads,fmt="%f")


    This iteration is made to compare the value of the grandient and the smoother 
    for the SGD function when the number of SGD_steps is large. 

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])
    samples=10
    start=time.time()
    mcmc_links=10*20
    SGD_steps=1
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=-0.468427
    sigma_0=1.27497   
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=0.41743
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=11)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest22.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest22.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest22.txt",Grads,fmt="%f")


    This iteration is made to check the what happnes when we break the mcmc chain.
    Prl_SGD_bridge_ch_paths_vtest21.txt

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    plt.plot(times[1:],x_reg,label="True signal")
    plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])
    samples=10
    start=time.time()
    mcmc_links=10*20
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=11)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest21.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest21.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest21.txt",Grads,fmt="%f")



    THE PROBLEM WAS THAT l_d=-1, and it was supposed to be l_d=0.
    This test is made to figure out why the ch_paths don't have the right dimensions.
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=-1
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=1
    start=time.time()
    mcmc_links=10*2
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=11)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest20.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest20.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest20.txt",Grads,fmt="%f")


    This run is made in order to run the algorithm for a shorter time 
    and be able to do comparations within the day. The additional code that resetted the 
    Gibbs sampler is removed from the Prl_SGD_bridge function in this iteration.
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=-1
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=10
    start=time.time()
    mcmc_links=10*20
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=11)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest19.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest19.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest19.txt",Grads,fmt="%f")



    34817473

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=-1
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=10*200
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest18.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest18.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest18.txt",Grads,fmt="%f")


    This iteration is made with a modified Prl_SGD_bridge function, at each SGD_step the conditional path is reseted
    to the path obtained by the particle filter.
    Prl_SGD_bridge_Grads_vtest17.txt
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=-1
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=1
    start=time.time()
    mcmc_links=10*2
    SGD_steps=5
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=(B+1)*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest17.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest17.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest17.txt",Grads,fmt="%f")


    Prl_SGD_bridge_Grads_vtest15.txt
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=2
    start=time.time()
    mcmc_links=10*2
    SGD_steps=1
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):
        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest15.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest15.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest15.txt",Grads,fmt="%f")

    34809538
    Prl_SGD_bridge_Grads_vtest14    

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=10*200
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):

        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))

    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest14.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest14.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest14.txt",Grads,fmt="%f")



        N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=2
    start=time.time()
    mcmc_links=10*20
    SGD_steps=1
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):

        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=5)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))

    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest13.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest13.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest13.txt",Grads,fmt="%f")


    34805311

    This iteration is made to check if the small change I made to the code 
    making a difference. 

    Prl_SGD_bridge_Grads_vtest12
    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=10*200
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):

        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))

    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest12.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest12.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest12.txt",Grads,fmt="%f")




    This realization of the code is made in order to compare the behavior of the system in the first step compared
    to the the last steps, in order to do so this realization starts with spefici parameters theta_0, sigma_0, and
    sd_0, these are the same as the last sgd step of the previous realization.

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=10
    start=time.time()
    mcmc_links=10*200
    SGD_steps=1
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    
    theta_0=-0.497098
    sigma_0=1.286875  
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=0.447954
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))
    for i in range(samples):
        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=10)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest11.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest11.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest11.txt",Grads,fmt="%f")


    34791076

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=9
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=10*200
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):

        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)
    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    print("Parallelized processes time:",end-start,"\n")
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))

    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest9.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest9.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest9.txt",Grads,fmt="%f")


    34788321

    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=8
    T=10
    t0=0
    l_d=-3
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=10*200
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.05
    alpha=0.01
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.1
    sigma_0=1.
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))
    Grads=np.zeros((samples,B,3))

    for i in range(samples):

        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))

    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest8.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest8.txt",pars,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_Grads_vtest8.txt",Grads,fmt="%f")






    N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=12
    T=3
    t0=0
    l_d=-6
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10    
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    #plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=10
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.0
    alpha=0.25
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=theta_true+0.4
    sigma_0=sigma_true-0.3
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3)) 
    Grads=np.zeros((samples,B,3))

    for i in range(samples):

    
        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    
    print("Parallelized processes time:",end-start,"\n")            
    
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            Grads[sample]=pool_outputs[sample][2]

            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    Grads=Grads.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest6.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest6.txt",pars,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_Grads_vtest6.txt",Grads,fmt="%f")  




    # We will check the value of the gradient, this case is special because we are using d<1
    Prl_SGD_bridge_ch_paths_vtest4

    N=50
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=8
    T=10
    t0=0
    l_d=-4
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #print(times, l_times)

    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    # x_reg 
    # stands for x regular
    sd_true=1e0
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=100
    start=time.time()
    mcmc_links=100
    SGD_steps=10
    B=mcmc_links*SGD_steps
    gamma=0.0
    alpha=0.25
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=theta_true
    sigma_0=sigma_true
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3)) 

    for i in range(samples):
        
       
        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=100)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    
    print("Parallelized processes time:",end-start,"\n")            
    
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest4.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest4.txt",pars,fmt="%f")



N=50
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=8
    T=10
    t0=0
    l_d=-4
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.2
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.array(range(t0,int(T/d)+1))*d
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #print(times, l_times)

    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    # x_reg 
    # stands for x regular
    sd_true=1e0
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=10
    start=time.time()
    mcmc_links=10
    SGD_steps=2
    B=mcmc_links*SGD_steps
    gamma=0.0
    alpha=0.25
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=theta_true
    sigma_0=sigma_true
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3)) 

    for i in range(samples):
        
        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=40)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()
    
    print("Parallelized processes time:",end-start,"\n")            
    
    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))
    
    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    np.savetxt("Observations&data/Prl_SGD_bridge_ch_paths_vtest3.txt",ch_paths,fmt="%f")
    np.savetxt("Observations&data/Prl_SGD_bridge_pars_vtest3.txt",pars,fmt="%f")



Prl_SGD_bridge_ch_paths_vtest2.txt
Prl_SGD_bridge_pars_vtest2.txt
34761187

N=100
    x0_sca=1.2
    x0=x0_sca+np.zeros(N)
    l=8
    T=10
    t0=0
    l_d=0
    d=2**(l_d)
    theta_true=-0.3
    sigma_true=1.1
    np.random.seed(7)
    collection_input=[bdg.b_ou_1d,theta_true,bdg.Sig_ou_1d,sigma_true]
    resamp_coef=1
    l_max=10
    x_true= bdg.gen_gen_data_1d(T,x0_sca,l_max,collection_input)
    x_reg=bdg.cut(T,l_max,-l_d,x_true)[1:]
    times=np.arange(t0,T+1,d)
    l_times=np.arange(t0,T,2**(-l))
    l_max_times=np.arange(t0,T,2**(-l_max))
    #print(times, l_times)

    #plt.plot(times[1:],x_reg,label="True signal")
    #plt.plot(l_max_times,x_true[:-1],label="True complete signal")
    # x_reg 
    # stands for x regular
    sd_true=5e-1
    np.random.seed(3)
    obs=bdg.gen_obs(x_reg,bdg.g_normal_1d,sd_true)
    plt.plot(times[1:], obs,label="Observations")
    #print(obs,x_reg)
    fd_rate=1e-4
    [theta_fd,sigma_fd,sd_fd]=np.array([theta_true,sigma_true,sd_true])+fd_rate*np.array([1,1,1])

    samples=20
    start=time.time()
    mcmc_links=20
    SGD_steps=1000
    B=mcmc_links*SGD_steps
    gamma=0.1
    alpha=0.25
    seed=1
    #mcmc_mean=np.zeros((samples,2,int(T/d))) # This varible was originally designed 
    # to store the mean of both processes, the one with multinomial sampling and the one with
    # backward sampling.
    resamp_coef=1
    pars=np.zeros((SGD_steps+1,3))
    theta_0=0.2
    sigma_0=1.6
    theta_0_fd=theta_0+fd_rate
    sigma_0_fd=sigma_0+fd_rate
    theta_0_aux=theta_0+0.2
    sigma_0_aux=sigma_0
    sigma_0_aux_fd=sigma_0_aux+fd_rate
    sd_0=sd_true
    inputs=[]
    seed=0
    ch_paths=np.zeros((samples,B,int(T/d)))
    pars=np.zeros((samples,SGD_steps+1,3))

    for i in range(samples):


        seed+=B*int((int(T/d))*(2**l*d-1))
        args=[t0,x0,T,bdg.b_ou_1d,theta_0,theta_0_fd,bdg.Sig_ou_1d,sigma_0,sigma_0_fd,bdg.b_ou_aux,theta_0_aux,bdg.Sig_ou_aux,sigma_0_aux,\
        sigma_0_aux_fd,bdg.r_quasi_normal_1d,[bdg.ou_sd,[theta_0_aux,sigma_0_aux]],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd]],\
        bdg.H_quasi_normal,[bdg.ou_sd,[theta_0_aux,sigma_0_aux],theta_0_aux],[bdg.ou_sd,[theta_0_aux,sigma_0_aux_fd],theta_0_aux],\
        bdg.sampling_ou,[theta_0_aux,sigma_0_aux],\
        obs,bdg.log_g_normal_den,sd_0, bdg.ou_trans_den,[theta_0_aux,sigma_0_aux],\
        bdg.Grad_log_aux_trans_ou_new,bdg.ou_trans_den, bdg.Grad_log_G_new,resamp_coef, l, d,N,seed,fd_rate,\
        mcmc_links,SGD_steps,gamma, alpha]
        inputs.append(args)

    pool = multiprocessing.Pool(processes=20)
    pool_outputs = pool.map(Prl_SGD_bridge, inputs)
    pool.close()
    pool.join()
    #blocks_pools.append(pool_outputs)
    xend1=time.time()
    end=time.time()

    print("Parallelized processes time:",end-start,"\n")

    for sample in range(samples):
            ch_paths[sample]=pool_outputs[sample][0]
            pars[sample]=pool_outputs[sample][1]
            #lps[sample,2]=sample
    #tel_est=np.reshape(np.array(pool_outputs),(len(eLes),samples,int(T/d),dim))

    #log_weightss=log_weightss.flatten()
    ch_paths=ch_paths.flatten()
    pars=pars.flatten()
    np.savetxt("data/Prl_SGD_bridge_ch_paths_vtest2.txt",ch_paths,fmt="%f")
    np.savetxt("data/Prl_SGD_bridge_pars_vtest2.txt",pars,fmt="%f")



"""
#%%